%!TEX program = xelatex
\documentclass[11pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{authblk}
\usepackage{tikz}
\usepackage{pgfplots}
\usepackage{verbatim}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{enumerate}
\usepackage{indentfirst}
\usepackage{amssymb}
\linespread{1.6}\setlength{\footskip}{20pt}\setlength{\parindent}{0pt}
\usetikzlibrary{shapes,snakes}
\newcommand{\argmax}{\operatornamewithlimits{argmax}}
\newcommand{\argmin}{\operatornamewithlimits{argmin}}
\DeclareMathOperator{\col}{col}
\usepackage{booktabs}
\newtheorem{theorem}{Theorem}
\newtheorem{note}{Note}
\newtheorem{definition}{Definition}
\newtheorem{proposition}{Proposition}
\newtheorem{claim}{Claim}
\newtheorem{lemma}{Lemma}
\newtheorem{example}{Example}
\newtheorem{corollary}{Corollary}
\usepackage{graphicx}
\usepackage{geometry}
\usepackage{hyperref}
\newcommand{\code}{	exttt}
\geometry{a4paper,scale=0.8}
\title{Inference and Learning}
\author[*]{Wenxiao Yang}
\affil[*]{Department of Mathematics, University of Illinois at Urbana-Champaign}
\date{2022}
\begin{document}
\maketitle
\tableofcontents
\newpage


\section{Statistical Inference}

\subsection{Basics}
Given an observation $x\in X$, we want to estimate an unknown state $\theta \in S$ (not necessarily random). The $\theta$ can form $x$ with $P_\theta(x)$. We use decision rule $\delta (x)$ to form an action (estimation of $\theta$) $a=\hat{\theta}$.

\textbf{Example:}
\begin{enumerate}[(1)]
    \item Binary hypothesis testing (detection) when $S=\{0,1\}$ e.g. $P_0\sim N(0,\sigma^2), P_1\sim N(\mu,\sigma^2)$
    \item Multiple hypothesis testing (classification) when $S=\{1,2,...,n\}$
    \item (Estimation) when $S=\mathbb{R}$ e.g. $P_\theta\in N(\theta,\sigma^2)$
\end{enumerate}

\subsection{Decision Rule Examples}
\subsubsection*{Binary HT Example}
For the example Binary HT, $P_0\sim N(0,\sigma^2), P_1\sim N(\mu,\sigma^2)$: decision rule $\delta: \mathbb{R} \rightarrow \{0,1\}$

We can find a $\tau$ such that $\delta(x)=\left\{\begin{matrix}
    1,&x\ge \tau\\
    0,& else
\end{matrix}\right.=\mathbf{1}_{x\geq \tau}$. Howe to choose $\tau$?

Type-I error probability: probability that $\theta$ is $0$ but receive $\delta(x)=1$. $$P_I=P_0\{\delta(x)=1\}=P_0\{x\geq \tau\}=Q\left(\frac{\tau}{\sigma}\right)$$
Type-II error probability: probability that $\theta$ is $1$ but receive $\delta(x)=0$. $$P_{II}=P_1\{\delta(x)=0\}=P_1(x<\tau)=Q(\frac{\mu-\tau}{\sigma})$$

Both $P_I$ and $P_{II}$ depends on $\tau$. $Q(t)=\int_t^\infty\frac{e^{-\frac{x^2}{2}}}{\sqrt{2\pi}}dx$

For $\tau=\frac{\mu}{2}$, $P_I=P_{II}=Q\left(\frac{\mu}{2\sigma}\right)$

\subsubsection*{Multiple HT Example}
Consider three state $S=\{1,2,3\}$.
We can find a $\tau$ such that $\delta(x)=\left\{\begin{matrix}
    1,&x< \tau_1\\
    2,& \tau_1\leq x\leq \tau_2\\
    3,& x>\tau_2
\end{matrix}\right.=\mathbf{1}_{x\geq \tau}$.

\textit{Conditional Error Probabilities:} probability that $\theta$ is $i$ but receive $\delta(x)=j$ (6 types in this example) $$P_i\{\delta(x)=j\}, \forall i\neq j$$

\subsubsection*{Estimation Example}
Ex: $P_\theta\sim N(\theta,\sigma^2)$. Perform $\delta(x)=\hat{\theta}$ by using mean-squared error (MSE):
$$MSE= \mathbb{E}_\theta \left[(\delta(x)-\theta)^2\right],\theta\in \mathbb{R}$$

\subsection{Maximum-Likelihood Principle (state is norandom)}
Maximum-Likelihood Principle $$\hat{\theta}=\argmax_{\theta\in S}P_{\theta}(x)=\argmax_{\theta\in S}\ln P_{\theta}(x)$$
Applied to the binary example: $P_0\sim N(0,\sigma^2), P_1\sim N(\mu,\sigma^2)$.

$P_0(x)=\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{x^2}{2\sigma^2}}, P_1(x)=\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(x-\mu)^2}{2\sigma^2}}$. $\ln P_0(x)=c-\frac{x^2}{2\sigma^2}$, $\ln P_1(x)=c-\frac{(x-\mu)^2}{2\sigma^2}$.

Then, the rule can become $$\hat{\theta}=\left\{\begin{matrix}
    0,&x^2<(x-\mu)^2\\
    1,&else
\end{matrix}\right.=\mathbf{1}_{x^2\geq (x-\mu)^2}=\mathbf{1}_{x\geq \frac{\mu}{2}}$$

\subsubsection*{Vector Observations}
Observations $X=\left(x_1,x_2,...,x_n\right)$, where i.i.d. $x_i\sim P_\theta$. Then $$P_\theta(X)=\prod_{i=1}^n P_\theta(x_i),\ \ln P_\theta(X)=\sum_{i=1}^n\ln P_\theta(x_i)$$
$\ln P_0(x)=cn-\frac{\sum_{i=1}^n x_i^2}{2\sigma^2}$, $\ln P_1(x)=cn-\frac{\sum_{i=1}^n(x_i-\mu)^2}{2\sigma^2}$.

Then, the rule can become $$\hat{\theta}=\left\{\begin{matrix}
    0,&\sum_{i=1}^nx_i^2<\sum_{i=1}^n(x_i-\mu)^2\\
    1,&else
\end{matrix}\right.=\mathbf{1}_{\sum_{i=1}^nx_i^2\geq \sum_{i=1}^n(x_i-\mu)^2}=\mathbf{1}_{\bar{x}\geq \frac{\mu}{2}}$$
where $\bar{x}=\frac{1}{n}\sum_{i=1}^nx_i$. Under both $H_0$ and $H_1$, $\bar{x}\sim N(0,\frac{\sigma^2}{n})$.

Then, type I error prob and type II error prob are the same $$P_I=P_0\{\bar{x}\geq \frac{\mu}{2}\}=P_{II}=P_1\{\bar{x}< \frac{\mu}{2}\}=Q\left(\frac{\mu\sqrt{n}}{2\sigma}\right)$$

\subsubsection*{Esitimation $S=\mathbb{R}$}
To estimate $\theta$ when $S=\mathbb{R}$
\begin{equation}
    \begin{aligned}
        &\max_{\theta\in \mathbb{R}}\sum_{i=1}^n\ln P_\theta(x_i)\\
        &\Leftrightarrow \max_{\theta\in \mathbb{R}} \left[cn-\frac{\sum_{i=1}^n(x_i-\theta)^2}{2\sigma^2}\right]\\
        &\Leftrightarrow \max_{\theta\in \mathbb{R}} \sum_{i=1}^n(x_i-\theta)^2 \Rightarrow \hat{\theta}=\bar{x}
    \end{aligned}
    \nonumber
\end{equation}

Then, with $\bar{x}\sim N(\theta,\frac{\sigma^2}{n})$, the $$MSE_\theta=\mathbb{E}_\theta\left(\bar{x}-\theta\right)^2=\frac{\sigma^2}{n}$$

\subsection{Bayesian Decision Rule (state is random)}
\subsubsection{Rules}
Prior probability distribution $\pi(\theta)$,

\underline{Loss/cost function} with action (estimation) $a$ is $l(a,\theta)$. e.g.
\begin{enumerate}
    \item (binary HT) Hamming/zero-one loss $l(a=\hat{\theta},\theta)=\mathbf{1}_{a\neq \theta}$
    \item (estimation) Squared error loss $l(a=\hat{\theta},\theta)=(a-\theta)^2$; Absolute error loss $l(a,\theta)=|a-\theta|$.
\end{enumerate}

\textbf{Risk of decision rule $\delta$:} $$R(\delta)=\mathbb{E}\left(l(\delta(X),\theta)\right)$$ where $(X,\theta)$ are random with prob $\pi(\theta),P_\theta$

\textbf{Note: to help be consistent with machine learning notations, we use $y$ to substitute $\theta$.}

The joint probability $P(x,y)=\pi(y)P_y(x)=P(x)\pi(y|x)$.

\textbf{Bayes rule}
$$\delta_B=\argmin_\delta R(\delta)$$
\underline{Derive Bayes rule}
\begin{equation}
    \begin{aligned}
        R(\delta)&=\int_x\int_y P(x,y) l(\delta(x),y) dy dx\\
        &=\int_x P(x)\int_y \pi(y|x) l(\delta(x),y) dy dx\\
    \end{aligned}
    \nonumber
\end{equation}
Solve optimization problem:
\begin{equation}
    \begin{aligned}
        \min_\delta \int_x P(x)\int_y \pi(y|x) l(\delta(x),y) dy dx
    \end{aligned}
    \nonumber
\end{equation}
\begin{center}
    \fcolorbox{black}{gray!10}{\parbox{.9\linewidth}{this problem can be transformed into optimization problems for each $x\in S$
    \begin{equation}
        \begin{aligned}
            \min_{\delta(x)} \int_y \pi(y|x) l(\delta(x),y) dy
        \end{aligned}
        \nonumber
    \end{equation}}}
\end{center}
\underline{The problem becomes to compute $\pi(y|x)$}. From $P(x,y)=\pi(y)P_y(x)=P(x)\pi(y|x)$, we know $$\pi(y|x)=\frac{\pi(y)P_y(x)}{P(x)}$$

\subsubsection{Maximum A Posteriori (MAP) Decision Rule (Binary example)}
\begin{example}
    Hamming/zero-one loss $l(a,y)=\mathbf{1}_{a\neq y}$
\end{example}

\textbf{Maximum A Posteriori (MAP) Decision Rule}:

Optimization problem is $\delta(x)=\argmin_{a} \sum_{y=0,1} \pi(y|x) \mathbf{1}_{a\neq y} dy=\argmax_{y\in\{0,1\}}\pi(y|x)$

Likelihood ratio: $L(x)=\frac{P_1(x)}{P_0(x)}$

Likelihood ratio test: threshold $\tau=\frac{\pi(0)}{\pi(1)}$. If $L(x)>\tau$ accept $H_1$ (equivalent to $P_1(x)\pi(1)>P_0(x)\pi(0)$ which is also equivalent to comparing $\pi(y|x)$).

In this rule the whole optimization problem also goes to
$$R(\delta_{MAP})=\int_x P(x)\min\{\pi(1|x),\pi(0|x)\}dx$$

\subsubsection{Minimum Mean Squared Error (MMSE) Rule ($\mathbb{R}^n$ example)}
\begin{example}
    (estimation) Squared error loss $l(a,y)=(a-y)^2$.
\end{example}
\subsubsection*{Minimum Mean Squared Error (MMSE) Rule:}
Optimization problem is $\delta(x)=\argmin_{a} \int_{y} \pi(y|x) (a-y)^2 dy$
\begin{equation}
    \begin{aligned}
        0=\int_{y} \pi(y|x) (\delta_B(x)-y) dy=\delta_B(x)-\mathbb{E}\left[Y|X=x\right]
    \end{aligned}
    \nonumber
\end{equation}
\begin{equation}
    \begin{aligned}
        \Rightarrow \delta_B(x)=\mathbb{E}\left[Y|X=x\right]
    \end{aligned}
    \nonumber
\end{equation}
which is called \textbf{conditional mean estimation}.

In this rule the whole optimization problem also goes to
$$R(\delta_{MMSE})=\int_x P(x)\int_y \pi(y|x) (y-\mathbb{E}\left[Y|X=x\right])^2 dy dx=\mathbb{E}_X Var\left[Y|X=x\right]$$

\textbf{Gaussian case:}
If $X\in \mathbb{R}^n$ and $(Y,X)$ are jointly Gaussian, then the conditional mean is a linear function of x, also called linear MMSE estimator.
\begin{equation}
    \begin{aligned}
        \mathbb{E}\left[Y|X=x\right]=\mathbb{E}[Y]+Cov(Y,X)Cov(X)^{-1}(x-\mathbb{E}[X])
    \end{aligned}
    \nonumber
\end{equation}
and the posterior risk is independent of $x$:
\begin{equation}
    \begin{aligned}
        Var\left[Y|X=x\right]=Var[Y]-Cov(Y,X)Cov(X)^{-1}Cov(X,Y)
    \end{aligned}
    \nonumber
\end{equation}
\subsection{Comparison}
Maximum-Likelihood Principle (state is norandom): $\delta_{ML}(x)=\argmax_y P_y(x)$.

Maximum A Posteriori (MAP) Decision Rule (state is random): $\delta_{MAP}(x)=\argmax_y \pi(y|x)=\argmax_y \{\pi(y|x),P_y(x)\}$


\section{Machine Learning in Inference}
Instead of given a prior distribution of $Y$, we are given a \textbf{training set} $T=(X_i,Y_i)_{i=1}^n$ where i.i.d. $(X_i,Y_i)\sim P$. (Distribution $P$ is unknown).

Risk: $R(\delta)=\mathbb{E}_P\left[l(\delta(X),Y)\right]$

The ture optimal decision rule is $$\delta_B=\argmin_\delta \mathbb{E}_P\left[l(\delta(X),Y)\right]$$
which is can't be computed since we don't know how actually $P$ is.

\subsection{Empirical Risk Minimization (ERM)}
Instead of computing optimal decision rule with $P$, we compute the optimal decisioin rule in the training set:
$$\hat{\delta}_n=\argmin_\delta \frac{1}{n}\sum_{i=1}^nl(\delta(X_i),Y_i)$$
The corresponding risk is $R(\hat{\delta}_n)=\mathbb{E}_P\left[l(\hat{\delta}_n(X),Y)\right]$. $\Delta R(\hat{\delta}_n)=R(\hat{\delta}_n)- R(\delta)>0$ always holds.

\textbf{Consistency:} if $\Delta R(\hat{\delta}_n) \rightarrow 0$ as $n \rightarrow \infty$.

\subsubsection{Example: Linear MMSE (LMMSE) estimator}
Use the decision role in the class of $\delta(x)=wx$. To find the linear MMSE (LMMSE) estimation $\delta^*(x)=w^*x$:
\begin{equation}
    \begin{aligned}
        w^*=\argmin_{w}\mathbb{E}_P\left[(wX-Y)^2\right]=\frac{\mathbb{E}[XY]}{\mathbb{E}[X^2]}
    \end{aligned}
    \nonumber
\end{equation}
The rule that minimizes the \textbf{empirical risk} is
\begin{equation}
    \begin{aligned}
        \hat{w}=\argmin_{w}\frac{1}{n}\sum_{i=1}^n(wX_i-Y_i)^2=\frac{\frac{1}{n}\sum_{i=1}^nX_iY_i}{\frac{1}{n}\sum_{i=1}^nX_i^2}
    \end{aligned}
    \nonumber
\end{equation}
The risk of the optimal rule $\delta^*=w^*x$ is $R(\delta^*)$ and the empirical risk under rule $\hat{\delta}(x)=\hat{w}x$ is $R(\hat{\delta}(x))$. $R(\hat{\delta})>R(\delta^*)$ always holds, and $$R(\hat{\delta})\rightarrow R(\delta^*)\text{ as }n \rightarrow \infty$$
According to CLT:
\begin{equation}
    \begin{aligned}
        \sqrt{n}\left(\frac{1}{n}\sum_iX_iY_i- \mathbb{E}(XY)\right) &\stackrel{d}{\longrightarrow} N(0,\sigma^2)\\
        \sqrt{n}\left(\frac{1}{n}\sum_iX_i^2- \mathbb{E}(X^2)\right) &\stackrel{d}{\longrightarrow} N(0,\sigma^2)
    \end{aligned}
    \nonumber
\end{equation}
Then,
\begin{equation}
    \begin{aligned}
        \frac{1}{n}\sum_iX_i^2=\mathbb{E}(X^2)+O(\frac{1}{\sqrt{n}})\\
        \frac{1}{n}\sum_iX_iY_i=\mathbb{E}(XY)+O(\frac{1}{\sqrt{n}})\\
    \end{aligned}
    \nonumber
\end{equation}
which means the error of estimators
\begin{equation}
    \begin{aligned}
        \hat{w}=\frac{\mathbb{E}(X^2)+O(\frac{1}{\sqrt{n}})}{\mathbb{E}(XY)+O(\frac{1}{\sqrt{n}})}=w^*+O(\frac{1}{\sqrt{n}})
    \end{aligned}
    \nonumber
\end{equation}
$$\hat{w}-w^*=O(\frac{1}{\sqrt{n}})$$
and the error of the risks:
\begin{equation}
    \begin{aligned}
        R(\hat{\delta})-R(\delta^*)&=\mathbb{E}_P[\hat{w}X-Y]^2-\mathbb{E}_P[w^*X-Y]^2\\
        &=\mathbb{E}_P[(\hat{w}-w^*)X+w^*X-Y]^2-\mathbb{E}_P[w^*X-Y]^2\\
        &=\mathbb{E}_P[(\hat{w}-w^*)X]^2+2(\hat{w}-w^*)\mathbb{E}_P[X(w^*X-Y)]\\
        &=(\hat{w}-w^*)\mathbb{E}_P(X^2)=O\left(\frac{1}{n}\right)
    \end{aligned}
    \nonumber
\end{equation}

\begin{center}
    \fcolorbox{black}{gray!10}{\parbox{.9\linewidth}{\textbf{\underline{Complexity}:}
    \begin{definition}
        A sequence $f(n)$ is $O(1)$ if $\lim_{n \rightarrow \infty}f(n)<\infty$.
    \end{definition}
    
    \begin{definition}
        A sequence $f(n)$ is $O(g(n))$ if $\frac{f(n)}{g(n)}$ is $O(1)$.
    \end{definition}
    
    \begin{definition}
        A sequence $f(n)$ is $o(1)$ if $\lim_{n \rightarrow \infty}\sup f(n)=0$.
    \end{definition}
    
    \begin{definition}
        A sequence $f(n)$ is $o(g(n))$ if $\lim_{n \rightarrow \infty}\sup \frac{f(n)}{g(n)}=0$.
    \end{definition}
    
    \begin{definition}
        A sequence $f(n)$ is \underline{asymptotic} to $g(n)$ if $\lim_{n \rightarrow \infty} \frac{f(n)}{g(n)}=1$. (This is denoted by $f(n)\sim g(n)$ as $a \rightarrow \infty$)
    \end{definition}
    }}
\end{center}

\subsubsection{Penalized ERM}
$\delta(x)=\sum_{j=1}^Jw_jx^j$

Pick $J=d$ and use ERM with $d$ dimensional $w$: $$\argmin_{w\in \mathbb{R}^d} \frac{1}{n}\sum_{i=1}^n[w^TX_i-Y_i]^2$$

Approach 1: Fix $d<<n$, use ERM.

Approach 2: (\textbf{Penalized ERM}) $$\min_\delta [R_{emp}(\delta)+J(\delta)]$$ ($J(\delta)$ is regularization (penalty) term)

\subsection{Stochastic Approximation}
Robbins and Monro (95)

\underline{Problem:} Find a root of function $h(x)$. ($f(x)=0$)

We do not observe $h(x)$ directly, but we observe $Y\sim P_x$ with
\begin{enumerate}[(1)]
    \item $\mathbb{E}[Y|X=x]=h(x)$
    \item $(Y|X=x)-h(x)$ is bounded
\end{enumerate}
\begin{example}
    $Y=X+Z$ with $\mathbb{E}[Z]=0$ and $Z$ is bounded.
\end{example}
Assumptions: 1. $h'(x^*)>0$; 2. $x^*$ is the unique root of $h$.

\textbf{SA Algorithm}
\begin{enumerate}[$\bullet$]
    \item Pick Sequence $\{a_n\}$ such that $\sum_{n=1}^\infty a_n=\infty$ and $\sum_{n=1}^\infty a^2_n<\infty$ (should converge to $0$ but not too quick) e.g. $a_n=n^{-\alpha}$ when $\alpha\in (\frac{1}{2},1]$.
    \item Initialize $X_1$
    \item Update for $n=1,2,...$, $Y_n\sim P(\cdot|X=X_n)$ $$X_{n+1}=X_n-a_n Y_n$$ until convergence.
\end{enumerate}

\begin{theorem}
    Under these assumptions $$X_n \stackrel{m.s.}{\longrightarrow} x^*\text{ as }n \rightarrow \infty$$ i.e., $\mathbb{E}(X_n-x^*)^2 \rightarrow 0$ as $n \rightarrow \infty$.
\end{theorem}

\begin{enumerate}[$\bullet$]
    \item \textbf{\underline{Performance Measure}} (Convergence rate): the root mean squared error (RMSE) $e_n=\sqrt{\mathbb{E}[(X_n-x^*)^2]}$.
    \item \underline{Projections}: If $x$ is constrained to live in an interval $I$, the update rule becomes $$X_{n+1}=\text{Proj}_x[X_n-a_nY_n]$$
    \item \underline{Averaging}: $$\bar{X}_n=\frac{1}{n}\sum_{i=1}^nX_i=\frac{X_n}{n}+\bar{X}_{n-1}\frac{n-1}{n}$$(nicer graph) (The benefits of this smoothing operation are mostly seen in the initial stages of the SA recursion, and \underline{do not improve the convergence rate}.)
\end{enumerate}

\begin{example}
    Let $h(x) = x$, in which case $x^* = 0$. $Y_n=h(X_n)+Z_n=X_n+Z_n$ where noise $Z_n$ is independent of $Y_n$ with $\mathbb{E}[Z_n]=0,Var(Z_n)=1$ and $Z_n$ is bounded.
\end{example}
Then,
    \begin{equation}
        \begin{aligned}
            X_{n+1}&=X_n-a_n(X_n+Z_n)\\
            &=(1-a_n)X_n-a_n Z_n
        \end{aligned}
        \nonumber
    \end{equation}
The MSE,
\begin{equation}
    \begin{aligned}
        e_{n+1}^2&=\mathbb{E}(X_{n+1}-x^*)^2=\mathbb{E}(X_{n+1})^2\\
        &=\mathbb{E}[(1-a_n)X_n-a_n Z_n]^2\\
        &=(1-a_n)^2\mathbb{E}X_n^2+a_n^2\mathbb{E}Z_n^2\\
        &=(1-a_n)^2e_n^2+a_n^2
    \end{aligned}
    \nonumber
\end{equation}
Pick $a_n=n^{-\alpha}$, where $\alpha\in (\frac{1}{2},1]$
\begin{equation}
    \begin{aligned}
        \Rightarrow e_{n+1}^2=(1-n^{-\alpha})^2e_n^2+n^{-2\alpha}
    \end{aligned}
    \nonumber
\end{equation}
\begin{center}
    \fcolorbox{black}{gray!10}{\parbox{.9\linewidth}{\center{Guess: $e_n=\sqrt{c}n^{-\beta}+H.O.T$}
    \begin{equation}
        \begin{aligned}
            c(n+1)^{-2\beta}+H.O.T=(1-n^{-\alpha})^2cn^{-2\beta}+n^{-2\alpha}+H.O.T
        \end{aligned}
        \nonumber
    \end{equation}
    (where $(n+1)^{-2\beta}=n^{-2\beta}(1+\frac{1}{n})^{-2\beta}=n^{-2\beta}[1-2\beta n^{^{-1}}+O(n^{-2})]$, by Taylor)
    \begin{equation}
        \begin{aligned}
            cn^{-2\beta}-2c\beta n^{^{-1-2\beta}}+H.O.T&=(1-n^{-\alpha})^2cn^{-2\beta}+n^{-2\alpha}+H.O.T\\
            -2c\beta n^{^{-1-2\beta}}+H.O.T&=-2cn^{-\alpha-2\beta}+n^{-2\alpha}+H.O.T
        \end{aligned}
        \nonumber
    \end{equation}
    \textbf{(For $\alpha<1$)}, $-2c\beta n^{^{-1-2\beta}}$ is not dominant term.
    $$H.O.T=-2cn^{-\alpha-2\beta}+n^{-2\alpha}+H.O.T$$
    Identify Power: $2\alpha=\alpha+2\beta$ $\Rightarrow$ $\beta=\frac{\alpha}{2}$ and $c=\frac{1}{2}$

    \textbf{(For $\alpha=1$)}, there are three dominant terms.
    $$-2c\beta n^{^{-1-2\beta}}+H.O.T=-2cn^{-1-2\beta}+n^{-2}+H.O.T$$
    Identify Power: $2=1+2\beta$ $\Rightarrow$ $\beta=\frac{1}{2}$ and $-2c\beta=-2c+1\Rightarrow c=1$

    $$e_n^2\sim c n^{-2\beta}$$
    To let the convergence rate as fast as possible, we want the $\beta$ to be as large as possible. Since $\beta=\frac{\alpha}{2}$, we pick the highest $\alpha=1 \Rightarrow \beta=\frac{1}{2},c=1$.
    $$e_n=O(n^{-\frac{1}{2}})\text{ with }a_n\sim\frac{1}{n}$$

    }}
\end{center}

\begin{example}
    Let $h(x) = x^3$, in which case $x^* = 0$. $Y_n=h(X_n)+Z_n=X_n^2+Z_n$ where noise $Z_n$ is independent of $Y_n$ with $\mathbb{E}[Z_n]=0,Var(Z_n)=1$ and $Z_n$ is bounded.
\end{example}
Then,
    \begin{equation}
        \begin{aligned}
            X_{n+1}&=X_n-a_n(X^3_n+Z_n)\\
            &=X_n-a_nX^3_n-a_n Z_n
        \end{aligned}
        \nonumber
    \end{equation}
Pick $a_n=n^{-\alpha}$, $\alpha\in (\frac{1}{2},1]$ $\Rightarrow \beta=\frac{1}{6},\alpha=\frac{2}{3}$ $\Rightarrow e_n\sim O(n^{-\frac{1}{6}})$

\subsection{Stochastic Gradient Descent (SGD)}
Solve $\min_{x\in\mathbb{R}^n}f(x)$.

We only use a \textbf{noisy version} $g(x,z)$ of $f(x)$, where $\mathbb{E}_z[g(x,z)]=f(x)$.
\begin{equation}
    \begin{aligned}
        \mathbb{E}_z[\nabla_x g(x,z)]=\nabla_x \mathbb{E}_z[g(x,z)]=\nabla f(x)
    \end{aligned}
    \nonumber
\end{equation}
Also pick sequence $\{a_n\}$ such that $\sum_{n=1}^\infty a_n=\infty$ and $\sum_{n=1}^\infty a^2_n<\infty$.

\textbf{SGD}
\begin{enumerate}[$\bullet$]
    \item Initialize $X_1$
    \item Update for $n=1,2,...$, $$X_{n+1}=X_n-a_n \nabla g(X_n,Z_n)$$
\end{enumerate}

\begin{example}
    $f(x)=\frac{1}{2}x^2,x\in \mathbb{R}$. Let $Z$ be a random variable with $\mathbb{E}(Z)=0,Var(Z)=1$. $$g(x,Z)=\frac{1}{2}(x+Z)^2-\frac{1}{2}$$
    \begin{equation}
        \begin{aligned}
            \mathbb{E}[g(x,Z)]&=\frac{1}{2}x^2=f(x)\\
            \nabla_x g(x,Z)&=x+Z \Rightarrow 
            \mathbb{E}[\nabla_xg(x,Z)]=\nabla f(x)
        \end{aligned}
        \nonumber
    \end{equation}
    $$X_{n+1}=X_n-a_n(X_n+Z_n)$$
    which is the same as the stochastic approximation.
\end{example}
\textbf{Main Results:} (Suppose the unique minimum is $x^*$)
\begin{enumerate}[(1)]
    \item \underline{Convergence:} $e_n \rightarrow 0$ as $n \rightarrow \infty$.
    \item \underline{Convergence Rate:} To achieve $\mathbb{E}[f(X_n)]-f(x^*)<\varepsilon$, we need $n=O(\frac{1}{\varepsilon})$ if $f$ is twice continuously differentiable and strongly convex.
\end{enumerate}

GD has linear convergence $\Rightarrow$ $e_n=O(e^{-cn})$; Solve $\varepsilon=O(e^{-cn}) \Rightarrow n=O(\ln \frac{1}{\varepsilon})$. (\textbf{SGD is much worse than GD}, cost more.)

\subsection{SGD Application to Empirical Risk Minimization (ERM)}
$$\min_{w\in \mathbb{R}^d}\frac{1}{n}\sum_{i=1}^nL(\delta_w(X_i),Y_i)+J(w)$$
where $f(w)=\frac{1}{n}\sum_{i=1}^nL(\delta_w(X_i),Y_i)$ is the empirical risk; $J(w)$ is the penalty (regularization) term. (e.g. $\delta_w(x)=w^Tx$, $L(\hat{y},y)=(\hat{y}-y)^2$)
To make $w$ more visible, we can write
\begin{equation}
    \begin{aligned}
        f(w)=\frac{1}{n}\sum_{i=1}^nL(\delta_w(X_i),Y_i)
        =\frac{1}{n}\sum_{i=1}^nQ(X_i,Y_i,w)
    \end{aligned}
    \nonumber
\end{equation}

In the problem $\min_{W\in \mathbb{R}^n}f(w)=\frac{1}{n}\sum_{i=1}^nQ(X_i,Y_i,w)$

\textbf{SGD}
\begin{enumerate}[$\bullet$]
    \item Initialize $W_1$
    \item Update for $k\geq 1$,
    \begin{enumerate}[{Step} 1:]
        \item Pick $i$ uniformly over $\{1,...,n\}$
        \item $W_{k+1}=W_k-a_k \nabla Q(X_k,Y_k,W_k)$
    \end{enumerate}
\end{enumerate}
After $k$ iterations,
$\mathbb{E}[f(W_k)]\leq \min_W f(W)+\varepsilon$, for $k=O(\frac{1}{\varepsilon})$ and $f$ twice differentiable and strongly convex.

Total Cost $O(\frac{d}{\varepsilon})$

\subsubsection*{Computational Cost of GD}
Update: $W_{k+1}=W_k-a_k \frac{1}{n}\sum_{i=1}^n\nabla Q(X_i,Y_i,W_k)$ $\Rightarrow O(dn)$

$\mathbb{E}[f(W_k)]\leq \min_W f(W)+\varepsilon$, for $k=O(\ln\frac{1}{\varepsilon})$ iterations.

Total Cost $O(dn\ln\frac{1}{\varepsilon})$


Typically $d=O(n^b)$ where $0<b<1$.

Reminder $\delta_B=$ Bayes rule
\begin{equation}
    \begin{aligned}
        \delta_B&=\text{ Bayes rule}\\
        \delta^*&=\text{ best rule in } D\\
        &=\argmin_{\delta\in D} R(\delta)\\
        \hat{\delta_n}&=\argmin_{\delta\in D}R_{emp}(\delta)\\
        \tilde{\delta_n}&=\text{solution of the algorithm after $k$ iterations}
    \end{aligned}
    \nonumber
\end{equation}
The excess risk
\begin{equation}
    \begin{aligned}
        \Delta R(\tilde{\delta_n})&=R(\tilde{\delta_n})-R(\delta_B)\\&=[R(\tilde{\delta_n})-R(\hat{\delta_n})]+[R(\hat{\delta_n})-R(\delta^*)]+[R(\delta^*)-R(\delta_B)]
    \end{aligned}
    \nonumber
\end{equation}

The expected excess risk
\begin{equation}
    \begin{aligned}
        =\mathbb{E}\Delta R(\tilde{\delta_n})=\mathbb{E}[R(\tilde{\delta_n})-R(\hat{\delta_n})]+\mathbb{E}[R(\hat{\delta_n})-R(\delta^*)]+\mathbb{E}[R(\delta^*)-R(\delta_B)]
    \end{aligned}
    \nonumber
\end{equation}

To balance the last two items, we want $\frac{d}{n}=O(d^{-\beta})$: solve $$\frac{d}{n}=d^{-\beta} \Rightarrow d^{1+\beta}=n \Rightarrow d=n^{\frac{1}{1+\beta}}$$
$$\Rightarrow\text{ the last two items } O(\frac{d}{n})=O(d^{-\beta})=O(n^{-\frac{\beta}{1+\beta}})$$

To balance the three items, we want
\begin{center}
    $\varepsilon=O(n^{-\frac{\beta}{1+\beta}})$ and $d=O(n^{\frac{1}{1+\beta}})$
    $$\Rightarrow n=O(\varepsilon^{-\frac{1+\beta}{\beta}})=O(d^{1+\beta})$$
\end{center}

The update rule $W_{k+1}=W_k-a_k \nabla Q(X_k,Y_k,W_k)$, where $i\sim \text{Uniform}\{1,2,...,n\}$

\textbf{Relation to Online Learning}

\textbf{Variations on Basic SGD}

mini batch: replace $S$ by a subset $B$ and $n$ by $|B|$
$$\frac{1}{|B|}\sum_{i\in B}Q(X_i,Y_i,W_k)$$
SVRG(Stochastic Variance Randomed Gradient)






\section{Deep Learning}

\begin{definition}
    \underline{Binary linear classifiers} distinguish between two categories through a linear function of the inputs.
\end{definition}

\begin{definition}
    \underline{Linearly separable} refers to a line that can be drawn to perfectly split the two classes.
\end{definition}

\subsection{Perceptron Algorithm}
The Perceptron algorithm is an efficient algorithm for learning a \textbf{linear separator} in $d-$dimensional space, with a mistake bound that depends on the margin of separation of the data.
\subsubsection{General Idea}
Given the training data $$D=\left\{\left\langle x^{(i)},y^{(i)}\right\rangle,i=1,...,n\right\}\in \left(\mathbb{R}^m\times\{0,1\}\right)^n$$ we want to know the exact value of $y\in\{0,1\}$.
\begin{center}\begin{figure}[htbp]
    \centering
    \includegraphics[scale=0.15]{Per_output.png}
    \caption{Perceptron Output}
    \label{}
\end{figure}\end{center}
\begin{center}
    \fcolorbox{black}{gray!10}{\parbox{.9\linewidth}{
    \subsubsection*{General idea:}
    \begin{enumerate}[$\bullet$]
        \item If the perceptron correctly predicts ($\hat{y}=y$):
        \begin{enumerate}[$\cdot$]
            \item Do nothing
        \end{enumerate}
        \item If the perceptron yields an incorrect prediction ($\hat{y}\neq y$):
        \begin{enumerate}[$\cdot$]
            \item If the prediction is $0$ and truth is $1$ ($\hat{y}=0|y=1 \Rightarrow e=y-\hat{y}=1$), add feature vector to weight vector.
            \item If the prediction is $1$ and truth is $0$ ($\hat{y}=1|y=0 \Rightarrow e=y-\hat{y}=-1$), subtract feature vector from the weight vector.
        \end{enumerate}
    \end{enumerate}
    }}
\end{center}

\begin{center}\begin{figure}[htbp]
    \centering
    \includegraphics[scale=0.2]{per.png}
    \caption{Perceptron}
    \label{}
\end{figure}\end{center}

\subsubsection{Activations}
\begin{definition}
    \underline{Activation functions} are element-wise gates for letting information propagate to future layers either transformed with non-linearities or left as-is.
\end{definition}
\textbf{Example of activation function:}
\begin{center}\begin{figure}[htbp]
    \centering
    \includegraphics[scale=0.2]{activations.png}
    \caption{Activations}
    \label{}
\end{figure}\end{center}
\begin{enumerate}[(1)]
    \item \textbf{Identity:} $\text{identity}(x)=I(x)=x$
    \item \textbf{Binary:} $\text{binary}(x)=\text{step}(x)=\left\{\begin{matrix}
        1,&x\geq 0\\
        0,&x<0
    \end{matrix}\right.$
    \item \textbf{Sigmoid:} $\sigma(x)=\frac{1}{e^{-x}}$; $\frac{d\sigma(x)}{dx}=\frac{1}{1+e^{-x}}\left(1-\frac{1}{1+e^{-x}}\right)=\sigma(x)\cdot(1-\sigma(x))$; $\sigma(-x)=1-\sigma(x)$.
    \item \textbf{TanH:} $tanh(x)=\frac{e^x-e^{-x}}{e^x+e^{-x}}$. (TanH is a rescaled sigmoid) $$tanh(x)=\frac{e^{2x}-1}{e^{2x}+1}=1-2\sigma(-2x)=2\sigma(2x)-1$$
    \item \textbf{ReLU:} $g(x)=\max(0,x)$
    \item \textbf{Leaky ReLU:} $g(x)=\max(0.1x,x)$
\end{enumerate}
Since we want the prediction to be either $0$ or $1$, we usually use binary function as the activation function in perceptron.


\subsubsection{Algorithm}
\textbf{Perceptron Algorithm:}
\begin{enumerate}[$\bullet$]
    \item Initialize weights (including a bias term) to zero, e.g. $W=[w,b]=0^{m+1}$.
    \item Under each training epoch: Compute for each sample $\left\langle x^{(i)},y^{(i)}\right\rangle\in D$
    \begin{enumerate}[$\cdot$]
        \item A prediction $\hat{y}^{(i)}=g({x^{(i)}}^TW)$
        \item Prediction error $e^{(i)}=y^{(i)}-\hat{y}^{(i)}$
        \item Weighted update $W=W+\eta e^{(i)}x^{(i)}$
    \end{enumerate}
\end{enumerate}

\subsubsection{Limitations}
\begin{enumerate}[$(1)$]
    \item Only provides a linear classifier boundary.
    \item Only allows for \textbf{binary classifier} between two classes.
    \item \textbf{No convergence possible} if classes are not linearly
    separable.
    \item Perceptron will yield \textbf{multiple boundary/"optimal"
    solutions}.
    \item Boundaries found may \textbf{not} perform \textbf{equally well}.
\end{enumerate}

\subsection{ADAptive LInear NEuron
(ADALINE)}
\subsubsection{General Idea}
Except the activation function in perceptron, we can add a threshold function.

In perceptron, we generate the estimation $\hat{y}$ (after binary function) to help update weight $\{w_i\}_{i=0}^m$. However, in ADALINE, we minimize MSE $z=x^TW$ to update weight $\{w_i\}_{i=0}^m$ before output estimation $\hat{y}$ (before binary function).

Before entering threshold (binary function), we want to minimize a mean-
squared error (MSE) loss
function to estimate weights.

e.g. suppose $g(x)=x$, let $z=x^TW$ be the input of threshold, for each $y$,$$W^*=\argmin_{W}L(z,y)=(y-z)^2$$
\begin{equation}
    \begin{aligned}
        \frac{\partial L(z,y)}{\partial w_i}=-2(y-z)\frac{\partial z}{\partial w_i}=-2(y-z)x_i
    \end{aligned}
    \nonumber
\end{equation}

\begin{center}\begin{figure}[htbp]
    \centering
    \includegraphics[scale=0.2]{adaline.png}
    \caption{ADALINE}
    \label{}
\end{figure}\end{center}

\subsubsection{Widrow-Hoff Delta Rule}
(Gradient Descent Rule for ADALINE)
\begin{enumerate}[$\bullet$]
    \item \textbf{Original: }$$W=W+\eta(y^{(j)}-z)x^{(j)}$$
    \item \textbf{Unit-norm: }$$W=W+\eta(y^{(j)}-z)\frac{x^{(j)}}{\|x^{(j)}\|}$$
    where $\|x\|=\sqrt{x_1^2+x_2^2+\cdots+x_m^2}$
\end{enumerate}

\textbf{The Perceptron and ADALINE use variants of the delta rule!}
\begin{enumerate}[(1)]
    \item \textbf{Perceptron}: Output used in delta rule is $\hat{y}=g(x^TW)$; $W=W+\eta(y^{(j)}-{\hat{y}}^{(j)})x^{(j)}$
    \item \textbf{ADALINE}: Output used to estimate weights is $z=x^TW$. $W=W+\eta(y^{(j)}-z)x^{(j)}$
\end{enumerate}

\subsection{Logistic Regression}
\subsubsection{Generative and Discriminative Classifiers}
The most important difference between naive Bayes and logistic regression is that \underline{logistic regression} is a \textbf{discriminative classifier} while \underline{naive Bayes} is a \textbf{generative classifier}.

Suppose we want to classify class $A$ (dogs) and class $B$ (cats) (More genearl form: assign a class $c\in C$ to a document $d\in D$):
\begin{enumerate}[(1)]
    \item \underline{\textbf{Generative model:}} A generative model would have the goal of understanding what dogs look like and what cats look like. You might literally ask such a model to ‘generate’, i.e., draw, a dog.\\
    e.g. \textbf{naive Bayes:} we do not directly compute the probability that the document $d$ belongs to each class $c$, $P(c|d)$. We compute likelihood $P(d|c)$ and prior probability $P(c)$ to generate best estimation $\hat{c}$. (i.e., we want to know what should the distribution of a document $d$ in class $c$ be like.) $$\hat{c}=\argmax_{c\in C}P(d|c)P(c)$$
    \item \underline{\textbf{Discriminative model:}} A discriminative model, by contrast, is only trying to learn to distinguish the classes (perhaps without learning much about them). That is we want to directly computing $P(c|d)$.
\end{enumerate}

\subsubsection{Sigmoid function}
The goal of binary logistic regression is to train a classifier that can make a binary decision about the class of a new input observation.

The input observation is $x=[x_1,...,x_m]^T$ and the output $y$ is either $1$ or $0$. Instead of using the optimal weights of each feature $x_i$ and binary activation function (\textbf{threshold}: $\hat{y}=1$ if $z\geq 0$ and $\hat{y}=0$ otherwise) to estimate  in Perceptron and ADALINE, \textbf{we want to estimate the probability $P(y=1|x)$.}

However, \underline{the weighted sum $z=x^TW=\sum_{i=1}^mw_ix_i+b$ ranges $-\infty$ to $\infty$}. We want to \textbf{force the $z$ to be a legal probability}, that is, to lie between $0$ and $1$.

The \textbf{sigmoid function} $\sigma(z)=\frac{1}{1+e^{-z}}$ can be used as acitivation for this purpose, $P(y=1|x)=\sigma(x^TW)$. Since $1-\sigma(x)=\sigma(-x)$, $P(y=0|x)=\sigma(-x^TW)$.

\subsubsection{The cross-entropy loss function}
We choose the parameters $W$ that maximize the log probability of the true $y$ labels in the training data given the observations $x$. The conditional probability
\begin{equation}
    \begin{aligned}
        p(y|x)=\left\{\begin{matrix}
            \hat{y},&y=1\\
            1-\hat{y},&y=0
        \end{matrix}\right.=\hat{y}^y(1-\hat{y})^{1-y}
    \end{aligned}
    \nonumber
\end{equation}
To maximize the probability, we log both sides:
\begin{equation}
    \begin{aligned}
        \log p(y|x)=y\log\hat{y}+(1-y)\log(1-\hat{y})
    \end{aligned}
    \nonumber
\end{equation}
Then, we want the $\hat{y}$ to maximize the probability (also the logarithm of the probability):
\begin{equation}
    \begin{aligned}
        \hat{y}^*&=\argmax_{\hat{y}\in[0,1]}\log p(y|x)\\
        &=\argmin_{\hat{y}\in[0,1]}-\log p(y|x)\\
        &=\argmin_{\hat{y}\in[0,1]}-(y\log\hat{y}+(1-y)\log(1-\hat{y}))
    \end{aligned}
    \nonumber
\end{equation}
The right hand side is exactly the \textbf{cross-entropy loss function}:
$$L(y,\hat{y})=-(y\log\hat{y}+(1-y)\log(1-\hat{y}))$$
where $\hat{y}^{(i)}=\sigma(w^Tx^{(i)}+b)$
\begin{equation}
    \begin{aligned}
        \frac{\partial L(y^{(i)},\hat{y}^{(i)})}{\partial w_j}=\left(\sigma(w^Tx^{(i)}+b)-y^{(i)}\right)x_j^{(i)}=(\hat{y}^{(i)}-y^{(i)})x_j^{(i)}
    \end{aligned}
    \nonumber
\end{equation}

The risk (Binary Cross-Entropy Cost) of a weight $W$ is $$J(W)=-\frac{1}{n}\sum_{i=1}^n(y^{(i)}\log\hat{y}^{(i)}+(1-y^{(i)})\log(1-\hat{y}^{(i)}))$$
\begin{equation}
    \begin{aligned}
        \frac{\partial J(w,b)}{\partial w_j}=\frac{1}{n}\sum_{i=1}^n\left(\sigma(w^Tx^{(i)}+b)-y^{(i)}\right)x_j^{(i)}
    \end{aligned}
    \nonumber
\end{equation}

\subsubsection{Algorithm}
\begin{enumerate}[$\bullet$]
    \item Initialize weights (including a bias term) to zero, e.g. $W=[w,b]=0^{m+1}$.
    \item Under each training epoch: Compute for each sample $\left\langle x^{(i)},y^{(i)}\right\rangle\in D$
    \begin{enumerate}[$\cdot$]
        \item A prediction $\hat{y}^{(i)}=g({x^{(i)}}^TW)$
        \item Prediction error $e^{(i)}=y^{(i)}-\hat{y}^{(i)}$
        \item Weighted update $W=W+\eta e^{(i)}x^{(i)}=W-\eta \nabla L(W)$
    \end{enumerate}
\end{enumerate}











































\end{document}