\contentsline {section}{\numberline {1}Learning Basics}{6}{section.1}%
\contentsline {subsection}{\numberline {1.1}Parameters and Hyperparameters}{6}{subsection.1.1}%
\contentsline {subsection}{\numberline {1.2}Neural Network: Back Propagation Algorithm}{6}{subsection.1.2}%
\contentsline {subsubsection}{\numberline {1.2.1}Activations}{6}{subsubsection.1.2.1}%
\contentsline {subsubsection}{\numberline {1.2.2}Multilayer Neural Network}{8}{subsubsection.1.2.2}%
\contentsline {subsubsection}{\numberline {1.2.3}A Simple Example of Back Propagation Algorithm}{9}{subsubsection.1.2.3}%
\contentsline {subsubsection}{\numberline {1.2.4}Back Propagation Algorithm}{10}{subsubsection.1.2.4}%
\contentsline {subsubsection}{\numberline {1.2.5}Other Methods}{12}{subsubsection.1.2.5}%
\contentsline {subsection}{\numberline {1.3}Perceptron Algorithm}{12}{subsection.1.3}%
\contentsline {subsubsection}{\numberline {1.3.1}General Idea}{12}{subsubsection.1.3.1}%
\contentsline {subsubsection}{\numberline {1.3.2}Algorithm}{14}{subsubsection.1.3.2}%
\contentsline {subsubsection}{\numberline {1.3.3}Limitations}{14}{subsubsection.1.3.3}%
\contentsline {subsection}{\numberline {1.4}ADAptive LInear NEuron (ADALINE)}{14}{subsection.1.4}%
\contentsline {subsubsection}{\numberline {1.4.1}General Idea}{14}{subsubsection.1.4.1}%
\contentsline {subsubsection}{\numberline {1.4.2}Widrow-Hoff Delta Rule}{15}{subsubsection.1.4.2}%
\contentsline {subsection}{\numberline {1.5}Logistic Regression (Binary-class Output)}{16}{subsection.1.5}%
\contentsline {subsubsection}{\numberline {1.5.1}Generative and Discriminative Classifiers}{16}{subsubsection.1.5.1}%
\contentsline {subsubsection}{\numberline {1.5.2}Sigmoid function}{16}{subsubsection.1.5.2}%
\contentsline {subsubsection}{\numberline {1.5.3}Cross-entropy Loss Function}{17}{subsubsection.1.5.3}%
\contentsline {subsubsection}{\numberline {1.5.4}Algorithm}{17}{subsubsection.1.5.4}%
\contentsline {subsection}{\numberline {1.6}Softmax Regression (Multi-class Output)}{18}{subsection.1.6}%
\contentsline {subsubsection}{\numberline {1.6.1}Multi-Class Classification and Multi-Label Classification}{18}{subsubsection.1.6.1}%
\contentsline {subsubsection}{\numberline {1.6.2}One-hot Encoding}{19}{subsubsection.1.6.2}%
\contentsline {subsubsection}{\numberline {1.6.3}Softmax function}{20}{subsubsection.1.6.3}%
\contentsline {subsubsection}{\numberline {1.6.4}Categorical Cross-entropy Loss Function}{20}{subsubsection.1.6.4}%
\contentsline {subsection}{\numberline {1.7}Deep Feedforward Networks}{21}{subsection.1.7}%
\contentsline {subsubsection}{\numberline {1.7.1}Definition}{21}{subsubsection.1.7.1}%
\contentsline {subsubsection}{\numberline {1.7.2}Universal Approximation Theorem}{22}{subsubsection.1.7.2}%
\contentsline {subsection}{\numberline {1.8} Mini-batch Optimization}{23}{subsection.1.8}%
\contentsline {subsubsection}{\numberline {1.8.1}Stochastic Gradient Descent (SGD) and Batch Gradient Descent (BGD)}{23}{subsubsection.1.8.1}%
\contentsline {subsubsection}{\numberline {1.8.2}Mini-Batch Gradient Descent (MBGD)}{23}{subsubsection.1.8.2}%
\contentsline {subsection}{\numberline {1.9}Weight Initialization}{24}{subsection.1.9}%
\contentsline {subsubsection}{\numberline {1.9.1}Xavier Initialization}{24}{subsubsection.1.9.1}%
\contentsline {subsubsection}{\numberline {1.9.2}He Activation}{25}{subsubsection.1.9.2}%
\contentsline {section}{\numberline {2}Adaptive Optimization}{25}{section.2}%
\contentsline {subsection}{\numberline {2.1}Exponentially Weighted Moving Averages}{25}{subsection.2.1}%
\contentsline {subsection}{\numberline {2.2}Adaptive Learning Rates}{26}{subsection.2.2}%
\contentsline {subsubsection}{\numberline {2.2.1}Momentum}{26}{subsubsection.2.2.1}%
\contentsline {subsubsection}{\numberline {2.2.2}Root Mean Square Propagation (RMSProp)}{26}{subsubsection.2.2.2}%
\contentsline {subsubsection}{\numberline {2.2.3}Adaptive Moment Estimation (ADAM)}{26}{subsubsection.2.2.3}%
\contentsline {section}{\numberline {3}Convolutional Neural Network (CNN)}{27}{section.3}%
\contentsline {subsection}{\numberline {3.1}Convolution and Cross-correlation}{27}{subsection.3.1}%
\contentsline {subsection}{\numberline {3.2}Padding (cover the border)}{28}{subsection.3.2}%
\contentsline {subsection}{\numberline {3.3}Stride}{29}{subsection.3.3}%
\contentsline {subsection}{\numberline {3.4}Other Layer Types}{29}{subsection.3.4}%
\contentsline {subsubsection}{\numberline {3.4.1}Pooling}{29}{subsubsection.3.4.1}%
\contentsline {subsubsection}{\numberline {3.4.2}Unpooling}{29}{subsubsection.3.4.2}%
\contentsline {subsection}{\numberline {3.5}3D Convolution}{30}{subsection.3.5}%
\contentsline {section}{\numberline {4}Generative model}{31}{section.4}%
\contentsline {subsection}{\numberline {4.1}Autoencoders}{31}{subsection.4.1}%
\contentsline {subsubsection}{\numberline {4.1.1}Basics}{31}{subsubsection.4.1.1}%
\contentsline {subsubsection}{\numberline {4.1.2}PCA and Autoencoders}{33}{subsubsection.4.1.2}%
\contentsline {subsubsection}{\numberline {4.1.3}Transposed Convolutions (upscale method)}{33}{subsubsection.4.1.3}%
\contentsline {section}{\numberline {5}Statistical Inference}{35}{section.5}%
\contentsline {subsection}{\numberline {5.1}Basics}{35}{subsection.5.1}%
\contentsline {subsection}{\numberline {5.2}Decision Rule Examples}{35}{subsection.5.2}%
\contentsline {subsection}{\numberline {5.3}Maximum-Likelihood Principle (state is norandom)}{36}{subsection.5.3}%
\contentsline {subsection}{\numberline {5.4}Bayesian Decision Rule (state is random)}{37}{subsection.5.4}%
\contentsline {subsubsection}{\numberline {5.4.1}Rules}{37}{subsubsection.5.4.1}%
\contentsline {subsubsection}{\numberline {5.4.2}Maximum A Posteriori (MAP) Decision Rule (Binary example)}{38}{subsubsection.5.4.2}%
\contentsline {subsubsection}{\numberline {5.4.3}Minimum Mean Squared Error (MMSE) Rule ($\mathbb {R}^n$ example)}{39}{subsubsection.5.4.3}%
\contentsline {subsection}{\numberline {5.5}Comparison}{40}{subsection.5.5}%
\contentsline {section}{\numberline {6}Machine Learning in Inference}{40}{section.6}%
\contentsline {subsection}{\numberline {6.1}Empirical Risk Minimization (ERM)}{40}{subsection.6.1}%
\contentsline {subsubsection}{\numberline {6.1.1}Example: Linear MMSE (LMMSE) estimator}{40}{subsubsection.6.1.1}%
\contentsline {subsubsection}{\numberline {6.1.2}Penalized ERM}{42}{subsubsection.6.1.2}%
\contentsline {subsection}{\numberline {6.2}Stochastic Approximation}{42}{subsection.6.2}%
\contentsline {subsection}{\numberline {6.3}Stochastic Gradient Descent (SGD)}{45}{subsection.6.3}%
\contentsline {subsection}{\numberline {6.4}SGD Application to Empirical Risk Minimization (ERM)}{45}{subsection.6.4}%
\contentsline {subsubsection}{\numberline {6.4.1}Different Gradient Descent for ERM}{46}{subsubsection.6.4.1}%
\contentsline {subsubsection}{\numberline {6.4.2}Constraints on Learning Problem}{46}{subsubsection.6.4.2}%
\contentsline {section}{\numberline {7}Stochastic Integration Methods}{48}{section.7}%
\contentsline {subsection}{\numberline {7.1}Deterministic Methods (Better in Low Dimension)}{48}{subsection.7.1}%
\contentsline {subsubsection}{\numberline {7.1.1}Riemann Integration}{48}{subsubsection.7.1.1}%
\contentsline {subsubsection}{\numberline {7.1.2}Trapezoidal Rule}{49}{subsubsection.7.1.2}%
\contentsline {subsubsection}{\numberline {7.1.3}Multidimensional Integration}{49}{subsubsection.7.1.3}%
\contentsline {subsection}{\numberline {7.2}Stachastic Methods (Better in High Dimension)}{50}{subsection.7.2}%
\contentsline {subsubsection}{\numberline {7.2.1}Classical Monte Carlo Integration}{50}{subsubsection.7.2.1}%
\contentsline {subsubsection}{\numberline {7.2.2}Importance Sampling}{50}{subsubsection.7.2.2}%
\contentsline {section}{\numberline {8}Bootstrap (not enough data)}{52}{section.8}%
\contentsline {subsection}{\numberline {8.1}Residual Bootstrap}{52}{subsection.8.1}%
\contentsline {section}{\numberline {9}Particle Filtering}{54}{section.9}%
\contentsline {subsection}{\numberline {9.1}Kalman Filtering (Linear Dynamic System)}{54}{subsection.9.1}%
\contentsline {subsection}{\numberline {9.2}Particle Filtering (Nonlinear Dynamic System)}{54}{subsection.9.2}%
\contentsline {subsubsection}{\numberline {9.2.1}Bayesian Recursive Filtering}{55}{subsubsection.9.2.1}%
\contentsline {subsubsection}{\numberline {9.2.2}Particle Filter (bootstrap filter)}{56}{subsubsection.9.2.2}%
\contentsline {section}{\numberline {10}EM Algorithm}{56}{section.10}%
\contentsline {subsection}{\numberline {10.1}General Structure of the EM Algorithm}{57}{subsection.10.1}%
\contentsline {subsection}{\numberline {10.2}Example 1: Variance Estimation}{58}{subsection.10.2}%
\contentsline {subsubsection}{\numberline {10.2.1}Maximum-Likelihood (ML) Estimation}{59}{subsubsection.10.2.1}%
\contentsline {subsubsection}{\numberline {10.2.2}EM Algorithm}{59}{subsubsection.10.2.2}%
\contentsline {subsection}{\numberline {10.3}Example 2: Estimation of Gaussian Mixtures}{60}{subsection.10.3}%
\contentsline {subsubsection}{\numberline {10.3.1}Unknown Means: ML estimation is hard}{60}{subsubsection.10.3.1}%
\contentsline {subsubsection}{\numberline {10.3.2}Unknown Means: EM Algorithm}{61}{subsubsection.10.3.2}%
\contentsline {subsubsection}{\numberline {10.3.3}Unknown Mixture Probabilities, Means and Variances}{61}{subsubsection.10.3.3}%
\contentsline {subsection}{\numberline {10.4}Information-Theoretic Functional}{62}{subsection.10.4}%
\contentsline {subsection}{\numberline {10.5}Convergence of EM Algorithm}{63}{subsection.10.5}%
\contentsline {subsection}{\numberline {10.6}EM As an Alternating Maximization Algorithm}{64}{subsection.10.6}%
\contentsline {section}{\numberline {11}Hidden Markov model (HMM)}{64}{section.11}%
\contentsline {subsection}{\numberline {11.1}Viterbi Algorithm: (MAP) estimate $X_{1:t}$ given $Y_{1:t}$}{65}{subsection.11.1}%
\contentsline {subsubsection}{\numberline {11.1.1}MAP estimation problem}{65}{subsubsection.11.1.1}%
\contentsline {subsubsection}{\numberline {11.1.2}Viterbi Algorithm}{65}{subsubsection.11.1.2}%
\contentsline {subsection}{\numberline {11.2}Bayesian Estimation of a Sequence: Need (MMSE) estimate $X_{1:t}$ given $Y_{1:t}$}{66}{subsection.11.2}%
\contentsline {subsection}{\numberline {11.3}Forward-Backward Algorithm: (MMSE) estimate $X_{1:t+1}$ given $Y_{1:t}$}{66}{subsection.11.3}%
\contentsline {subsubsection}{\numberline {11.3.1}$\gamma _t(x) \triangleq \mathrm {P}\left \{X_t=x \mid \boldsymbol {Y}=\boldsymbol {y}\right \}$}{67}{subsubsection.11.3.1}%
\contentsline {subsubsection}{\numberline {11.3.2}$\xi _t\left (x, x^{\prime }\right ) \triangleq \mathrm {P}\left \{X_t=x, X_{t+1}=x^{\prime } \mid \boldsymbol {Y}=\boldsymbol {y}\right \}$}{68}{subsubsection.11.3.2}%
\contentsline {subsubsection}{\numberline {11.3.3}Scaling Factors}{69}{subsubsection.11.3.3}%
\contentsline {section}{\numberline {12}Graphic Models}{69}{section.12}%
\contentsline {subsection}{\numberline {12.1}Graph Theory}{70}{subsection.12.1}%
\contentsline {subsection}{\numberline {12.2}Bayesian Networks}{71}{subsection.12.2}%
\contentsline {subsection}{\numberline {12.3}Markov Networks}{71}{subsection.12.3}%
\contentsline {subsubsection}{\numberline {12.3.1}General Form}{71}{subsubsection.12.3.1}%
\contentsline {subsubsection}{\numberline {12.3.2}Hammersley-Clifford theorem}{72}{subsubsection.12.3.2}%
\contentsline {subsubsection}{\numberline {12.3.3}Form of Gibbs distribution (Boltzmann distribution)}{72}{subsubsection.12.3.3}%
\contentsline {subsection}{\numberline {12.4}Conversion of directed graph to undirected graph}{73}{subsection.12.4}%
\contentsline {subsection}{\numberline {12.5} Inference and Learning}{73}{subsection.12.5}%
\contentsline {subsubsection}{\numberline {12.5.1}Inference on Trees}{73}{subsubsection.12.5.1}%
\contentsline {section}{\numberline {13}Variational Inference, Mean-Field Techniques}{76}{section.13}%
\contentsline {subsection}{\numberline {13.1}Naive Mean-Field Methods}{76}{subsection.13.1}%
\contentsline {subsubsection}{\numberline {13.1.1}Graphical Models}{77}{subsubsection.13.1.1}%
\contentsline {subsubsection}{\numberline {13.1.2}Ising Model}{77}{subsubsection.13.1.2}%
\contentsline {subsection}{\numberline {13.2}Exponential Families of Probability Distributions}{78}{subsection.13.2}%
\contentsline {subsection}{\numberline {13.3}ML Estimation}{80}{subsection.13.3}%
\contentsline {subsection}{\numberline {13.4}Maximum Entropy}{81}{subsection.13.4}%
\contentsline {subsection}{\numberline {13.5}}{82}{subsection.13.5}%
\contentsline {subsection}{\numberline {13.6}Connection between Exponential Families and Graphic Models}{82}{subsection.13.6}%
\contentsline {subsubsection}{\numberline {13.6.1}Marginal polytope}{82}{subsubsection.13.6.1}%
\contentsline {subsubsection}{\numberline {13.6.2}Locally Consistent Marginal Distributions}{83}{subsubsection.13.6.2}%
\contentsline {subsubsection}{\numberline {13.6.3}Entropy on Tree Graphs}{83}{subsubsection.13.6.3}%
