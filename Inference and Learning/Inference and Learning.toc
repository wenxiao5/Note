\contentsline {section}{\numberline {1}Statistical Inference}{4}{section.1}%
\contentsline {subsection}{\numberline {1.1}Basics}{4}{subsection.1.1}%
\contentsline {subsection}{\numberline {1.2}Decision Rule Examples}{4}{subsection.1.2}%
\contentsline {subsection}{\numberline {1.3}Maximum-Likelihood Principle (state is norandom)}{5}{subsection.1.3}%
\contentsline {subsection}{\numberline {1.4}Bayesian Decision Rule (state is random)}{6}{subsection.1.4}%
\contentsline {subsubsection}{\numberline {1.4.1}Rules}{6}{subsubsection.1.4.1}%
\contentsline {subsubsection}{\numberline {1.4.2}Maximum A Posteriori (MAP) Decision Rule (Binary example)}{7}{subsubsection.1.4.2}%
\contentsline {subsubsection}{\numberline {1.4.3}Minimum Mean Squared Error (MMSE) Rule ($\mathbb {R}^n$ example)}{7}{subsubsection.1.4.3}%
\contentsline {subsection}{\numberline {1.5}Comparison}{8}{subsection.1.5}%
\contentsline {section}{\numberline {2}Machine Learning in Inference}{8}{section.2}%
\contentsline {subsection}{\numberline {2.1}Empirical Risk Minimization (ERM)}{9}{subsection.2.1}%
\contentsline {subsubsection}{\numberline {2.1.1}Example: Linear MMSE (LMMSE) estimator}{9}{subsubsection.2.1.1}%
\contentsline {subsubsection}{\numberline {2.1.2}Penalized ERM}{10}{subsubsection.2.1.2}%
\contentsline {subsection}{\numberline {2.2}Stochastic Approximation}{10}{subsection.2.2}%
\contentsline {subsection}{\numberline {2.3}Stochastic Gradient Descent (SGD)}{14}{subsection.2.3}%
\contentsline {subsection}{\numberline {2.4}SGD Application to Empirical Risk Minimization (ERM)}{14}{subsection.2.4}%
\contentsline {subsubsection}{\numberline {2.4.1}Different Gradient Descent for ERM}{15}{subsubsection.2.4.1}%
\contentsline {subsubsection}{\numberline {2.4.2}Constraints on Learning Problem}{15}{subsubsection.2.4.2}%
\contentsline {section}{\numberline {3}Stachastic Integradtion Methods}{17}{section.3}%
\contentsline {subsection}{\numberline {3.1}Deterministic Methods (Better in Low Dimension)}{17}{subsection.3.1}%
\contentsline {subsubsection}{\numberline {3.1.1}Riemann Integration}{17}{subsubsection.3.1.1}%
\contentsline {subsubsection}{\numberline {3.1.2}Trapezoidal Rule}{18}{subsubsection.3.1.2}%
\contentsline {subsubsection}{\numberline {3.1.3}Multidimensional Integration}{18}{subsubsection.3.1.3}%
\contentsline {subsection}{\numberline {3.2}Stachastic Methods (Better in High Dimension)}{19}{subsection.3.2}%
\contentsline {subsubsection}{\numberline {3.2.1}Classical Monte Carlo Integration}{19}{subsubsection.3.2.1}%
\contentsline {subsubsection}{\numberline {3.2.2}Importance Sampling}{19}{subsubsection.3.2.2}%
\contentsline {section}{\numberline {4}Bootstrap (not enough data)}{21}{section.4}%
\contentsline {subsection}{\numberline {4.1}Residual Bootstrap}{21}{subsection.4.1}%
\contentsline {section}{\numberline {5}Particle Filtering}{23}{section.5}%
\contentsline {subsection}{\numberline {5.1}Kalman Filtering}{23}{subsection.5.1}%
\contentsline {subsection}{\numberline {5.2}Particle Filtering}{23}{subsection.5.2}%
\contentsline {subsubsection}{\numberline {5.2.1}Bayesian Recursive Filtering}{24}{subsubsection.5.2.1}%
\contentsline {subsubsection}{\numberline {5.2.2}Particle Filter (bootstrap filter)}{24}{subsubsection.5.2.2}%
\contentsline {section}{\numberline {6}}{25}{section.6}%
\contentsline {subsection}{\numberline {6.1}EM Algorithm}{25}{subsection.6.1}%
\contentsline {section}{\numberline {7}Deep Learning}{26}{section.7}%
\contentsline {subsection}{\numberline {7.1}Parameters and Hyperparameters}{26}{subsection.7.1}%
\contentsline {subsection}{\numberline {7.2}Neural Network: Back Propagation Algorithm}{27}{subsection.7.2}%
\contentsline {subsubsection}{\numberline {7.2.1}Activations}{27}{subsubsection.7.2.1}%
\contentsline {subsubsection}{\numberline {7.2.2}Multilayer Neural Network}{29}{subsubsection.7.2.2}%
\contentsline {subsubsection}{\numberline {7.2.3}A Simple Example of Back Propagation Algorithm}{30}{subsubsection.7.2.3}%
\contentsline {subsubsection}{\numberline {7.2.4}Back Propagation Algorithm}{31}{subsubsection.7.2.4}%
\contentsline {subsubsection}{\numberline {7.2.5}Other Methods}{33}{subsubsection.7.2.5}%
\contentsline {subsection}{\numberline {7.3}Perceptron Algorithm}{33}{subsection.7.3}%
\contentsline {subsubsection}{\numberline {7.3.1}General Idea}{33}{subsubsection.7.3.1}%
\contentsline {subsubsection}{\numberline {7.3.2}Algorithm}{34}{subsubsection.7.3.2}%
\contentsline {subsubsection}{\numberline {7.3.3}Limitations}{35}{subsubsection.7.3.3}%
\contentsline {subsection}{\numberline {7.4}ADAptive LInear NEuron (ADALINE)}{35}{subsection.7.4}%
\contentsline {subsubsection}{\numberline {7.4.1}General Idea}{35}{subsubsection.7.4.1}%
\contentsline {subsubsection}{\numberline {7.4.2}Widrow-Hoff Delta Rule}{36}{subsubsection.7.4.2}%
\contentsline {subsection}{\numberline {7.5}Logistic Regression (Binary-class Output)}{37}{subsection.7.5}%
\contentsline {subsubsection}{\numberline {7.5.1}Generative and Discriminative Classifiers}{37}{subsubsection.7.5.1}%
\contentsline {subsubsection}{\numberline {7.5.2}Sigmoid function}{37}{subsubsection.7.5.2}%
\contentsline {subsubsection}{\numberline {7.5.3}Cross-entropy Loss Function}{38}{subsubsection.7.5.3}%
\contentsline {subsubsection}{\numberline {7.5.4}Algorithm}{38}{subsubsection.7.5.4}%
\contentsline {subsection}{\numberline {7.6}Softmax Regression (Multi-class Output)}{39}{subsection.7.6}%
\contentsline {subsubsection}{\numberline {7.6.1}Multi-Class Classification and Multi-Label Classification}{39}{subsubsection.7.6.1}%
\contentsline {subsubsection}{\numberline {7.6.2}One-hot Encoding}{40}{subsubsection.7.6.2}%
\contentsline {subsubsection}{\numberline {7.6.3}Softmax function}{41}{subsubsection.7.6.3}%
\contentsline {subsubsection}{\numberline {7.6.4}Categorical Cross-entropy Loss Function}{41}{subsubsection.7.6.4}%
\contentsline {subsection}{\numberline {7.7}Deep Feedforward Networks}{42}{subsection.7.7}%
\contentsline {subsubsection}{\numberline {7.7.1}Definition}{42}{subsubsection.7.7.1}%
\contentsline {subsubsection}{\numberline {7.7.2}Universal Approximation Theorem}{43}{subsubsection.7.7.2}%
\contentsline {subsection}{\numberline {7.8} Mini-batch Optimization}{44}{subsection.7.8}%
\contentsline {subsubsection}{\numberline {7.8.1}Stochastic Gradient Descent (SGD) and Batch Gradient Descent (BGD)}{44}{subsubsection.7.8.1}%
\contentsline {subsubsection}{\numberline {7.8.2}Mini-Batch Gradient Descent (MBGD)}{44}{subsubsection.7.8.2}%
