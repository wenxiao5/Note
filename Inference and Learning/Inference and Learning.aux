\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Information-Theoretic Functional}{7}{section.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Learning Basics}{7}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Parameters and Hyperparameters}{7}{subsection.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Neural Network: Back Propagation Algorithm}{7}{subsection.2.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Simple Neural Network}}{8}{figure.1}\protected@file@percent }
\newlabel{}{{1}{8}{Simple Neural Network}{figure.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.1}Activations}{8}{subsubsection.2.2.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Activations}}{9}{figure.2}\protected@file@percent }
\newlabel{}{{2}{9}{Activations}{figure.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.2}Multilayer Neural Network}{10}{subsubsection.2.2.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Multilayer Neural Network}}{10}{figure.3}\protected@file@percent }
\newlabel{}{{3}{10}{Multilayer Neural Network}{figure.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.3}A Simple Example of Back Propagation Algorithm}{11}{subsubsection.2.2.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Two Independent Pathways}}{11}{figure.4}\protected@file@percent }
\newlabel{}{{4}{11}{Two Independent Pathways}{figure.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.4}Back Propagation Algorithm}{12}{subsubsection.2.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.5}Other Methods}{13}{subsubsection.2.2.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Perceptron Algorithm}{14}{subsection.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.1}General Idea}{14}{subsubsection.2.3.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Perceptron Output}}{14}{figure.5}\protected@file@percent }
\newlabel{}{{5}{14}{Perceptron Output}{figure.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Perceptron}}{15}{figure.6}\protected@file@percent }
\newlabel{}{{6}{15}{Perceptron}{figure.6}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.2}Algorithm}{15}{subsubsection.2.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.3}Limitations}{15}{subsubsection.2.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}ADAptive LInear NEuron (ADALINE)}{16}{subsection.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.4.1}General Idea}{16}{subsubsection.2.4.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces ADALINE}}{16}{figure.7}\protected@file@percent }
\newlabel{}{{7}{16}{ADALINE}{figure.7}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.4.2}Widrow-Hoff Delta Rule}{17}{subsubsection.2.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5}Logistic Regression (Binary-class Output)}{17}{subsection.2.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.5.1}Generative and Discriminative Classifiers}{17}{subsubsection.2.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.5.2}Sigmoid function}{18}{subsubsection.2.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.5.3}Cross-entropy Loss Function}{18}{subsubsection.2.5.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.5.4}Algorithm}{19}{subsubsection.2.5.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6}Softmax Regression (Multi-class Output)}{19}{subsection.2.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.6.1}Multi-Class Classification and Multi-Label Classification}{19}{subsubsection.2.6.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Multi-Class Classification}}{19}{figure.8}\protected@file@percent }
\newlabel{}{{8}{19}{Multi-Class Classification}{figure.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Multi-Label Classification}}{20}{figure.9}\protected@file@percent }
\newlabel{}{{9}{20}{Multi-Label Classification}{figure.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Examples of Activation Layer and Loss Choice}}{20}{figure.10}\protected@file@percent }
\newlabel{}{{10}{20}{Examples of Activation Layer and Loss Choice}{figure.10}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.6.2}One-hot Encoding}{21}{subsubsection.2.6.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces One-hot encoding}}{21}{figure.11}\protected@file@percent }
\newlabel{}{{11}{21}{One-hot encoding}{figure.11}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.6.3}Softmax function}{22}{subsubsection.2.6.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.6.4}Categorical Cross-entropy Loss Function}{22}{subsubsection.2.6.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.7}Deep Feedforward Networks}{23}{subsection.2.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.7.1}Definition}{23}{subsubsection.2.7.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Deep Neural Network}}{23}{figure.12}\protected@file@percent }
\newlabel{}{{12}{23}{Deep Neural Network}{figure.12}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.7.2}Universal Approximation Theorem}{23}{subsubsection.2.7.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces Universal Approximation Theorem}}{24}{figure.13}\protected@file@percent }
\newlabel{}{{13}{24}{Universal Approximation Theorem}{figure.13}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.8} Mini-batch Optimization}{24}{subsection.2.8}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.8.1}Stochastic Gradient Descent (SGD) and Batch Gradient Descent (BGD)}{24}{subsubsection.2.8.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces BGD and SGD}}{25}{figure.14}\protected@file@percent }
\newlabel{}{{14}{25}{BGD and SGD}{figure.14}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.8.2}Mini-Batch Gradient Descent (MBGD)}{25}{subsubsection.2.8.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces SGD and MBGD}}{26}{figure.15}\protected@file@percent }
\newlabel{}{{15}{26}{SGD and MBGD}{figure.15}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces Comparison of Approaches}}{26}{figure.16}\protected@file@percent }
\newlabel{}{{16}{26}{Comparison of Approaches}{figure.16}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.9}Weight Initialization}{26}{subsection.2.9}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.9.1}Xavier Initialization}{26}{subsubsection.2.9.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.9.2}He Activation}{27}{subsubsection.2.9.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3}Adaptive Optimization}{27}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Exponentially Weighted Moving Averages}{27}{subsection.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Adaptive Learning Rates}{27}{subsection.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.1}Momentum}{27}{subsubsection.3.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.2}Root Mean Square Propagation (RMSProp)}{28}{subsubsection.3.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.3}Adaptive Moment Estimation (ADAM)}{28}{subsubsection.3.2.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {17}{\ignorespaces Pseudocode of ADAM}}{29}{figure.17}\protected@file@percent }
\newlabel{}{{17}{29}{Pseudocode of ADAM}{figure.17}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Convolutional Neural Network (CNN)}{29}{section.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Convolution and Cross-correlation}{29}{subsection.4.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {18}{\ignorespaces Convolution Used in CNN (actually cross-correlation)}}{30}{figure.18}\protected@file@percent }
\newlabel{}{{18}{30}{Convolution Used in CNN (actually cross-correlation)}{figure.18}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Padding (cover the border)}{31}{subsection.4.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {19}{\ignorespaces Padding: $p=1$}}{31}{figure.19}\protected@file@percent }
\newlabel{}{{19}{31}{Padding: $p=1$}{figure.19}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Stride}{31}{subsection.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Other Layer Types}{32}{subsection.4.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.4.1}Pooling}{32}{subsubsection.4.4.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {20}{\ignorespaces Example: maxpool}}{32}{figure.20}\protected@file@percent }
\newlabel{}{{20}{32}{Example: maxpool}{figure.20}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.4.2}Unpooling}{32}{subsubsection.4.4.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {21}{\ignorespaces Example: maxpool+unpool}}{32}{figure.21}\protected@file@percent }
\newlabel{}{{21}{32}{Example: maxpool+unpool}{figure.21}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5}3D Convolution}{32}{subsection.4.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {22}{\ignorespaces 3D Convolution}}{33}{figure.22}\protected@file@percent }
\newlabel{}{{22}{33}{3D Convolution}{figure.22}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Generative model}{33}{section.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Autoencoders}{33}{subsection.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.1.1}Basics}{34}{subsubsection.5.1.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {23}{\ignorespaces Autoencoders}}{34}{figure.23}\protected@file@percent }
\newlabel{}{{23}{34}{Autoencoders}{figure.23}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.1.2}PCA and Autoencoders}{34}{subsubsection.5.1.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {24}{\ignorespaces Principal Component Analysis (PCA)}}{35}{figure.24}\protected@file@percent }
\newlabel{}{{24}{35}{Principal Component Analysis (PCA)}{figure.24}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.1.3}Transposed Convolutions (upscale method)}{36}{subsubsection.5.1.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {25}{\ignorespaces Transposed Convolutions}}{36}{figure.25}\protected@file@percent }
\newlabel{}{{25}{36}{Transposed Convolutions}{figure.25}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {26}{\ignorespaces Comparison}}{36}{figure.26}\protected@file@percent }
\newlabel{}{{26}{36}{Comparison}{figure.26}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {27}{\ignorespaces Transposed Convolution with Stride}}{37}{figure.27}\protected@file@percent }
\newlabel{}{{27}{37}{Transposed Convolution with Stride}{figure.27}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Statistical Inference}{37}{section.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Basics}{37}{subsection.6.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}Decision Rule Examples}{37}{subsection.6.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3}Maximum-Likelihood Principle (state is norandom)}{38}{subsection.6.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4}Bayesian Decision Rule (state is random)}{39}{subsection.6.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.4.1}Rules}{39}{subsubsection.6.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.4.2}Maximum A Posteriori (MAP) Decision Rule (Binary example)}{40}{subsubsection.6.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.4.3}Minimum Mean Squared Error (MMSE) Rule ($\mathbb  {R}^n$ example)}{41}{subsubsection.6.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.5}Comparison}{42}{subsection.6.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {7}Machine Learning in Inference}{42}{section.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1}Empirical Risk Minimization (ERM)}{42}{subsection.7.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.1.1}Example: Linear MMSE (LMMSE) estimator}{42}{subsubsection.7.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.1.2}Penalized ERM}{44}{subsubsection.7.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2}Stochastic Approximation}{44}{subsection.7.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.3}Stochastic Gradient Descent (SGD)}{47}{subsection.7.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.4}SGD Application to Empirical Risk Minimization (ERM)}{47}{subsection.7.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.4.1}Different Gradient Descent for ERM}{48}{subsubsection.7.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.4.2}Constraints on Learning Problem}{48}{subsubsection.7.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {8}Stochastic Integration Methods}{50}{section.8}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.1}Deterministic Methods (Better in Low Dimension)}{50}{subsection.8.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.1.1}Riemann Integration}{50}{subsubsection.8.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.1.2}Trapezoidal Rule}{51}{subsubsection.8.1.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {28}{\ignorespaces (a) Riemann approximation; (b) Trapezoidal approximation.}}{51}{figure.28}\protected@file@percent }
\newlabel{}{{28}{51}{(a) Riemann approximation; (b) Trapezoidal approximation}{figure.28}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.1.3}Multidimensional Integration}{51}{subsubsection.8.1.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {29}{\ignorespaces Two-dimensional integration using regular grid.}}{51}{figure.29}\protected@file@percent }
\newlabel{}{{29}{51}{Two-dimensional integration using regular grid}{figure.29}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.2}Stachastic Methods (Better in High Dimension)}{52}{subsection.8.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.2.1}Classical Monte Carlo Integration}{52}{subsubsection.8.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.2.2}Importance Sampling}{52}{subsubsection.8.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {9}Bootstrap (not enough data)}{54}{section.9}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {9.1}Residual Bootstrap}{54}{subsection.9.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {10}Particle Filtering}{56}{section.10}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {10.1}Kalman Filtering (Linear Dynamic System)}{56}{subsection.10.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {10.2}Particle Filtering (Nonlinear Dynamic System)}{56}{subsection.10.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {30}{\ignorespaces Hidden Markov Model}}{57}{figure.30}\protected@file@percent }
\newlabel{}{{30}{57}{Hidden Markov Model}{figure.30}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {10.2.1}Bayesian Recursive Filtering}{57}{subsubsection.10.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {10.2.2}Particle Filter (bootstrap filter)}{58}{subsubsection.10.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {11}EM Algorithm}{58}{section.11}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {11.1}General Structure of the EM Algorithm}{59}{subsection.11.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {31}{\ignorespaces Complete and incomplete data spaces $Z$ and $Y$.}}{60}{figure.31}\protected@file@percent }
\newlabel{}{{31}{60}{Complete and incomplete data spaces $Z$ and $Y$}{figure.31}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.2}Example 1: Variance Estimation}{60}{subsection.11.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {11.2.1}Maximum-Likelihood (ML) Estimation}{61}{subsubsection.11.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {11.2.2}EM Algorithm}{61}{subsubsection.11.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {11.3}Example 2: Estimation of Gaussian Mixtures}{62}{subsection.11.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {11.3.1}Unknown Means: ML estimation is hard}{62}{subsubsection.11.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {11.3.2}Unknown Means: EM Algorithm}{63}{subsubsection.11.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {11.3.3}Unknown Mixture Probabilities, Means and Variances}{63}{subsubsection.11.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {11.4}Convergence of EM Algorithm}{64}{subsection.11.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {11.5}EM As an Alternating Maximization Algorithm}{65}{subsection.11.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {12}Hidden Markov model (HMM)}{66}{section.12}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {32}{\ignorespaces Hidden Markov Model (HMM)}}{66}{figure.32}\protected@file@percent }
\newlabel{}{{32}{66}{Hidden Markov Model (HMM)}{figure.32}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {12.1}Viterbi Algorithm: (MAP) estimate $X_{1:t}$ given $Y_{1:t}$}{66}{subsection.12.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {12.1.1}MAP estimation problem}{66}{subsubsection.12.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {12.1.2}Viterbi Algorithm}{67}{subsubsection.12.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {12.2}Bayesian Estimation of a Sequence: Need (MMSE) estimate $X_{1:t}$ given $Y_{1:t}$}{67}{subsection.12.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {33}{\ignorespaces (a) Trellis diagram; (b)—(e) evolution of the Viterbi algorithm, showing surviving paths and values $V(t,x)$ at times $t = 2,3,4,5$; (f) optimal path $\vec  {x}^* = (0,2,0,2,0)$ and its value $\varepsilon (\vec  {x}^*) = 11$.}}{68}{figure.33}\protected@file@percent }
\newlabel{}{{33}{68}{(a) Trellis diagram; (b)—(e) evolution of the Viterbi algorithm, showing surviving paths and values $V(t,x)$ at times $t = 2,3,4,5$; (f) optimal path $\vec {x}^* = (0,2,0,2,0)$ and its value $\varepsilon (\vec {x}^*) = 11$}{figure.33}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {12.3}Forward-Backward Algorithm: (MMSE) estimate $X_{1:t+1}$ given $Y_{1:t}$}{68}{subsection.12.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {12.3.1}$\gamma _t(x) \triangleq \mathrm  {P}\left \{X_t=x \mid \boldsymbol  {Y}=\boldsymbol  {y}\right \}$}{68}{subsubsection.12.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {12.3.2}$\xi _t\left (x, x^{\prime }\right ) \triangleq \mathrm  {P}\left \{X_t=x, X_{t+1}=x^{\prime } \mid \boldsymbol  {Y}=\boldsymbol  {y}\right \}$}{70}{subsubsection.12.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {12.3.3}Scaling Factors}{70}{subsubsection.12.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {13}Graphic Models}{71}{section.13}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {13.1}Graph Theory}{71}{subsection.13.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {34}{\ignorespaces (a) Directed and (b) Undirected graph.}}{72}{figure.34}\protected@file@percent }
\newlabel{}{{34}{72}{(a) Directed and (b) Undirected graph}{figure.34}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {13.2}Bayesian Networks}{72}{subsection.13.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {13.3}Markov Networks}{73}{subsection.13.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {13.3.1}General Form}{73}{subsubsection.13.3.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {35}{\ignorespaces (a) (b) Two Bayesian networks and (c) a Markov network.}}{73}{figure.35}\protected@file@percent }
\newlabel{}{{35}{73}{(a) (b) Two Bayesian networks and (c) a Markov network}{figure.35}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {13.3.2}Hammersley-Clifford theorem}{74}{subsubsection.13.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {13.3.3}Form of Gibbs distribution (Boltzmann distribution)}{74}{subsubsection.13.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {13.4}Conversion of directed graph to undirected graph}{74}{subsection.13.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {36}{\ignorespaces Graph moralization}}{75}{figure.36}\protected@file@percent }
\newlabel{}{{36}{75}{Graph moralization}{figure.36}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {13.5} Inference and Learning}{75}{subsection.13.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {13.5.1}Inference on Trees}{75}{subsubsection.13.5.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {37}{\ignorespaces Example 1}}{75}{figure.37}\protected@file@percent }
\newlabel{}{{37}{75}{Example 1}{figure.37}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {38}{\ignorespaces Belief propagation in a tree}}{76}{figure.38}\protected@file@percent }
\newlabel{}{{38}{76}{Belief propagation in a tree}{figure.38}{}}
\@writefile{toc}{\contentsline {section}{\numberline {14}Variational Inference, Mean-Field Techniques}{77}{section.14}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {14.1}Naive Mean-Field Methods}{77}{subsection.14.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {14.1.1}Graphical Models}{78}{subsubsection.14.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {14.1.2}Ising Model}{79}{subsubsection.14.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {14.2}Exponential Families of Probability Distributions}{80}{subsection.14.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {14.3}ML Estimation}{82}{subsection.14.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {14.4}Maximum Entropy}{82}{subsection.14.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {14.5}}{84}{subsection.14.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {14.6}Connection between Exponential Families and Graphic Models}{84}{subsection.14.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {14.6.1}Marginal polytope}{85}{subsubsection.14.6.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {14.6.2}Locally Consistent Marginal Distributions}{85}{subsubsection.14.6.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {14.6.3}Entropy on Tree Graphs}{86}{subsubsection.14.6.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {14.7}Naive Mean-Field Methods In Graph}{86}{subsection.14.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {14.8}Structural Mean Field Optimization}{87}{subsection.14.8}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {14.9}Bethe Entropy Approximation}{87}{subsection.14.9}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {15}$\ell _1$ Penalized Least Squares Minimization}{88}{section.15}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {15.1}Special Cases}{88}{subsection.15.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {15.1.1}Identity $A$}{88}{subsubsection.15.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {15.1.2}Orthonormal $A$}{89}{subsubsection.15.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {15.2}Quadratic Optimization}{89}{subsection.15.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {15.3}Genearl Solution: Lasso}{89}{subsection.15.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {15.4}Genearl Solution: Iterative Soft Thresholding Algorithm (ISTA)}{89}{subsection.15.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {15.5}Convergence Rate}{91}{subsection.15.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {15.6}Fast Iterative Soft Thresholding Algorithm (FISTA)}{91}{subsection.15.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {15.7}Alternating Direction Method of Multipliers (ADMM)}{92}{subsection.15.7}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {16}Compressive Sensing}{92}{section.16}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {16.1}Definitions related to Sparsity}{92}{subsection.16.1}\protected@file@percent }
\gdef \@abspage@last{94}
