\contentsline {chapter}{\numberline {1}Field and Vector Space}{1}{chapter.1}%
\contentsline {section}{\numberline {1.1}Field $(\mathbb {F},+,\cdot )$}{1}{section.1.1}%
\contentsline {subsection}{\numberline {1.1.1}Definition of Field \small {(@ Lec 02 of ECON 204)}}{1}{subsection.1.1.1}%
\contentsline {section}{\numberline {1.2}Vector Space}{1}{section.1.2}%
\contentsline {subsection}{\numberline {1.2.1}Definition of Vector Space \small {(@ Lec 02 of ECON 204)}}{1}{subsection.1.2.1}%
\contentsline {subsection}{\numberline {1.2.2}Theorem: A field is a vector space over its subfield}{2}{subsection.1.2.2}%
\contentsline {subsection}{\numberline {1.2.3}Vector Subspace}{2}{subsection.1.2.3}%
\contentsline {subsection}{\numberline {1.2.4}Linear Independent}{2}{subsection.1.2.4}%
\contentsline {subsection}{\numberline {1.2.5}span V, basis, dimension}{2}{subsection.1.2.5}%
\contentsline {subsection}{\numberline {1.2.6}Dimension}{3}{subsection.1.2.6}%
\contentsline {subsection}{\numberline {1.2.7}Theorem: $|V|>\dim X \Rightarrow $ linearly dependent}{3}{subsection.1.2.7}%
\contentsline {subsection}{\numberline {1.2.8}Theorem: $|V|=n$: Linear Indep $\Leftrightarrow $ Spans $\Rightarrow $ Basis}{4}{subsection.1.2.8}%
\contentsline {subsection}{\numberline {1.2.9}Standard basis vectors}{4}{subsection.1.2.9}%
\contentsline {section}{\numberline {1.3}Linear Transformation}{4}{section.1.3}%
\contentsline {subsection}{\numberline {1.3.1}Definition of Linear Transformation}{4}{subsection.1.3.1}%
\contentsline {subsection}{\numberline {1.3.2}Composition of Linear Transformations is also Linear}{4}{subsection.1.3.2}%
\contentsline {subsection}{\numberline {1.3.3}Function from a Basis extends uniquely to a Linear Transformation}{4}{subsection.1.3.3}%
\contentsline {subsection}{\numberline {1.3.4}A Linear Transformation Correponds to a Matrix}{5}{subsection.1.3.4}%
\contentsline {subsection}{\numberline {1.3.5}Image, Kernel, Rank}{5}{subsection.1.3.5}%
\contentsline {subsection}{\numberline {1.3.6}The Rank-Nullity Theorem: $\dim X=\dim \textnormal {ker }T+\textnormal {Rank }T$}{5}{subsection.1.3.6}%
\contentsline {subsection}{\numberline {1.3.7}Theorem: Linear Transformation $T$ is 1-to-1 $\Leftrightarrow $ $\textnormal {ker }T = \{0\}$}{6}{subsection.1.3.7}%
\contentsline {subsection}{\numberline {1.3.8}Definition of Invertible Linear Transformation}{6}{subsection.1.3.8}%
\contentsline {subsection}{\numberline {1.3.9}Theorem: Inverse of a Linear Transformation is also Linear}{6}{subsection.1.3.9}%
\contentsline {subsection}{\numberline {1.3.10}Theorem: Linear Transformation is completely determined by values on basis}{6}{subsection.1.3.10}%
\contentsline {subsection}{\numberline {1.3.11}GL(V): set of invertible linear transformations $V \rightarrow V$}{6}{subsection.1.3.11}%
\contentsline {section}{\numberline {1.4}Isomorphisms}{7}{section.1.4}%
\contentsline {subsection}{\numberline {1.4.1}Isomorphic: $\exists $ invertible $T \in L(X, Y)$}{7}{subsection.1.4.1}%
\contentsline {subsection}{\numberline {1.4.2}Theorem: Isomorphic $\Leftrightarrow $ $\dim X = \dim Y$}{7}{subsection.1.4.2}%
\contentsline {section}{\numberline {1.5}Quotient Vector Spaces}{7}{section.1.5}%
\contentsline {chapter}{\numberline {2}Basic Definition}{8}{chapter.2}%
\contentsline {section}{\numberline {2.1}Square Matrix $A_{n\times n}$: $det(A)$, singular}{8}{section.2.1}%
\contentsline {section}{\numberline {2.2}Orthogonal Vectors}{8}{section.2.2}%
\contentsline {section}{\numberline {2.3}Orthonormal Vectors}{8}{section.2.3}%
\contentsline {chapter}{\numberline {3}Eigenvalues Related}{9}{chapter.3}%
\contentsline {section}{\numberline {3.1}Eigenvalues, Eigenvectors Definition}{9}{section.3.1}%
\contentsline {section}{\numberline {3.2}Diagonalizable Matrix}{9}{section.3.2}%
\contentsline {section}{\numberline {3.3}Eigen Decomposition of Symmetric Matrices Results}{10}{section.3.3}%
\contentsline {section}{\numberline {3.4}Diagonalization of Real Symmetric Matrices}{10}{section.3.4}%
\contentsline {subsection}{\numberline {3.4.1}Proposition: $\lambda _{\min }\|x\|^2\leq x^TAx\leq \lambda _{\max }\|x\|^2$}{11}{subsection.3.4.1}%
\contentsline {subsection}{\numberline {3.4.2}Proposition: $\lambda ^2$ is the eigenvalue of $A^2$ and $A^TA$}{11}{subsection.3.4.2}%
\contentsline {section}{\numberline {3.5}Trace}{11}{section.3.5}%
\contentsline {section}{\numberline {3.6}Jacobian matrix}{12}{section.3.6}%
\contentsline {section}{\numberline {3.7}Hessian matrix}{12}{section.3.7}%
\contentsline {section}{\numberline {3.8}Positive Definite Matrices}{12}{section.3.8}%
\contentsline {subsection}{\numberline {3.8.1}Definition}{12}{subsection.3.8.1}%
\contentsline {subsection}{\numberline {3.8.2}Condition number (for PD matrix)}{13}{subsection.3.8.2}%
\contentsline {subsection}{\numberline {3.8.3}Diagonal matrix situation}{13}{subsection.3.8.3}%
\contentsline {subsection}{\numberline {3.8.4}Using eigenvalues}{14}{subsection.3.8.4}%
\contentsline {subsection}{\numberline {3.8.5}Sylvesterâ€™s Criterion}{14}{subsection.3.8.5}%
\contentsline {section}{\numberline {3.9}Matrix Norm (Induced Norm) and Spectral Radius}{15}{section.3.9}%
\contentsline {chapter}{\numberline {4}Euclidean geometry basics}{17}{chapter.4}%
\contentsline {section}{\numberline {4.1}Norm}{17}{section.4.1}%
\contentsline {subsection}{\numberline {4.1.1}Vector's Norm}{17}{subsection.4.1.1}%
\contentsline {subsection}{\numberline {4.1.2}Matrix's Norm}{17}{subsection.4.1.2}%
\contentsline {subsection}{\numberline {4.1.3}Difference between Spectral Radius and Spectral Norm}{18}{subsection.4.1.3}%
\contentsline {section}{\numberline {4.2} Euclidean distance, inner product}{18}{section.4.2}%
\contentsline {section}{\numberline {4.3}General Inner Products}{19}{section.4.3}%
\contentsline {subsection}{\numberline {4.3.1} Inner Product}{19}{subsection.4.3.1}%
\contentsline {subsection}{\numberline {4.3.2}Theorem: $*$ is inner product iff $\vec {x}*\vec {y}=\vec {x}^T H \vec {y}$ for some symmetric $H$}{19}{subsection.4.3.2}%
\contentsline {section}{\numberline {4.4}Isometry}{20}{section.4.4}%
\contentsline {section}{\numberline {4.5} Linear isometries i.e. orthogonal group}{21}{section.4.5}%
\contentsline {section}{\numberline {4.6}Special orthogonal group}{21}{section.4.6}%
\contentsline {section}{\numberline {4.7}translation}{21}{section.4.7}%
\contentsline {section}{\numberline {4.8}All isometries can be represented by a composition of \textit {a translation} and \textit {an orthogonal transformation}}{22}{section.4.8}%
\contentsline {chapter}{\numberline {5}Algebra Computation}{23}{chapter.5}%
\contentsline {section}{\numberline {5.1}Hessian Matrix}{23}{section.5.1}%
\contentsline {section}{\numberline {5.2}Taylor's Expansion}{23}{section.5.2}%
\contentsline {section}{\numberline {5.3} Random Vectors and Random Matrices}{23}{section.5.3}%
\contentsline {subsection}{\numberline {5.3.1}Mean}{23}{subsection.5.3.1}%
\contentsline {subsection}{\numberline {5.3.2}Variance, Covariance}{24}{subsection.5.3.2}%
\contentsline {section}{\numberline {5.4} Matrix Multiplication}{25}{section.5.4}%
\contentsline {section}{\numberline {5.5}Matrix Derivation}{25}{section.5.5}%
\contentsline {section}{\numberline {5.6}Matrix Inversion}{27}{section.5.6}%
\contentsline {subsection}{\numberline {5.6.1}Woodbury matrix identity}{27}{subsection.5.6.1}%
\contentsline {section}{\numberline {5.7}Linear Regression: Least Square}{27}{section.5.7}%
\contentsline {subsection}{\numberline {5.7.1}Normal Equations}{27}{subsection.5.7.1}%
\contentsline {section}{\numberline {5.8}LU Decomposition (Restricted to Square)}{28}{section.5.8}%
\contentsline {section}{\numberline {5.9}SVD: Singular Value Decomposition}{29}{section.5.9}%
\contentsline {subsection}{\numberline {5.9.1}Pseudo-inverse}{29}{subsection.5.9.1}%
\contentsline {subsection}{\numberline {5.9.2}Analysis of $A^TA$ and $AA^T$}{30}{subsection.5.9.2}%
\contentsline {subsection}{\numberline {5.9.3}Solve Normal Equations}{31}{subsection.5.9.3}%
\contentsline {subsection}{\numberline {5.9.4}Low-Rank Approximation}{31}{subsection.5.9.4}%
