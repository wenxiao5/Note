%!TEX program = xelatex
\documentclass[11pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{authblk}
\usepackage{ctex}
\usepackage{tikz}
\usepackage{pgfplots}
\usepackage{verbatim}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{indentfirst}
\usepackage{amssymb}
\setlength{\parindent}{0pt}
\usetikzlibrary{shapes,snakes}
\newcommand{\argmax}{\operatornamewithlimits{argmax}}
\newcommand{\argmin}{\operatornamewithlimits{argmin}}
\DeclareMathOperator{\col}{col}
\usepackage{booktabs}
\newtheorem{theorem}{Theorem}
\newtheorem{note}{Note}
\newtheorem{definition}{Definition}
\newtheorem{proposition}{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem{example}{Example}
\newtheorem{corollary}{Corollary}
\usepackage{graphicx}
\usepackage{geometry}
\usepackage{hyperref}
\newcommand{\code}{	exttt}
\geometry{a4paper,scale=0.8}
\title{STAT 425 Note 2}
\author[*]{Wenxiao Yang}
\affil[*]{Department of Mathematics, University of Illinois at Urbana-Champaign}
\date{2021}

\usepackage{listings}
\usepackage{xcolor}

\lstset{numbers=left,numberstyle=\tiny,keywordstyle=\color{blue},commentstyle=\color[cmyk]{1,0,1,0},frame=single,escapeinside=``,extendedchars=false,xleftmargin=2em,xrightmargin=2em,aboveskip=1em,tabsize=4,showspaces=false}






\begin{document}
\maketitle
\tableofcontents
\newpage




\section{Generalized Least Squares}
What do we do if the errors are \textbf{correlated} or \textbf{heteroscedastic}?\\
Suppose $\varepsilon \sim \mathcal{N}_{n}(0, \Sigma)$, where $\Sigma$ is the variance-covariance matrix.
\subsection{GLS, $\Sigma$ known ($\hat{\beta}=\left(\mathbf{X}^{\top} \Sigma^{-1} \mathbf{X}\right)^{-1} \mathbf{X}^{\top} \Sigma^{-1} \mathbf{y}$, ${RSS}=(\mathbf{y}-\mathbf{X} \beta)^{\top} \Sigma^{-1}(\mathbf{y}-\mathbf{X} \beta)$)}
\subsubsection{Method 1: Transform back to OLS}
$$
\mathbf{y}=\mathbf{X} \beta+\varepsilon
$$
where $\varepsilon \sim \mathcal{N}_{n}(0, \Sigma)$ and $\Sigma$ is a known, symmetric, positive definite covariance matrix.\\
When the errors are heteroscedastic or correlated:\\
Transform this problem back to Ordinary Least-Squares (OLS):\\
1. Assume $S^{-1}$ exists and write
$$
\Sigma=S S^{\top}
$$
(We could use, for example, the Cholesky decomposition from linear algebra to obtain $S$.)\\

2. Multiply the model equation by $S^{−1}$ on both sides:
$$
\begin{aligned}
\mathbf{y} &=\mathbf{X} \beta+\varepsilon \\
S^{-1} \mathbf{y} &=S^{-1}(\mathbf{X} \beta+\varepsilon) \\
\underbrace{S^{-1} \mathbf{y}}_{:=\mathbf{y}^{*}} &=\underbrace{S^{-1} \mathbf{X}}_{:=\mathbf{x}^{*}} \beta+\underbrace{S^{-1} \varepsilon}_{:=\varepsilon^{*}} \\
\mathbf{y}^{*} &=\mathbf{X}^{*} \beta+\varepsilon^{*}
\end{aligned}
$$
This implies that
$$
\varepsilon^{*} \sim \mathcal{N}(S^{-1} \mathbf{0}, \underbrace{S^{-1} \Sigma\left(S^{-1}\right)^{\top}}_{=\text {ldentity }})=\mathcal{N}(\mathbf{0}, \mathbf{I})
$$
since $S^{-1} \Sigma\left(S^{-1}\right)^{\top}=S^{-1} S S^{\top}\left(S^{-1}\right)^{\top}=I$\\

3. For the transformed model, we can solve for $\beta$ using OLS:
$$
\mathbf{y}^{*}=\mathbf{X}^{*} \beta+\varepsilon^{*}
$$
where $\mathbf{y}^{*}=S^{-1} \mathbf{y}, \mathbf{X}^{*}=S^{-1} \mathbf{X}$\\
So, the estimator for $\beta$ computes as
$$
\begin{aligned}
\hat{\beta} &=\left(\mathbf{X}^{* \top} \mathbf{X}^{*}\right)^{-1} \mathbf{X}^{* \top} \mathbf{y}^{*} \\
&=(\mathbf{X}^{\top} \underbrace{\left(S^{-1}\right)^{\top} S^{-1}}_{=\Sigma^{-1}} \mathbf{X})^{-1} \mathbf{X}^{\top} \underbrace{\left(S^{-1}\right)^{\top} S^{-1}}_{=\Sigma^{-1}} \mathbf{y} \\
&=\left(\mathbf{X}^{\top} \Sigma^{-1} \mathbf{X}\right)^{-1} \mathbf{X}^{\top} \Sigma^{-1} \mathbf{y}
\end{aligned}
$$
Note that the solution we obtained minimizes:
$$
\left\|\mathbf{y}^{*}-\mathbf{X}^{*} \beta\right\|^{2}=(\mathbf{y}-\mathbf{X} \beta)^{\top} \Sigma^{-1}(\mathbf{y}-\mathbf{X} \beta)
$$

\subsubsection{Weighted Least Squares (WLS)}
Suppose that $\Sigma$ is a diagonal matrix of unequal error variances:
$$
\Sigma=\operatorname{diag}\left(\sigma_{1}^{2}, \sigma_{2}^{2}, \ldots, \sigma_{n}^{2}\right)
$$
The GLS estimate of $\beta$ minimizes:
$$
(\mathbf{y}-\mathbf{X} \beta)^{\top} \Sigma^{-1}(\mathbf{y}-\mathbf{X} \beta)=\sum_{i=1}^{n} \frac{\left(y_{i}-\mathbf{x}_{i}^{\top} \beta\right)^{2}}{\sigma_{i}^{2}}
$$
This problem is known as the Weighted Least-Squares (WLS).\\
Note that the errors are weighted by
$$
w_{i}=\frac{1}{\sigma_{i}^{2}}
$$
smaller weights for samples with larger variances.

\subsubsection{WLS special example : Replicated Observations}
Suppose we collected multiple observations for each $\mathbf{x}_{i}$. We use double subscripts to indicate the replicate observations:
$$
\left(\mathbf{x}_{i}, y_{i 1}, y_{i 2}, \ldots, y_{i n_{i}}\right)
$$
Let $y_{i}$ denote the average of the $n_{i}$ observations sharing $\mathbf{x}_{i}$. Then the residual sum of squares for $\beta$ equals
$$
\sum_{i=1}^{n} \sum_{j=1}^{n_{i}}\left(y_{i j}-\mathbf{x}_{i}^{\top} \beta\right)^{2}=\sum_{i=1}^{n} n_{i}\left(y_{i}-\mathbf{x}_{i}^{\top} \beta\right)^{2}+\sum_{i=1}^{n} \sum_{j=1}^{n_{i}}\left(y_{i j}-y_{i}\right)^{2}
$$
Minimizing the $RSS$ to solve for $\beta$ is the same as minimizing the ﬁrst term on the right only.
$$\hat{\beta}=\argmin_\beta \sum_{i=1}^n n_i(y_i-\mathbf{x}_i^T\beta)^2$$

\subsubsection{Method 2: Likelihood Estimation}
Model: $\mathbf{y} \sim N_{n}(\mathbf{X} \beta, \Sigma)$\\
Log-likelihood:
$$
\begin{aligned}
&\log (p(\mathbf{y} \mid \beta, \Sigma)) \\
&\quad=\log \left\{\frac{1}{(2 \pi)^{n / 2}|\Sigma|^{1 / 2}} \exp \left[-\frac{1}{2}(\mathbf{y}-\mathbf{X} \beta)^{\top} \Sigma^{-1}(\mathbf{y}-\mathbf{X} \beta)\right]\right\} \\
&\quad=-\frac{1}{2}(\mathbf{y}-\mathbf{X} \beta)^{\top} \boldsymbol{\Sigma}^{-1}(\mathbf{y}-\mathbf{X} \beta)+\text { Constant. }
\end{aligned}
$$
Therefore the MLE is given by
$$
\hat{\beta}_{m l e}=\arg \min _{\beta}(\mathbf{y}-\mathbf{X} \beta)^{\top} \Sigma^{-1}(\mathbf{y}-\mathbf{X} \beta)
$$

\subsection{GLS, $\Sigma$ unknown}
\subsubsection{Method 1: Estimation of Variance ($r_i^2$)/Standard Deviation Function ($|r_i|$)}
$$\sigma_i^2=\mathbb{E}(\varepsilon_i^2)-(\mathbb{E}(\varepsilon_i))^2$$
Since we assume $E(\varepsilon_i)=0$, we have
$$\sigma_i^2=\mathbb{E}(\varepsilon_i^2)$$
Which implies $r_i^2$ is an estimator of $\sigma_i^2$; $|r_i|$ is an estimator of the standard deviation $\sigma_i$.\\

\textbf{Estimate Variance Function} $\hat{v_i}(x)$

1. Fit a regression model using OLS, and obtain the residuals $r_i$.\\
2. Regress the squared residuals $r_i^2$ against the appropriate predictor variables.\\
Denote $\hat{v_i}$ be the ﬁtted value from variance function
$$w_i=\frac{1}{\hat{v_i}}$$

\textbf{Estimate Standard Deviation Function} $\hat{s_i}(x)$

1. Fit a regression model using OLS, and obtain the residuals $r_i$.\\
2. Regress the absolute residuals $|r_i|$ against the appropriate predictor variables.\\
Denote $\hat{s_i}$ be the ﬁtted value from standard deviation function
$$w_i=\frac{1}{(\hat{s_i})^2}$$
The estimated variances are then placed in the variance-covariance matrix $\Sigma$ and the regression coeﬃcients are estimated using the WLS (Weighted Least Squares method).

\subsubsection{Method 2: iterative approach}
1. Start with some initial guess of $\Sigma$\\
2. Use $\Sigma$ to estimate $\beta$\\
3. Use residuals (since we have known $\beta$) to estimate $\Sigma$\\
4. Iterate until convergence.\\

It looks like a good idea; however the methods will not work if we do not assume some structure about $\Sigma$ (too many parameters to be estimated).

Based on the application, we can assume a particular structure for $\Sigma$ that does not involve too many parameters.\\
Then, we can model $\beta$ and $\Sigma$ simultaneously.\\
For example , for AR(1) times series (auto-regressive model of order 1),
the structure of $\Sigma$ would be:
$$\Sigma=\sigma^2 \begin{pmatrix}
    1& \rho& \rho^2& \rho^3& \cdots\\
    \rho& 1& \rho& \rho^2& \cdots \\
    \cdots & \cdots& \cdots &\cdots&\cdots \\
    \rho^{n-1}& \rho^{n-2}&\cdots&\cdots&1
\end{pmatrix}$$
$\Sigma$ as a function of $\rho$ and $\sigma^2$.

\section{Lack of Fit Tests}
\subsection{Gaussian Assumption}
Gaussian Assumption, which can be summarized concisely as:
$$y\sim \mathcal{N}_{n}(\mathbf{X}\beta, \sigma^2 \mathbf{I})$$
Under these assumptions:
\begin{equation}
    \begin{aligned}
        \hat{\beta}&=\left(\mathbf{X}^{T} \mathbf{X}\right)^{-1} \mathbf{X}^{T} y\sim \mathcal{N}_{p}(\beta,\sigma^2\left(\mathbf{X}^{T} \mathbf{X}\right)^{-1}),\\
        \hat{y}&=\mathbf{X}\hat{\beta}\sim \mathcal{N}_{n}(\mathbf{X}\beta, \sigma^2 \mathbf{H})
    \end{aligned}
    \nonumber
\end{equation}
independently,
$$\hat{\sigma}^2=\frac{RSS}{n-p}=\frac{||y-\hat{y}||^2}{n-p}\sim \sigma^2\frac{\chi^2_{n-p}}{n-p}$$

\subsection{When $\sigma^2$ is known}
Intuition:\\
If the model is correct, then $\hat{\sigma}^2$ is an unbiased estimate of $\sigma^2$.\\
If we know $\sigma^2$, we could construct a test based on the ratio $\frac{\hat{\sigma}^2}{\sigma^2}$, a measure of \textit{lack-of-ﬁt}.

In this case we want to test the hypothesis:
$$
\left\{\begin{array}{l}
H_{0}: \text { There is no lack of fit. } \\
H_{\alpha}: \text { There is lack of fit. }
\end{array}\right.
$$
We use the test statistic:
$$
\frac{\hat{\sigma}^{2}}{\sigma^{2}}=\frac{R S S /(n-p)}{\sigma^{2}} \sim \frac{\chi_{n-p}^{2}}{n-p}
$$
Lack of fit means the error variance is large related to the value of $\sigma^{2}$, i.e., the test statistic is large.\\
Conclude that there is lack of fit (i.e. Reject $\left.H_{0}\right)$, if:
$$
(n-p) \frac{\hat{\sigma}^{2}}{\sigma^{2}} \geq \chi_{n-p}^{2}(1-\alpha)
$$

\subsection{When $\sigma^2$ is unknown}
\subsubsection{Hypothesis}
If $\sigma^{2}$ is unknown, a general approach is to compare an estimate of $\sigma^{2}$ based on a \underline{much bigger/general model}.\\
If we can derive the distribution (under $\mathrm{H}_{0}$ ) of $\hat{\sigma}_{\text {LinearModel }}^{2} / \hat{\sigma}_{\text {BigModel }}^{2}$, then we reduce this problem to a two model comparison test problem.\\
The null hypothesis is the current model:\\
$H_{0}: \mathbb{E}\left(y_{i}\right)=\mathbf{x}_{i}^{\top} \beta, \quad i=1,2, \ldots, n, \quad$ for some vector $\beta$\\
The more general model is assumed under the alternative hypothesis:\\
$H_{\alpha}: \mathbb{E}\left(y_{i}\right)=f\left(\mathbf{x}_{i}\right), \quad i=1,2, \ldots, n$, \quad for some function $f$\\

\subsubsection{Under the null hypothesis $H_{0}$}
$y_{i j}=\mathbf{x}_{i}^{\top} \beta+\varepsilon_{i j}$, some $\beta, \varepsilon_{i j} \sim$ iid $\mathcal{N}\left(0, \sigma^{2}\right)$\\
$R S S_{0}$ with $d f=n-p$
\subsubsection{Under the alternative big-model hypothesis $H_{\alpha}$ :}
$y_{i j}=f\left(\mathbf{x}_{i}\right)+\varepsilon_{i j}$, some function $f, \varepsilon_{i j} \sim$ iid $\mathcal{N}\left(0, \sigma^{2}\right)$\\
\textbf{Can we estimate $\sigma^{2}$ for the big model in $H_{\alpha}$ ?}\\
- The answer is yes, if there is some replication in the data, i.e., there are multiple observations (replicates) for some (at least) of the same $\mathbf{x}_{i}$ values.\\
- Schematically we can represent these replicates as:
$$
\left(\mathbf{x}_{i}, y_{i 1}, y_{i 2}, \ldots, y_{i n_{i}}\right), \quad i=1: m, \quad n=\sum_{i} n_{i}
$$
$R S S_{a}$ with $d f=n-m=\sum_{i}\left(n_{i}-1\right)$, where
$$
R S S_{a}=\sum_{i=1}^{m} \sum_{j=1}^{n_{i}}\left(y_{i j}-\bar{y}_{i .}\right)^{2}
$$
\subsubsection{$F-$Test}
All of the degrees of freedom for $R S S_{a}$ come from the replications. Therefore, with replication we can do an $\mathrm{F}$ test for lack of fit:
$$
F=\frac{\left(R S S_{0}-R S S_{a}\right) /(m-p)}{R S S_{a} /(n-m)} \sim F_{m-p, n-m}
$$

\section{Polynomials Regression}
\subsection{Basic Function}
\begin{equation}
    \begin{aligned}
        y_i&=f(x_i)+\varepsilon_i\\
        y_i&=\beta_0+\sum_{j=1}^db_j(x_i)\beta_j+\varepsilon_i\\
        y_i&=\beta_0+\sum_{j=1}^d\beta_j
        x_i^j+\varepsilon_i\\
    \end{aligned}
    \nonumber
\end{equation}
$d$ is the \textbf{degree of the polynomial component}.\\
\subsection{Choose Order $d$}
1.\textbf{Forward Approach}: Keep adding terms until the last added term is not signiﬁcant.\\
2.\textbf{Backward Approach}: Start with a large $d$, and keep eliminating the terms that are not statistically signiﬁcant, starting with the highest order term.\\

Once we pick up a $d$, we do not test the signiﬁcance of the lower-order terms. (include all the lower-order terms in our model by default)\\
\textbf{Reasoning}: we do not want our results to be affected by a change of location/scale of the data. $(z_i-2)^2=z_i^2-4z_i+4$.\\
\textbf{Exception}: particular polynomial function (physics law).


\subsection{Orthogonal Polynomials}
Successive predictors $x^j$ are \textbf{highly correlated} introducing multicollinearity problems.
\begin{equation}
    \begin{aligned}
        y_i=\beta_0+\beta_1 \mathbf{z}_i+...+\beta_d \mathbf{z}_d+\varepsilon_i
    \end{aligned}
    \nonumber
\end{equation}
where $\mathbf{z}_j=a_1+b_2x+...+\kappa_j x^j$ is a polynomial of order $j$ with coefficients chosen such that $\mathbf{z}_j^T\mathbf{z}_j=0$

\subsection{Piece-wise Polynomials}
If the true mean of $E(Y | X = x) = f (x)$ is too wiggly, we might need to ﬁt a higher order polynomial, which is not always a good idea.\\

Instead we will consider \textbf{piece-wise polynomials}:

1.we divide the range of $x$ into several intervals, and

2.within each interval $f (x)$ is a low-order polynomial, e.g., cubic or quadratic, but the polynomial coeﬃcients change from interval to interval;

3.in addition we require the overall $f (x)$ to be continuous up to certain derivatives.

\subsection{Cubic Splines}
\textbf{Polynomials}: smooth, but each point affects the fit globally.\\
\textbf{Piece-wise Polynomials}: localizes the influence of each data point, but are not smooth enough.\\
\textbf{Splines}: combines the beneficail aspects of both approaches.\\

A \textbf{Cubic Spline} is a curve constructed from sections of cubic polynomials, joined together so that the curve is \textit{continuous up to second derivative}.\\
The points at which the sections join are called the \textbf{knots} of the spline.\\

We want to define a cubic spline function in the interval $[a, b]$\\
- Define $m$ knots such that: $a<\xi_{1}<\xi_{2}<\ldots<\xi_{m}<b$\\
- A function $g$ defined on $[a, b]$ is a cubic spline with respect to knots $\left\{\xi_{i}\right\}_{i=1}^{m}$ if:\\
1. $g$ is a cubic polynomial in each of the $m+1$ intervals,
$$
g(x)=d_{i} x^{3}+c_{i} x^{2}+b_{i} x+a_{i}, \quad x \in\left[\xi_{i}, \xi_{i+1}\right]
$$
where $i=0, \ldots, m, \xi_{0}=a$ and $\xi_{m+1}=b$\\
2. $g$ is continuous up to the 2 nd derivative: since $g$ is continuous up to the 2nd derivative for any point inside an interval, it suffices to check the following conditions:
$$
g^{(0,1,2)}\left(\xi_{i}^{+}\right)=g^{(0,1,2)}\left(\xi_{i}^{-}\right), \quad i=1: m
$$
This expression indicates that the function and the first and second order derivatives are continuous at the knots.



\end{document}