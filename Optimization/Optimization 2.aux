\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Unconstrained Optimization}{7}{section.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Basic Definitions}{7}{subsection.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.1.1}Optimization in a Set}{7}{subsubsection.1.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.1.2}Minimizer}{7}{subsubsection.1.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.1.3}Stationary Point, Saddle Point}{7}{subsubsection.1.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.1.4} Conditions for Global Minimizer: (1) exists global-minimizer; (2) has the minimum value in all stationary points}{8}{subsubsection.1.1.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Special Situation: Optimization in $\mathbb  {R}$}{8}{subsection.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.2.1} Necessary condition of local-min: $f'(x^*)=0$}{8}{subsubsection.1.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.2.2} Sufficient condition of local-min: $f'(x^*)=0, f''(x^*)\geq 0$}{8}{subsubsection.1.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.2.3} Sufficient condition of global-min: $f'(x^*)=0$ and $f''(x)\geq 0,\forall x\in I$}{8}{subsubsection.1.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3} Restriction to a Line}{9}{subsection.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.3.1} Definition: $\phi _{\vec  {u}}(t)=f(\vec  {x}+t\vec  {u}),\ \vec  {x},\vec  {u}\in \mathbb  {R}^n, t\in \mathbb  {R}$}{9}{subsubsection.1.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.3.2}Derivatives: $\phi '_{\vec  {u}}(t)=\nabla f(\vec  {x}+t\vec  {u})\vec  {u}$, $\phi ^{''}_{\vec  {u}} (t)=\vec  {u}^T {Hf}(\vec  {x}+t\vec  {u})\vec  {u}$}{9}{subsubsection.1.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.3.3}Lemma: $x^*$ is a global minimizer of $f$ $\Leftrightarrow $ $t=0$ is the global minimizer of $\phi _{\vec  {u}}(t)=f(\vec  {x}+t\vec  {u})$, $\forall \vec  {u}\in \mathbb  {R}^n$}{10}{subsubsection.1.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4}General: Optimization in $\mathbb  {R}^n$}{10}{subsection.1.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.4.1}Local-min Necessary Condition $1$: $\nabla f$ is continuous, $x^*$ is a local minimizer $\Rightarrow \nabla f(x^*)=0$}{10}{subsubsection.1.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.4.2}Local-min Necessary Condition $2$: $Hf$ is continuous, $x^*$ is a local minimizer $\Rightarrow \nabla ^2 f(x^*)\succeq 0$}{11}{subsubsection.1.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.4.3}Local-min Sufficient Condition $1$: $Hf$ is continuous, $\nabla f(\vec  {x}^*)=0$, $\vec  {u}^T Hf(\vec  {x}) \vec  {u}\geq 0,\forall \vec  {u}\in \mathbb  {R}^n$ and $\exists r>0, \|\vec  {x}-\vec  {x}^*\|<r$ $\Rightarrow $ $\vec  {x}^*$ is a local minimizer}{12}{subsubsection.1.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.4.4}Local-min Sufficient Condition $1'$: $Hf$ is continuous, $\nabla f(\vec  {x}^*)=0$, $\nabla ^2 f(\vec  {x}^*)\succ 0$ $\Rightarrow $ $\vec  {x}^*$ is a local minimizer}{12}{subsubsection.1.4.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.4.5}Global-min Sufficient Condition: $Hf$ is continuous, $\nabla f(\vec  {x}^*)=0$, $\nabla ^2f(\vec  {x})\succeq 0,\forall \vec  {x}$ $\Rightarrow $ $\vec  {x}^*$ is a global minimizer}{12}{subsubsection.1.4.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.4.6}$Hf(\vec  {x}^*)$ is indefinite $\Rightarrow $ $\vec  {x}^*$ is saddle point}{13}{subsubsection.1.4.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.4.7}$Hf(\vec  {x}^*)\succ 0$/$\prec 0$ $\Rightarrow $ critical point $\vec  {x}^*$ is strictly local-min/local-max}{13}{subsubsection.1.4.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.4.8}Steps to Find Minimum in $\mathbb  {R}^n$}{13}{subsubsection.1.4.8}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.5}Existence of Global-min}{14}{subsection.1.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.5.1}(Bolzano-)Weierstrass Theorem: Compact set $X$ $\Rightarrow $ $\exists $ global-min/max}{14}{subsubsection.1.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.5.2}Coercive function $f$ $\Rightarrow $ $\exists $ global-min}{14}{subsubsection.1.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.5.3}Method of finding-global-min-among-stationary-points (FGMSP)}{15}{subsubsection.1.5.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Convexity}{15}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Convex Set}{15}{subsection.2.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Convex Set}}{15}{figure.1}\protected@file@percent }
\newlabel{}{{1}{15}{Convex Set}{figure.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.1}Prop: convex sets $e_1,e_2,...e_n$, then $\cap _{i=1}^ne_i$ is convex}{16}{subsubsection.2.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Convex Hull}{16}{subsection.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.1}Convex Hull $conv(S)$ is the set of all convex combinations of points in $S$}{16}{subsubsection.2.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.2}Theorem: convex set $S$, convex combination $\lambda _1 x_1+\cdots  +\lambda _k x_k\in S$, $\forall x_1,...,x_k\in S$}{16}{subsubsection.2.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.3}Corollary: $conv(S)$ is the smallest convex set containing $S$}{16}{subsubsection.2.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Convex Function}{17}{subsection.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.1}Definition: $f$ is convex $\Leftrightarrow $ $f(\alpha x+(1-\alpha ) y) \leq \alpha f(x)+(1-\alpha ) f(y), \forall x, y \in C, \forall \alpha \in [0,1]$}{17}{subsubsection.2.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.2}First-order: $f$ is convex $\Leftrightarrow $ $f(z) \geq f(x)+(z-x)^{T} \nabla f(x), \forall x, z \in C$}{17}{subsubsection.2.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.3}Second-order: $f$ is convex $\Leftrightarrow $ $\nabla ^{2} f(x) \succeq 0,\ \forall x \in C$}{18}{subsubsection.2.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.4}Sufficient Condition of Strictly Convex: $\nabla ^{2} f(x) \succ 0$}{18}{subsubsection.2.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.5}Prop: Max and Linear combination of convex functions are also convex}{18}{subsubsection.2.3.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Lemma: function $f$ is a convex function iff $\phi (t)=f(\vec  {x}+t\vec  {u})$ is convex of $t$}{19}{subsection.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5}Proposition: Convex function $f$, $\nabla f(x^*)=0$ $\Rightarrow $ global-min}{19}{subsection.2.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6}Application: Unconstrained Quadratic Optimization}{20}{subsection.2.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.7}Theorem: If $f$ is convex and $g$ is convex and increasing, $(g\cdot f)(x)$ is convex}{22}{subsection.2.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.8}Corollary: $f$ is linear, $g$ is convex (not necessarily increasing) $\Rightarrow $ $g\cdot f$ is convex}{23}{subsection.2.8}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.9}Epigraph and Jensen's Inequality}{23}{subsection.2.9}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.9.1}Def: epigraph ${epi}(f)=\{(x,y)\in C\times \mathbb  {R}:y\geq f(x)\}$}{23}{subsubsection.2.9.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.9.2}Lemma: $f$ is convex function $\Leftrightarrow $ ${epi}(f)$ is a convex set}{23}{subsubsection.2.9.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.9.3}Jensen's Inequality: $f(\DOTSB \sum@ \slimits@ _{i=1}^k\lambda _i x_i)\leq \DOTSB \sum@ \slimits@ _{i=1}^k\lambda _if(x_i)$}{24}{subsubsection.2.9.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3}Geometric Program (GP)}{24}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Arithmetic Mean-Geometric Mean Inequality (A-G inequality) $\delta _1x_2+\delta _2x_2+\cdots  +\delta _nx_n\geq x_1^{\delta _1}x_2^{\delta _2}\cdots  x_n^{\delta _n}$}{24}{subsection.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Unconstrained Geometric Programs}{25}{subsection.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.1}Def: Posynomial}{25}{subsubsection.3.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.2}General Strategy: A-G inequality}{25}{subsubsection.3.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.3}Dual of the Unconstrained GP}{26}{subsubsection.3.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4}Strongly Convexity}{28}{section.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}$\mu $-Strongly Convex: $ \langle \nabla f(w)-\nabla f(v), w-v\rangle \geq \mu \|w-v\|^{2}$}{28}{subsection.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}$\mu $-strongly convex $\Leftrightarrow \nabla ^{2} f(x) \succeq \mu I\Leftrightarrow $"$f(x)-\frac  {m}{2}\|x\|^2$ is convex"}{28}{subsection.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Lemma: Strongly convexity $\Rightarrow $ Strictly convexity}{28}{subsection.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Lemma: $\nabla ^2 f(x)\succeq mI$ $\Rightarrow $ $f(y)\geq f(x)+\nabla f(x)^T(y-x)+\frac  {m}{2}\|y-x\|^2$}{29}{subsection.4.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5}Lipschitz Gradient ($L$-Smooth)}{29}{section.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Theorem: $-MI\preceq \nabla ^2 f(x)\preceq MI$ $\Rightarrow $ $f$ is $M$-smooth}{30}{subsection.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Descent Lemma: $f$ is $L$-smooth $\Rightarrow $ $f(y)\leq f(x)+\nabla f(x)^T(y-x)+\frac  {L}{2}\|y-x\|^2$}{30}{subsection.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Co-coercivity Condition: $(\nabla f(x)-\nabla f(y))^T(x-y)\geq \frac  {1}{L}\|\nabla f(x)-\nabla f(y)\|^2$}{31}{subsection.5.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6}Gradient Methods}{32}{section.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Steepest Descent}{32}{subsection.6.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}Methods for Choosing Step Size $\alpha _k$}{33}{subsection.6.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3}Algorithm Convergence}{35}{subsection.6.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4}Convergence of The Steepest Descent with Fixed Step Size}{35}{subsection.6.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.4.1}Theorem: $f$ is $L$-smooth $\Rightarrow $ $\{x_k\}$ converges to stationary point}{35}{subsubsection.6.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.4.2}Theorem: $f$ is convex and $L$-smooth $\Rightarrow $ $f(x_k)$ converges to global-min value with rate $\frac  {1}{k}$}{37}{subsubsection.6.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.4.3}Theorem: $f$ is strongly convex and $L-$smooth $\Rightarrow $ $\{x_k\}$ converges to global-min geometrically}{39}{subsubsection.6.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.5}Convergence of Gradient Descent on Smooth Strongly-Convex Functions}{41}{subsection.6.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.6}From convergence rate to iteration complexity}{44}{subsection.6.6}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {7}Newton's Method}{45}{section.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1}Generalization to Optimization}{46}{subsection.7.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2}A New Interpretation of Newton’s Method}{46}{subsection.7.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.3}Convergence of Newton's Method}{46}{subsection.7.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.4}Note: Cons and Pros}{48}{subsection.7.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.5}Modifications to ensure global convergence}{49}{subsection.7.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.6}Quasi-Newton Methods}{49}{subsection.7.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.6.1}BFGS Method}{50}{subsubsection.7.6.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.7}Trust-Region Method}{51}{subsection.7.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.8}Cubic Regularization}{51}{subsection.7.8}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {8}Neural Networks}{52}{section.8}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.1}Basics}{52}{subsection.8.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Neuron Examples}}{52}{figure.2}\protected@file@percent }
\newlabel{Neuron}{{2}{52}{Neuron Examples}{figure.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Simple Neural Network}}{53}{figure.3}\protected@file@percent }
\newlabel{}{{3}{53}{Simple Neural Network}{figure.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.2}Multilayer Neural Network}{53}{subsection.8.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Multilayer Neural Network}}{53}{figure.4}\protected@file@percent }
\newlabel{}{{4}{53}{Multilayer Neural Network}{figure.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.3}A Simple Example of Back Propagation Algorithm}{54}{subsection.8.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Two Independent Pathways}}{55}{figure.5}\protected@file@percent }
\newlabel{}{{5}{55}{Two Independent Pathways}{figure.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.4}Back Propagation Algorithm}{55}{subsection.8.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.5}Other Methods}{57}{subsection.8.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {9}Constrained Optimization and Gradient Projection}{57}{section.9}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {9.1}Constrained Optimization: Basic}{57}{subsection.9.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {9.1.1}Def: Optimality}{57}{subsubsection.9.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {9.1.2}Prop: local-min$\Rightarrow \nabla f(x^*)^T(x-x^*)\geq 0,\forall x\in \&$ $\Leftrightarrow $ global-min in convex }{58}{subsubsection.9.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {9.1.3}Def: Interior Point}{58}{subsubsection.9.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {9.2}Constrained Optimization Example}{59}{subsection.9.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {9.3}Projection onto Closed Convex Set}{59}{subsection.9.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {9.3.1}Def: Projection $[z]^\&$}{59}{subsubsection.9.3.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Projection onto Closed Convex Set}}{60}{figure.6}\protected@file@percent }
\newlabel{}{{6}{60}{Projection onto Closed Convex Set}{figure.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Projection onto Closed non-Convex Set}}{60}{figure.7}\protected@file@percent }
\newlabel{}{{7}{60}{Projection onto Closed non-Convex Set}{figure.7}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {9.3.2}Prop: \underline  {unique} projection $[z]^\&$ on \underline  {closed convex} subset of $\mathbb  {R}^n$}{60}{subsubsection.9.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {9.3.3}Projection Theorem: $x=[z]^\&$ is projection on \underline  {closed conex} subset of $\mathbb  {R}^n$$\Leftrightarrow $ $(z-x)^T(y-x)\leq 0, \forall y\in \&$}{61}{subsubsection.9.3.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Necessary and Sufficient Condition for Projection}}{61}{figure.8}\protected@file@percent }
\newlabel{}{{8}{61}{Necessary and Sufficient Condition for Projection}{figure.8}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {9.3.4}Prop: Projection is non-expansive $\|[x]^\&-[z]^\&\|\leq \|x-z\|,\forall x,z\in \mathbb  {R}^n$}{61}{subsubsection.9.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {9.4}Projection on (Linear) Subspaces of $\mathbb  {R}^n$}{62}{subsection.9.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {9.4.1}Orthogonality Principle in subspaces of $\mathbb  {R}^n$: $(z-y^*)^Tx= 0,\forall x\in \&$}{62}{subsubsection.9.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {9.5}Gradient Projection Method}{62}{subsection.9.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Point from $\mathbb  {R}^2$ to $\mathbb  {R}$}}{63}{figure.9}\protected@file@percent }
\newlabel{}{{9}{63}{Point from $\mathbb {R}^2$ to $\mathbb {R}$}{figure.9}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {9.5.1}Def: \underline  {fixed point} in fixed step-size steepest descent method, $\tilde  {x}=[\tilde  {x}-\alpha \nabla f(\tilde  {x})]^\&$}{63}{subsubsection.9.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {9.5.2}Prop: $L-$smooth, $0<\alpha <\frac  {2}{L}$ $\Rightarrow $ limit point is a fixed point (in fixed step-size steepest descent method)}{63}{subsubsection.9.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {9.5.3}Prop: $x$ is minimizer in convex func $\Leftrightarrow $ fixed point (in fixed step-size steepest descent method)}{64}{subsubsection.9.5.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {9.5.4}Thm: Convergence of Gradient Projection: Convex, $L-$smooth, $0<\alpha <\frac  {2}{L}$ $\Rightarrow $ $f(x_k)\rightarrow f(x^*)$ at rate $\frac  {1}{k}$}{64}{subsubsection.9.5.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {9.5.5}Thm: Strongly convex, Lipschitz gradient $\Rightarrow $ $\{x_k\}$ converges to $x^*$ geometrically}{64}{subsubsection.9.5.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {10}Optimization with Equality Constraints}{65}{section.10}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {10.1}Basic}{65}{subsection.10.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {10.2}Lagrange Mutiplier Theorem}{66}{subsection.10.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {10.2.1}First-order necessary condition: $\exists \lambda , \nabla f(x^*)+\DOTSB \sum@ \slimits@ _{i=1}^m\lambda _i \nabla h_i(x^*)=0$}{66}{subsubsection.10.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {10.2.2}Second-order necessary condition: $z^T\left (\nabla ^2 f(x^*)+\DOTSB \sum@ \slimits@ _{i=1}^m\lambda _i \nabla ^2 h_i(x^*)\right )z\geq 0,\forall z\in V(x^*)$}{67}{subsubsection.10.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {10.2.3}Sufficient Condition: $\exists \lambda $ 1. $\nabla f(x^*)+\DOTSB \sum@ \slimits@ _{i=1}^m\lambda _i \nabla h_i(x^*)=0$ 2. $z^T\big  (\nabla ^2 f(x^*)+\DOTSB \sum@ \slimits@ _{i=1}^m\lambda _i \nabla ^2 h_i(x^*)\big  )z> 0,\forall z\in V(x^*),z\neq  0$}{68}{subsubsection.10.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {10.2.4}Lagrangian Function}{69}{subsubsection.10.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {10.2.5}Example}{69}{subsubsection.10.2.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {10.2.6}Sensitivity Analysis $f(x^*(u))=f(x^*)-\lambda ^Tu+O(\|u\|)$}{70}{subsubsection.10.2.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {10.2.7}Linear Constraints}{71}{subsubsection.10.2.7}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {11}Optimization with Inequality Constraints}{73}{section.11}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {11.1}Basic}{73}{subsection.11.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {11.1.1}Active vs. Inactive Inequality Constraints}{73}{subsubsection.11.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {11.1.2}ICP $\rightarrow $ ECP}{73}{subsubsection.11.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {11.1.3}Intuition $\mu _j\geq 0, \forall j\in A(x^*)$}{74}{subsubsection.11.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {11.1.4}Complementary Slackness}{74}{subsubsection.11.1.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {11.2}Karush–Kuhn–Tucker (KKT) Necessary Conditions}{74}{subsection.11.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {11.3}Karush–Kuhn–Tucker (KKT) Sufficient Conditions}{76}{subsection.11.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {11.4}General Sufficiency Condition}{79}{subsection.11.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {11.5}Barrier Method}{80}{subsection.11.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {11.6}An Exmaple Using KKT or Barrier}{81}{subsection.11.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {11.6.1}Solution using KKT conditions}{81}{subsubsection.11.6.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {11.6.2}Solution using logarithmic barrier}{82}{subsubsection.11.6.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {11.7}Penalty Method (For ECP)}{82}{subsection.11.7}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {12}Duality}{83}{section.12}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {12.1}Weak Duality Theorem: $\max _{(\lambda ,\mu )\in G}D(\lambda ,\mu )\leq \min _{x\in F}f(x)$}{84}{subsection.12.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {12.2}Strong Duality Theorem: under some conditions, $\max _{(\lambda ,\mu )\in G}D(\lambda ,\mu )= \min _{x\in F}f(x)$}{85}{subsection.12.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {12.2.1}Slater's sufficient condition for strong duality}{86}{subsubsection.12.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {12.2.2}Example}{87}{subsubsection.12.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {12.3}Dual of Linear Program}{87}{subsection.12.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {13}Augmented Lagrangian Method (adjusted penalty method)}{88}{section.13}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {13.1}Motivation}{88}{subsection.13.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {13.2}Augemented Lagrangian Method}{89}{subsection.13.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {13.2.1}Method of Multipliers}{90}{subsubsection.13.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {14}Sub-gradient Methods}{92}{section.14}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {14.1}Sub-gradient}{92}{subsection.14.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {14.2}Sub-differential}{93}{subsection.14.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {14.3}More examples}{93}{subsection.14.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {14.4}First-order necessary conditions for optimality in terms of subgradient}{95}{subsection.14.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {14.5}Properties of Subgradients}{95}{subsection.14.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {14.6}Sub-gradient Descent for Unconstrained Optimization}{95}{subsection.14.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {14.7}(Revised) Sub-gradient "descent" with diminishing stepsize}{97}{subsection.14.7}\protected@file@percent }
\gdef \@abspage@last{98}
