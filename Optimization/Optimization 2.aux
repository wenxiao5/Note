\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand*\HyPL@Entry[1]{}
\HyPL@Entry{0<</S/D>>}
\@writefile{toc}{\contentsline {section}{\numberline {1}Unconstrained Optimization}{4}{section.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Conditions for Optimality}{4}{subsection.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Global minimizer, Local minimizer}{4}{subsection.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}Optimization in $\mathbb  {R}$}{4}{subsection.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.3.1}Theorem: local minimizer $\Rightarrow f'(x^*)=0$}{4}{subsubsection.1.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.3.2}Theorem: $f'(x^*)=0, f''(x^*)\geq 0 \Rightarrow $ local minimizer}{4}{subsubsection.1.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4}Optimization in $\mathbb  {R}^n$}{4}{subsection.1.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.4.1}Necessary Conditions for Optimality: Local Extremum $\Rightarrow \nabla f(x^*)=0$}{4}{subsubsection.1.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.4.2}Stationary Point, Saddle Point}{5}{subsubsection.1.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.4.3}Second Order Necessary Condition}{6}{subsubsection.1.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.4.4}Sufficient Conditions for Optimality}{6}{subsubsection.1.4.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.4.5}Using Optimality Conditions to Find Minimum}{7}{subsubsection.1.4.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.4.6}Fix Conditions for Global Optimality}{7}{subsubsection.1.4.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.5}Optimization in a Set}{8}{subsection.1.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.5.1}Existence of Global-min}{8}{subsubsection.1.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.6}Method of finding-global-min-among-stationary-points (FGMSP)}{9}{subsection.1.6}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Convexity}{9}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Definition}{9}{subsection.2.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces }}{10}{figure.1}\protected@file@percent }
\newlabel{}{{1}{10}{}{figure.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Convex$\Rightarrow $Stationary point is global-min}{11}{subsection.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Unconstrained Quadratic Optimization}{11}{subsection.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Strongly Convexity}{13}{subsection.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.4.1}$\mu $-Strongly Convex: $ \langle \nabla f(w)-\nabla f(v), w-v\rangle \geq \mu \|w-v\|^{2}$}{13}{subsubsection.2.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.4.2}$\mu $-strongly convex $\Leftrightarrow \nabla ^{2} f(x) \succeq \mu I$}{13}{subsubsection.2.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.4.3}Lemma: Strongly convexity $\Rightarrow $ Strictly convexity}{13}{subsubsection.2.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.4.4}Lemma: $\nabla ^2 f(x)\succeq mI$ $\Rightarrow $ $f(y)\geq f(x)+\nabla f(x)^T(y-x)+\frac  {m}{2}\|y-x\|^2$}{13}{subsubsection.2.4.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3}Gradient Methods}{14}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Steepest Descent}{14}{subsection.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Methods for Choosing $\alpha _k$}{15}{subsection.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Optimal(Exact) Line Search}{15}{subsection.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Armijo's Rule}{15}{subsection.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5}Armijo's Rule for Steepest Descent}{16}{subsection.3.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4}Convergence of GD with Constant Stepsize}{17}{section.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Lipschitz Gradient ($L$-Smooth)}{17}{subsection.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.1}Theorem: $-MI\preceq \nabla ^2 f(x)\preceq MI$ $\Rightarrow $ $\nabla f(x)$ is Lipschitz with constant $M$}{17}{subsubsection.4.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.2}Descent Lemma: $\nabla f(x)$ is Lipschitz with constant $L$ $\Rightarrow f(y)\leq f(x)+\nabla f(x)^T(y-x)+\frac  {L}{2}\|y-x\|^2$}{18}{subsubsection.4.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.3}Co-coercivity Condition: $(\nabla f(x)-\nabla f(y))^T(x-y)\geq \frac  {1}{L}\|\nabla f(x)-\nabla f(y)\|^2$}{19}{subsubsection.4.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Convergence of Steepest Descent with Fixed Stepsize}{19}{subsection.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.1}Theorem: $f$ has Lipschitz gradient $\Rightarrow $ $\{x_k\}$ converges to stationary point}{19}{subsubsection.4.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Convergence of GD for convex functions}{21}{subsection.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.1}Theorem: $f$ is convex and has Lipschitz gradient $\Rightarrow $ $f(x_k)$ converges to global-min value with rate $\frac  {1}{k}$}{21}{subsubsection.4.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Convergence of GD for strongly convex functions}{22}{subsection.4.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.4.1}Theorem: Strongly convex, Lipschitz gradient $\Rightarrow $ $\{x_k\}$ converges to global-min geometrically}{22}{subsubsection.4.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.4.2}Example}{24}{subsubsection.4.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5}Convergence of Gradient Descent on Smooth Strongly-Convex Functions}{24}{subsection.4.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.6}From convergence rate to iteration complexity}{27}{subsection.4.6}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5}Newton's Method}{28}{section.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Generalization to Optimization}{28}{subsection.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}A New Interpretation of Newtonâ€™s Method}{29}{subsection.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Convergence of Newton's Method}{29}{subsection.5.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4}Note: Cons and Pros}{30}{subsection.5.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5}Modifications to ensure global convergence}{31}{subsection.5.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.6}Quasi-Newton Methods}{31}{subsection.5.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.6.1}BFGS Method}{32}{subsubsection.5.6.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.7}Trust-Region Method}{33}{subsection.5.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.8}Cubic Regularization}{34}{subsection.5.8}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6}Neural Networks}{34}{section.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Neuron}{34}{subsection.6.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Neuron Examples}}{34}{figure.2}\protected@file@percent }
\newlabel{Neuron}{{2}{34}{Neuron Examples}{figure.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Vector Input}}{35}{figure.3}\protected@file@percent }
\newlabel{}{{3}{35}{Vector Input}{figure.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Multilayer Neural Network}}{35}{figure.4}\protected@file@percent }
\newlabel{}{{4}{35}{Multilayer Neural Network}{figure.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}Multilayer Neural Network}{35}{subsection.6.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3}Back Propagation Algorithm}{36}{subsection.6.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4}Other Methods}{37}{subsection.6.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {7}Constrained Optimization and Gradient Projection}{38}{section.7}\protected@file@percent }
\gdef \@abspage@last{38}
