\contentsline {section}{\numberline {1}Extreme Value and Coercive Functions}{8}{section.1}%
\contentsline {subsection}{\numberline {1.1}Bolzano-Weierstrass theorem}{8}{subsection.1.1}%
\contentsline {subsubsection}{\numberline {1.1.1}Sequence Convergence}{8}{subsubsection.1.1.1}%
\contentsline {subsubsection}{\numberline {1.1.2}Bolzano-Weierstrass Theorem: Compact set $S$, $\exists $ subsequence converges to $\vec {x}^*\in S$}{8}{subsubsection.1.1.2}%
\contentsline {subsubsection}{\numberline {1.1.3}Extreme Value Theorem: continuous $f$ compact set $\rightarrow \mathbb {R}$ has global-min}{8}{subsubsection.1.1.3}%
\contentsline {subsubsection}{\numberline {1.1.4}Corollary: Non-empty and Compact level set $\{x \mid f(x) \leq c\}$ $\Rightarrow $ $\exists $ $f$'s global-min/max}{8}{subsubsection.1.1.4}%
\contentsline {subsection}{\numberline {1.2}Coercive function}{9}{subsection.1.2}%
\contentsline {subsubsection}{\numberline {1.2.1}Def: Coercive function}{9}{subsubsection.1.2.1}%
\contentsline {subsubsection}{\numberline {1.2.2}Coercive function $f$ $\Rightarrow $ $\exists $ global-min}{9}{subsubsection.1.2.2}%
\contentsline {subsubsection}{\numberline {1.2.3}Lemma: $f: \mathbb {R}^n \rightarrow \mathbb {R}$ is coercive $\Leftrightarrow $ $\lim _{\|\vec {x}\| \rightarrow \infty }f(\vec {x})=+\infty $ for all possible directions.}{9}{subsubsection.1.2.3}%
\contentsline {subsubsection}{\numberline {1.2.4}Lemma: sum of \textit {coercive functions} is a coercive function}{9}{subsubsection.1.2.4}%
\contentsline {subsubsection}{\numberline {1.2.5}Lemma: \textit {coercive function} $+$ \textit {bounded below function} is a coercive function}{9}{subsubsection.1.2.5}%
\contentsline {subsubsection}{\numberline {1.2.6}Get coercive function: convex $f$, $f^\varepsilon (\vec {x})=f(\vec {x})+\varepsilon \|\vec {x}\|^2$ is coercive}{10}{subsubsection.1.2.6}%
\contentsline {subsection}{\numberline {1.3}Polynomials Coercive}{10}{subsection.1.3}%
\contentsline {subsubsection}{\numberline {1.3.1}Quadratic forms: $A_{n\times n}$ is positive definite $\Leftrightarrow $ quadratic form $f(\vec {x})=\vec {x}^TA \vec {x}$ is coercive}{10}{subsubsection.1.3.1}%
\contentsline {subsubsection}{\numberline {1.3.2}Higher-degree polynomials}{10}{subsubsection.1.3.2}%
\contentsline {section}{\numberline {2}Unconstrained Optimization}{11}{section.2}%
\contentsline {subsection}{\numberline {2.1}Basic Definitions}{11}{subsection.2.1}%
\contentsline {subsubsection}{\numberline {2.1.1}Optimization in a Set}{11}{subsubsection.2.1.1}%
\contentsline {subsubsection}{\numberline {2.1.2}Minimizer}{12}{subsubsection.2.1.2}%
\contentsline {subsubsection}{\numberline {2.1.3}Stationary Point, Saddle Point}{12}{subsubsection.2.1.3}%
\contentsline {subsubsection}{\numberline {2.1.4} Conditions for Global Minimizer: (1) exists global-minimizer; (2) has the minimum value in all stationary points}{12}{subsubsection.2.1.4}%
\contentsline {subsection}{\numberline {2.2}Special Situation: Optimization in $\mathbb {R}$}{13}{subsection.2.2}%
\contentsline {subsubsection}{\numberline {2.2.1} Necessary condition of local-min: $f'(x^*)=0$}{13}{subsubsection.2.2.1}%
\contentsline {subsubsection}{\numberline {2.2.2} Sufficient condition of local-min: $f'(x^*)=0, f''(x^*)\geq 0$}{13}{subsubsection.2.2.2}%
\contentsline {subsubsection}{\numberline {2.2.3} Sufficient condition of global-min: $f'(x^*)=0$ and $f''(x)\geq 0,\forall x\in I$}{13}{subsubsection.2.2.3}%
\contentsline {subsection}{\numberline {2.3} Restriction to a Line}{14}{subsection.2.3}%
\contentsline {subsubsection}{\numberline {2.3.1} Definition: $\phi _{\vec {u}}(t)=f(\vec {x}+t\vec {u}),\ \vec {x},\vec {u}\in \mathbb {R}^n, t\in \mathbb {R}$}{14}{subsubsection.2.3.1}%
\contentsline {subsubsection}{\numberline {2.3.2}Derivatives: $\phi '_{\vec {u}}(t)=\nabla f(\vec {x}+t\vec {u})\vec {u}$, $\phi ^{''}_{\vec {u}} (t)=\vec {u}^T {Hf}(\vec {x}+t\vec {u})\vec {u}$}{14}{subsubsection.2.3.2}%
\contentsline {subsubsection}{\numberline {2.3.3}Lemma: $x^*$ is a global minimizer of $f$ $\Leftrightarrow $ $t=0$ is the global minimizer of $\phi _{\vec {u}}(t)=f(\vec {x}+t\vec {u})$, $\forall \vec {u}\in \mathbb {R}^n$}{14}{subsubsection.2.3.3}%
\contentsline {subsection}{\numberline {2.4}General: Optimization in $\mathbb {R}^n$}{15}{subsection.2.4}%
\contentsline {subsubsection}{\numberline {2.4.1}Local-min Necessary Condition $1$: $\nabla f$ is continuous, $x^*$ is a local minimizer $\Rightarrow \nabla f(x^*)=0$}{15}{subsubsection.2.4.1}%
\contentsline {subsubsection}{\numberline {2.4.2}Local-min Necessary Condition $2$: $Hf$ is continuous, $x^*$ is a local minimizer $\Rightarrow \nabla ^2 f(x^*)\succeq 0$}{16}{subsubsection.2.4.2}%
\contentsline {subsubsection}{\numberline {2.4.3}Local-min Sufficient Condition $1$: $Hf$ is continuous, $\nabla f(\vec {x}^*)=0$, $\vec {u}^T Hf(\vec {x}) \vec {u}\geq 0,\forall \vec {u}\in \mathbb {R}^n$ and $\exists r>0, \|\vec {x}-\vec {x}^*\|<r$ $\Rightarrow $ $\vec {x}^*$ is a local minimizer}{16}{subsubsection.2.4.3}%
\contentsline {subsubsection}{\numberline {2.4.4}Local-min Sufficient Condition $1'$: $Hf$ is continuous, $\nabla f(\vec {x}^*)=0$, $\nabla ^2 f(\vec {x}^*)\succ 0$ $\Rightarrow $ $\vec {x}^*$ is a local minimizer}{16}{subsubsection.2.4.4}%
\contentsline {subsubsection}{\numberline {2.4.5}Global-min Sufficient Condition: $Hf$ is continuous, $\nabla f(\vec {x}^*)=0$, $\nabla ^2f(\vec {x})\succeq 0,\forall \vec {x}$ $\Rightarrow $ $\vec {x}^*$ is a global minimizer}{17}{subsubsection.2.4.5}%
\contentsline {subsubsection}{\numberline {2.4.6}$Hf(\vec {x}^*)$ is indefinite $\Rightarrow $ $\vec {x}^*$ is saddle point}{17}{subsubsection.2.4.6}%
\contentsline {subsubsection}{\numberline {2.4.7}$Hf(\vec {x}^*)\succ 0$/$\prec 0$ $\Rightarrow $ critical point $\vec {x}^*$ is strictly local-min/local-max}{18}{subsubsection.2.4.7}%
\contentsline {subsubsection}{\numberline {2.4.8}Steps to Find Minimum in $\mathbb {R}^n$}{18}{subsubsection.2.4.8}%
\contentsline {subsection}{\numberline {2.5}Existence of Global-min}{19}{subsection.2.5}%
\contentsline {subsubsection}{\numberline {2.5.1}(Bolzano-)Weierstrass Theorem: Compact set $X$ $\Rightarrow $ $\exists $ global-min/max}{19}{subsubsection.2.5.1}%
\contentsline {subsubsection}{\numberline {2.5.2}Def: Coercive function}{19}{subsubsection.2.5.2}%
\contentsline {subsubsection}{\numberline {2.5.3}Coercive function $f$ $\Rightarrow $ $\exists $ global-min}{19}{subsubsection.2.5.3}%
\contentsline {subsubsection}{\numberline {2.5.4}Method of finding-global-min-among-stationary-points (FGMSP)}{20}{subsubsection.2.5.4}%
\contentsline {section}{\numberline {3}Convexity}{20}{section.3}%
\contentsline {subsection}{\numberline {3.1}Convex Set}{20}{subsection.3.1}%
\contentsline {subsubsection}{\numberline {3.1.1}Prop: convex sets $e_1,e_2,...e_n$, then $\cap _{i=1}^ne_i$ is convex}{21}{subsubsection.3.1.1}%
\contentsline {subsection}{\numberline {3.2}Convex Hull}{21}{subsection.3.2}%
\contentsline {subsubsection}{\numberline {3.2.1}Convex Hull $conv(S)$ is the set of all convex combinations of points in $S$}{21}{subsubsection.3.2.1}%
\contentsline {subsubsection}{\numberline {3.2.2}Theorem: convex set $S$, convex combination $\lambda _1 x_1+\cdots +\lambda _k x_k\in S$, $\forall x_1,...,x_k\in S$}{21}{subsubsection.3.2.2}%
\contentsline {subsubsection}{\numberline {3.2.3}Corollary: $conv(S)$ is the smallest convex set containing $S$}{21}{subsubsection.3.2.3}%
\contentsline {subsection}{\numberline {3.3}Optimization over Convex Set}{22}{subsection.3.3}%
\contentsline {subsection}{\numberline {3.4}Convex Function}{22}{subsection.3.4}%
\contentsline {subsubsection}{\numberline {3.4.1}Definition: $f$ is convex $\Leftrightarrow $ $f(\alpha x+(1-\alpha ) y) \leq \alpha f(x)+(1-\alpha ) f(y), \forall x, y \in C, \forall \alpha \in [0,1]$}{22}{subsubsection.3.4.1}%
\contentsline {subsubsection}{\numberline {3.4.2}First-order: $f$ is convex $\Leftrightarrow $ $f(z) \geq f(x)+(z-x)^{T} \nabla f(x), \forall x, z \in C$}{22}{subsubsection.3.4.2}%
\contentsline {subsubsection}{\numberline {3.4.3}Prop: local-min$\Rightarrow \nabla f(x^*)^T(x-x^*)\geq 0,\forall x\in \&$ $\Leftrightarrow $ global-min in convex }{23}{subsubsection.3.4.3}%
\contentsline {subsubsection}{\numberline {3.4.4}Second-order: $f$ is convex $\Leftrightarrow $ $\nabla ^{2} f(x) \succeq 0,\ \forall x \in C$}{24}{subsubsection.3.4.4}%
\contentsline {subsubsection}{\numberline {3.4.5}Sufficient Condition of Strictly Convex: $\nabla ^{2} f(x) \succ 0$}{25}{subsubsection.3.4.5}%
\contentsline {subsubsection}{\numberline {3.4.6}Prop: Max and Linear combination of convex functions are also convex}{25}{subsubsection.3.4.6}%
\contentsline {subsection}{\numberline {3.5}Lemma: function $f$ is a convex function iff $\phi (t)=f(\vec {x}+t\vec {u})$ is convex of $t$}{26}{subsection.3.5}%
\contentsline {subsection}{\numberline {3.6}Proposition: Convex function $f$, $\nabla f(x^*)=0$ $\Rightarrow $ global-min}{26}{subsection.3.6}%
\contentsline {subsection}{\numberline {3.7}Application: Unconstrained Quadratic Optimization}{27}{subsection.3.7}%
\contentsline {subsection}{\numberline {3.8}Theorem: If $f$ is convex and $g$ is convex and increasing, $(g\cdot f)(x)$ is convex}{28}{subsection.3.8}%
\contentsline {subsection}{\numberline {3.9}Corollary: $f$ is linear, $g$ is convex (not necessarily increasing) $\Rightarrow $ $g\cdot f$ is convex}{29}{subsection.3.9}%
\contentsline {subsection}{\numberline {3.10}Epigraph and Jensen's Inequality}{30}{subsection.3.10}%
\contentsline {subsubsection}{\numberline {3.10.1}Def: epigraph ${epi}(f)=\{(x,y)\in C\times \mathbb {R}:y\geq f(x)\}$}{30}{subsubsection.3.10.1}%
\contentsline {subsubsection}{\numberline {3.10.2}Lemma: $f$ is convex function $\Leftrightarrow $ ${epi}(f)$ is a convex set}{30}{subsubsection.3.10.2}%
\contentsline {subsubsection}{\numberline {3.10.3}Jensen's Inequality: $f(\DOTSB \sum@ \slimits@ _{i=1}^k\lambda _i x_i)\leq \DOTSB \sum@ \slimits@ _{i=1}^k\lambda _if(x_i)$}{30}{subsubsection.3.10.3}%
\contentsline {subsection}{\numberline {3.11} Subgradients of Convex Functions}{31}{subsection.3.11}%
\contentsline {subsubsection}{\numberline {3.11.1}Sub-gradient $\vec {d}$: $f(\vec {x})\geq f(\vec {x}^*)+\vec {d}\cdot (\vec {x}-\vec {x}^*), \forall \vec {x}\in C$}{31}{subsubsection.3.11.1}%
\contentsline {subsubsection}{\numberline {3.11.2}Sub-differential: set of all sub-gradient}{32}{subsubsection.3.11.2}%
\contentsline {subsubsection}{\numberline {3.11.3}More examples}{33}{subsubsection.3.11.3}%
\contentsline {section}{\numberline {4}Geometric Programming (GP)}{34}{section.4}%
\contentsline {subsection}{\numberline {4.1}Arithmetic Mean-Geometric Mean Inequality (A-G inequality) $\delta _1x_2+\delta _2x_2+\cdots +\delta _nx_n\geq x_1^{\delta _1}x_2^{\delta _2}\cdots x_n^{\delta _n}$}{34}{subsection.4.1}%
\contentsline {subsection}{\numberline {4.2}Unconstrained Geometric Programs}{35}{subsection.4.2}%
\contentsline {subsubsection}{\numberline {4.2.1}Def: Posynomial}{35}{subsubsection.4.2.1}%
\contentsline {subsubsection}{\numberline {4.2.2}General Strategy: A-G inequality}{36}{subsubsection.4.2.2}%
\contentsline {subsubsection}{\numberline {4.2.3}Dual of the Unconstrained GP}{36}{subsubsection.4.2.3}%
\contentsline {section}{\numberline {5}Polynomial Interpolation}{38}{section.5}%
\contentsline {subsection}{\numberline {5.1}Method 1: $M\vec {a}=\vec {y}$}{38}{subsection.5.1}%
\contentsline {subsection}{\numberline {5.2}Method 2: Lagrange Interpolation Formula}{39}{subsection.5.2}%
\contentsline {subsection}{\numberline {5.3}Lines of Best Fit}{39}{subsection.5.3}%
\contentsline {subsection}{\numberline {5.4}Least-Square Problem (Overconstrainted $A \vec {x}= \vec {b}$)}{40}{subsection.5.4}%
\contentsline {subsubsection}{\numberline {5.4.1}Lemma: closest point $\Leftrightarrow $ $(A\vec {x}^*-\vec {y})\bot \vec {a},\ \forall \vec {a}\in V$}{40}{subsubsection.5.4.1}%
\contentsline {subsubsection}{\numberline {5.4.2}Theorem: $\vec {x}^*=(A^TA)^{-1}A^T\vec {y}=A^+ \vec {y}$}{40}{subsubsection.5.4.2}%
\contentsline {subsubsection}{\numberline {5.4.3}Def: Projection Matrix: $P=AA^+$; Projection of $\vec {y}$ on $V$: $A \vec {x}^*=P \vec {y}$}{41}{subsubsection.5.4.3}%
\contentsline {subsubsection}{\numberline {5.4.4}Special Case: Projection on vector ${Proj}_{\vec {a}}(\vec {y})=\frac {(\vec {a}\cdot \vec {y})\vec {a}}{\|\vec {a}\|^2}$}{41}{subsubsection.5.4.4}%
\contentsline {subsubsection}{\numberline {5.4.5}Theorem: Projection Matrix $=$ Sum of outer products of orthonormal basis}{41}{subsubsection.5.4.5}%
\contentsline {subsubsection}{\numberline {5.4.6}Corollary: $Q$ has orthonormal columns $\Rightarrow $ $\vec {x}^*=Q^T \vec {y}$. ($Q^+=Q^T$)}{42}{subsubsection.5.4.6}%
\contentsline {subsubsection}{\numberline {5.4.7}The Gram-Schmidt process}{42}{subsubsection.5.4.7}%
\contentsline {subsection}{\numberline {5.5}Minimum-norm problems (Underconstrainted $A \vec {x}= \vec {b}$)}{43}{subsection.5.5}%
\contentsline {subsubsection}{\numberline {5.5.1}Applying the least-squares technique}{43}{subsubsection.5.5.1}%
\contentsline {subsubsection}{\numberline {5.5.2}The short cut method}{45}{subsubsection.5.5.2}%
\contentsline {subsubsection}{\numberline {5.5.3}The short cut method with $H$-norm}{45}{subsubsection.5.5.3}%
\contentsline {section}{\numberline {6}The Closest Point: Projection}{47}{section.6}%
\contentsline {subsection}{\numberline {6.1}Set Theory Basis}{47}{subsection.6.1}%
\contentsline {subsection}{\numberline {6.2}Def: Projection $[z]^\&$}{47}{subsection.6.2}%
\contentsline {subsection}{\numberline {6.3}\underline {Unique} projection $[z]^\&$ on \underline {closed convex} subset of $\mathbb {R}^n$}{48}{subsection.6.3}%
\contentsline {subsection}{\numberline {6.4}Obtuse Angle Criterion: $[z]^\&$ is projection $\Leftrightarrow $ $(z-[z]^\&)^T(y-[z]^\&)\leq 0, \forall y\in \&$}{48}{subsection.6.4}%
\contentsline {subsubsection}{\numberline {6.4.1}Special Case (Linear subspace): Orthogonality Principle $(z-[z]^\&)^Tx= 0,\forall x\in \&$}{50}{subsubsection.6.4.1}%
\contentsline {subsection}{\numberline {6.5}Prop: Projection is Non-expansive $\|[x]^\&-[z]^\&\|\leq \|x-z\|,\forall x,z\in \mathbb {R}^n$}{50}{subsection.6.5}%
\contentsline {subsection}{\numberline {6.6}The Separation Theorem}{51}{subsection.6.6}%
\contentsline {subsection}{\numberline {6.7}Bolzano-Weierstrass theorem}{52}{subsection.6.7}%
\contentsline {subsubsection}{\numberline {6.7.1}Sequence Convergence}{52}{subsubsection.6.7.1}%
\contentsline {subsubsection}{\numberline {6.7.2}Bolzano-Weierstrass Theorem: Compact set $S$, $\exists $ subsequence converges to $\vec {x}^*\in S$}{52}{subsubsection.6.7.2}%
\contentsline {subsubsection}{\numberline {6.7.3}Extreme Value Theorem: continuous $f$ compact set $\rightarrow \mathbb {R}$ has global-min}{52}{subsubsection.6.7.3}%
\contentsline {subsubsection}{\numberline {6.7.4}Support Theorem: $z\in bd(C)$, $\exists \vec {u}$ s.t. $\vec {u}\cdot \vec {x}\leq \vec {u}\cdot \vec {z}, \forall \vec {x}\in C$}{53}{subsubsection.6.7.4}%
\contentsline {section}{\numberline {7}Constrained Optimization}{53}{section.7}%
\contentsline {subsection}{\numberline {7.1}Perturbation of Constraints}{54}{subsection.7.1}%
\contentsline {subsubsection}{\numberline {7.1.1}Theorem: convex program $\Rightarrow $ $MP(\vec {z})$ is a convex function on convex domain}{54}{subsubsection.7.1.1}%
\contentsline {subsection}{\numberline {7.2}Karush-Kuhn-Tucker theorem}{55}{subsection.7.2}%
\contentsline {subsubsection}{\numberline {7.2.1}Def: $P$ is super-consistent (Slater's condition): $\exists \vec {x}^*\in S$ s.t. $\vec {g}(\vec {x}^*)<\vec {0}$}{55}{subsubsection.7.2.1}%
\contentsline {subsubsection}{\numberline {7.2.2}Lemma: Convex $P$ is super-consistent $\Rightarrow $ $\exists $ sensitivity vector $\vec {\lambda }\geq 0$ s.t. $MP(\vec {z})\geq MP(\vec {0})-\vec {\lambda }\cdot \vec {z},\forall \vec {z}$}{55}{subsubsection.7.2.2}%
\contentsline {subsubsection}{\numberline {7.2.3}Goalposts Lemma: $\vec {x}^{(0)}$ is feasible for $P(\vec {g}(\vec {x}^{(0)}))$ and $MP(\vec {g}(\vec {x}^{(0)}))\leq f(\vec {x}^{(0)})$}{56}{subsubsection.7.2.3}%
\contentsline {subsubsection}{\numberline {7.2.4}KKT Theorem: Saddle Point Form}{56}{subsubsection.7.2.4}%
\contentsline {subsubsection}{\numberline {7.2.5}KKT Theorem: Gradient Form}{58}{subsubsection.7.2.5}%
\contentsline {subsubsection}{\numberline {7.2.6}Relationship between KKT and Optimal Solution}{60}{subsubsection.7.2.6}%
\contentsline {subsection}{\numberline {7.3}KKT Duality}{60}{subsection.7.3}%
\contentsline {subsubsection}{\numberline {7.3.1}Thm: If $P$ has optimal solution $\vec {x}^*$ and sensitivity vector $\vec {\lambda }^*$, $h(\vec {\lambda })=\inf _{\vec {x}\in S}\{L(\vec {x},\vec {\lambda })\}\leq h(\vec {\lambda ^*})=f(\vec {x}^*), \forall \vec {\lambda }$}{60}{subsubsection.7.3.1}%
\contentsline {subsubsection}{\numberline {7.3.2}KKT Duality}{61}{subsubsection.7.3.2}%
\contentsline {subsubsection}{\numberline {7.3.3}The Duality Gap}{61}{subsubsection.7.3.3}%
\contentsline {subsection}{\numberline {7.4}With Equality Constraints}{62}{subsection.7.4}%
\contentsline {subsubsection}{\numberline {7.4.1}KKT with equality constraints}{62}{subsubsection.7.4.1}%
\contentsline {subsubsection}{\numberline {7.4.2}Equality constraints in the penalty method}{63}{subsubsection.7.4.2}%
\contentsline {subsubsection}{\numberline {7.4.3}More general super-consistent}{63}{subsubsection.7.4.3}%
\contentsline {subsubsection}{\numberline {7.4.4}Equality constraints in geometric programming}{64}{subsubsection.7.4.4}%
\contentsline {section}{\numberline {8}Constrained Geometric Programming}{65}{section.8}%
\contentsline {subsection}{\numberline {8.1}Standard Form of Geometric Programming}{65}{subsection.8.1}%
\contentsline {subsection}{\numberline {8.2}Example}{65}{subsection.8.2}%
\contentsline {subsection}{\numberline {8.3}The geometric programming dual, in general}{68}{subsection.8.3}%
\contentsline {subsection}{\numberline {8.4}Using a dual solution to find a primal solution}{70}{subsection.8.4}%
\contentsline {section}{\numberline {9}Penalty Method}{71}{section.9}%
\contentsline {subsection}{\numberline {9.1}The Courant-Beltrami penalty function}{72}{subsection.9.1}%
\contentsline {subsubsection}{\numberline {9.1.1}Example}{72}{subsubsection.9.1.1}%
\contentsline {subsubsection}{\numberline {9.1.2}Guarantees assuming convergence}{72}{subsubsection.9.1.2}%
\contentsline {subsubsection}{\numberline {9.1.3}Guaranteeing convergence: (1)feasible $P$; (2)continuous $g_i$; (3)coercive $f$}{73}{subsubsection.9.1.3}%
\contentsline {subsection}{\numberline {9.2}The Penalty Method and KKT Duality}{74}{subsection.9.2}%
\contentsline {subsubsection}{\numberline {9.2.1}Thm: (1)feasible $P$; (2)continuous $g_i$; (3)coercive $f$ $\Rightarrow MP=MD$}{74}{subsubsection.9.2.1}%
\contentsline {subsubsection}{\numberline {9.2.2}If $f$ not coercive}{75}{subsubsection.9.2.2}%
\contentsline {section}{\numberline {10}Optimization with Equality Constraints}{76}{section.10}%
\contentsline {subsection}{\numberline {10.1}Basic}{76}{subsection.10.1}%
\contentsline {subsection}{\numberline {10.2}Lagrange Mutiplier Theorem}{76}{subsection.10.2}%
\contentsline {subsubsection}{\numberline {10.2.1}First-order necessary condition: $\exists \lambda , \nabla f(x^*)+\DOTSB \sum@ \slimits@ _{i=1}^m\lambda _i \nabla h_i(x^*)=0$}{76}{subsubsection.10.2.1}%
\contentsline {subsubsection}{\numberline {10.2.2}Second-order necessary condition: $z^T\left (\nabla ^2 f(x^*)+\DOTSB \sum@ \slimits@ _{i=1}^m\lambda _i \nabla ^2 h_i(x^*)\right )z\geq 0,\forall z\in V(x^*)$}{78}{subsubsection.10.2.2}%
\contentsline {subsubsection}{\numberline {10.2.3}Sufficient Condition: $\exists \lambda $ 1. $\nabla f(x^*)+\DOTSB \sum@ \slimits@ _{i=1}^m\lambda _i \nabla h_i(x^*)=0$ 2. $z^T\big (\nabla ^2 f(x^*)+\DOTSB \sum@ \slimits@ _{i=1}^m\lambda _i \nabla ^2 h_i(x^*)\big )z> 0,\forall z\in V(x^*),z\neq 0$}{79}{subsubsection.10.2.3}%
\contentsline {subsubsection}{\numberline {10.2.4}Lagrangian Function}{79}{subsubsection.10.2.4}%
\contentsline {subsubsection}{\numberline {10.2.5}Example}{80}{subsubsection.10.2.5}%
\contentsline {subsubsection}{\numberline {10.2.6}Sensitivity Analysis $f(x^*(u))=f(x^*)-\lambda ^Tu+O(\|u\|)$}{81}{subsubsection.10.2.6}%
\contentsline {subsubsection}{\numberline {10.2.7}Linear Constraints}{81}{subsubsection.10.2.7}%
\contentsline {section}{\numberline {11}Optimization with Inequality Constraints}{83}{section.11}%
\contentsline {subsection}{\numberline {11.1}Basic}{83}{subsection.11.1}%
\contentsline {subsubsection}{\numberline {11.1.1}Active vs. Inactive Inequality Constraints}{83}{subsubsection.11.1.1}%
\contentsline {subsubsection}{\numberline {11.1.2}ICP $\rightarrow $ ECP}{84}{subsubsection.11.1.2}%
\contentsline {subsubsection}{\numberline {11.1.3}Intuition $\mu _j\geq 0, \forall j\in A(x^*)$}{84}{subsubsection.11.1.3}%
\contentsline {subsubsection}{\numberline {11.1.4}Complementary Slackness}{84}{subsubsection.11.1.4}%
\contentsline {subsection}{\numberline {11.2}Karush–Kuhn–Tucker (KKT) Necessary Conditions}{84}{subsection.11.2}%
\contentsline {subsection}{\numberline {11.3}Karush–Kuhn–Tucker (KKT) Sufficient Conditions}{87}{subsection.11.3}%
\contentsline {subsection}{\numberline {11.4}General Sufficiency Condition}{89}{subsection.11.4}%
\contentsline {subsection}{\numberline {11.5}Barrier Method}{90}{subsection.11.5}%
\contentsline {subsection}{\numberline {11.6}An Exmaple Using KKT or Barrier}{92}{subsection.11.6}%
\contentsline {subsubsection}{\numberline {11.6.1}Solution using KKT conditions}{92}{subsubsection.11.6.1}%
\contentsline {subsubsection}{\numberline {11.6.2}Solution using logarithmic barrier}{93}{subsubsection.11.6.2}%
\contentsline {subsection}{\numberline {11.7}Penalty Method (For ECP)}{93}{subsection.11.7}%
\contentsline {section}{\numberline {12}Duality}{94}{section.12}%
\contentsline {subsection}{\numberline {12.1}Weak Duality Theorem: $\max _{(\lambda ,\mu )\in G}D(\lambda ,\mu )\leq \min _{x\in F}f(x)$}{95}{subsection.12.1}%
\contentsline {subsection}{\numberline {12.2}Strong Duality Theorem: under some conditions, $\max _{(\lambda ,\mu )\in G}D(\lambda ,\mu )= \min _{x\in F}f(x)$}{95}{subsection.12.2}%
\contentsline {subsubsection}{\numberline {12.2.1}Slater's sufficient condition for strong duality}{97}{subsubsection.12.2.1}%
\contentsline {subsubsection}{\numberline {12.2.2}Example}{97}{subsubsection.12.2.2}%
\contentsline {subsection}{\numberline {12.3}Dual of Linear Program}{98}{subsection.12.3}%
\contentsline {section}{\numberline {13}Strongly Convexity}{99}{section.13}%
\contentsline {subsection}{\numberline {13.1}$\mu $-Strongly Convex: $ \langle \nabla f(w)-\nabla f(v), w-v\rangle \geq \mu \|w-v\|^{2}$}{99}{subsection.13.1}%
\contentsline {subsection}{\numberline {13.2}$\mu $-strongly convex $\Leftrightarrow \nabla ^{2} f(x) \succeq \mu I\Leftrightarrow $"$f(x)-\frac {m}{2}\|x\|^2$ is convex"}{99}{subsection.13.2}%
\contentsline {subsection}{\numberline {13.3}Lemma: Strongly convexity $\Rightarrow $ Strictly convexity}{100}{subsection.13.3}%
\contentsline {subsection}{\numberline {13.4}Lemma: $\nabla ^2 f(x)\succeq mI$ $\Rightarrow $ $f(y)\geq f(x)+\nabla f(x)^T(y-x)+\frac {m}{2}\|y-x\|^2$}{100}{subsection.13.4}%
\contentsline {section}{\numberline {14}Lipschitz Gradient ($L$-Smooth)}{101}{section.14}%
\contentsline {subsection}{\numberline {14.1}Theorem: $-MI\preceq \nabla ^2 f(x)\preceq MI$ $\Rightarrow $ $f$ is $M$-smooth}{101}{subsection.14.1}%
\contentsline {subsection}{\numberline {14.2}Descent Lemma: $f$ is $L$-smooth $\Rightarrow $ $f(y)\leq f(x)+\nabla f(x)^T(y-x)+\frac {L}{2}\|y-x\|^2$}{102}{subsection.14.2}%
\contentsline {subsection}{\numberline {14.3}Co-coercivity Condition: $(\nabla f(x)-\nabla f(y))^T(x-y)\geq \frac {1}{L}\|\nabla f(x)-\nabla f(y)\|^2$}{103}{subsection.14.3}%
