\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Unconstrained Optimization}{4}{section.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Conditions for Optimality}{4}{subsection.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Global minimizer, Local minimizer}{4}{subsection.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}Optimization in $\mathbb  {R}$}{4}{subsection.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.3.1}Theorem: local minimizer $\Rightarrow f'(x^*)=0$}{4}{subsubsection.1.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.3.2}Theorem: $f'(x^*)=0, f''(x^*)\geq 0 \Rightarrow $ local minimizer}{4}{subsubsection.1.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4}Optimization in $\mathbb  {R}^n$}{4}{subsection.1.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.4.1}First Order Necessary Condition: $\nabla f(x^*)=0$}{4}{subsubsection.1.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.4.2}Stationary Point, Saddle Point}{5}{subsubsection.1.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.4.3}Second Order Necessary Condition $\nabla ^2 f(x^*)\succeq 0$}{5}{subsubsection.1.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.4.4}Sufficient Conditions for Optimality $\nabla f(x^*)=0\text  { and }\nabla ^2 f(x^*)\succ 0$}{6}{subsubsection.1.4.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.4.5}Using Optimality Conditions to Find Minimum}{6}{subsubsection.1.4.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.4.6}Fix Conditions for Global Optimality}{7}{subsubsection.1.4.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.5}Optimization in a Set}{7}{subsection.1.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.5.1}Existence of Global-min}{7}{subsubsection.1.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.6}Method of finding-global-min-among-stationary-points (FGMSP)}{8}{subsection.1.6}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Convexity}{8}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Definition}{8}{subsection.2.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces }}{9}{figure.1}\protected@file@percent }
\newlabel{}{{1}{9}{}{figure.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Convex$\Rightarrow $Stationary point is global-min}{10}{subsection.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Unconstrained Quadratic Optimization}{10}{subsection.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Strongly Convexity}{11}{subsection.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.4.1}$\mu $-Strongly Convex: $ \langle \nabla f(w)-\nabla f(v), w-v\rangle \geq \mu \|w-v\|^{2}$}{11}{subsubsection.2.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.4.2}$\mu $-strongly convex $\Leftrightarrow \nabla ^{2} f(x) \succeq \mu I$}{11}{subsubsection.2.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.4.3}Lemma: Strongly convexity $\Rightarrow $ Strictly convexity}{12}{subsubsection.2.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.4.4}Lemma: $\nabla ^2 f(x)\succeq mI$ $\Rightarrow $ $f(y)\geq f(x)+\nabla f(x)^T(y-x)+\frac  {m}{2}\|y-x\|^2$}{12}{subsubsection.2.4.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3}Gradient Methods}{12}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Steepest Descent}{13}{subsection.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Methods for Choosing $\alpha _k$}{13}{subsection.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Optimal(Exact) Line Search}{13}{subsection.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Armijo's Rule}{14}{subsection.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5}Armijo's Rule for Steepest Descent}{14}{subsection.3.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4}Convergence of GD with Constant Stepsize}{15}{section.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Lipschitz Gradient ($L$-Smooth)}{15}{subsection.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.1}Theorem: $-MI\preceq \nabla ^2 f(x)\preceq MI$ $\Rightarrow $ $\nabla f(x)$ is Lipschitz with constant $M$}{15}{subsubsection.4.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.2}Descent Lemma: $\nabla f(x)$ is Lipschitz with constant $L$ $\Rightarrow f(y)\leq f(x)+\nabla f(x)^T(y-x)+\frac  {L}{2}\|y-x\|^2$}{16}{subsubsection.4.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.3}Co-coercivity Condition: $(\nabla f(x)-\nabla f(y))^T(x-y)\geq \frac  {1}{L}\|\nabla f(x)-\nabla f(y)\|^2$}{17}{subsubsection.4.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Convergence of Steepest Descent with Fixed Stepsize}{17}{subsection.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.1}Theorem: $f$ has Lipschitz gradient $\Rightarrow $ $\{x_k\}$ converges to stationary point}{17}{subsubsection.4.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Convergence of GD for convex functions}{19}{subsection.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.1}Theorem: $f$ is convex and has Lipschitz gradient $\Rightarrow $ $f(x_k)$ converges to global-min value with rate $\frac  {1}{k}$}{19}{subsubsection.4.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Convergence of GD for strongly convex functions}{20}{subsection.4.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.4.1}Theorem: Strongly convex, Lipschitz gradient $\Rightarrow $ $\{x_k\}$ converges to global-min geometrically}{20}{subsubsection.4.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.4.2}Example}{21}{subsubsection.4.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5}Convergence of Gradient Descent on Smooth Strongly-Convex Functions}{22}{subsection.4.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.6}From convergence rate to iteration complexity}{24}{subsection.4.6}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5}Newton's Method}{25}{section.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Generalization to Optimization}{25}{subsection.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}A New Interpretation of Newton’s Method}{25}{subsection.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Convergence of Newton's Method}{26}{subsection.5.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4}Note: Cons and Pros}{27}{subsection.5.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5}Modifications to ensure global convergence}{28}{subsection.5.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.6}Quasi-Newton Methods}{28}{subsection.5.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.6.1}BFGS Method}{28}{subsubsection.5.6.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.7}Trust-Region Method}{30}{subsection.5.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.8}Cubic Regularization}{30}{subsection.5.8}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6}Neural Networks}{30}{section.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Neuron}{30}{subsection.6.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Neuron Examples}}{30}{figure.2}\protected@file@percent }
\newlabel{Neuron}{{2}{30}{Neuron Examples}{figure.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Vector Input}}{31}{figure.3}\protected@file@percent }
\newlabel{}{{3}{31}{Vector Input}{figure.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}Multilayer Neural Network}{31}{subsection.6.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Multilayer Neural Network}}{31}{figure.4}\protected@file@percent }
\newlabel{}{{4}{31}{Multilayer Neural Network}{figure.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3}Back Propagation Algorithm}{32}{subsection.6.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4}Other Methods}{34}{subsection.6.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {7}Constrained Optimization and Gradient Projection}{34}{section.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1}Constrained Optimization: Basic}{34}{subsection.7.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.1.1}Def: Optimality}{34}{subsubsection.7.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.1.2}Prop: local-min$\Rightarrow \nabla f(x^*)^T(x-x^*)\geq 0,\forall x\in \&$ $\Leftrightarrow $ global-min in convex }{34}{subsubsection.7.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.1.3}Def: Interior Point}{35}{subsubsection.7.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2}Constrained Optimization Example}{35}{subsection.7.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.3}Projection onto Closed Convex Set}{35}{subsection.7.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.3.1}Def: Projection $[z]^\&$}{35}{subsubsection.7.3.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Projection onto Closed Convex Set}}{36}{figure.5}\protected@file@percent }
\newlabel{}{{5}{36}{Projection onto Closed Convex Set}{figure.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Projection onto Closed non-Convex Set}}{36}{figure.6}\protected@file@percent }
\newlabel{}{{6}{36}{Projection onto Closed non-Convex Set}{figure.6}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.3.2}Prop: \underline  {unique} projection $[z]^\&$ on \underline  {closed convex} subset of $\mathbb  {R}^n$}{36}{subsubsection.7.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.3.3}Projection Theorem: $x=[z]^\&$ is projection on \underline  {closed conex} subset of $\mathbb  {R}^n$$\Leftrightarrow $ $(z-x)^T(y-x)\leq 0, \forall y\in \&$}{36}{subsubsection.7.3.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Necessary and Sufficient Condition for Projection}}{37}{figure.7}\protected@file@percent }
\newlabel{}{{7}{37}{Necessary and Sufficient Condition for Projection}{figure.7}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.3.4}Prop: Projection is non-expansive $\|[x]^\&-[z]^\&\|\leq \|x-z\|,\forall x,z\in \mathbb  {R}^n$}{37}{subsubsection.7.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.4}Projection on (Linear) Subspaces of $\mathbb  {R}^n$}{37}{subsection.7.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.4.1}Orthogonality Principle in subspaces of $\mathbb  {R}^n$: $(z-y^*)^Tx= 0,\forall x\in \&$}{37}{subsubsection.7.4.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Point from $\mathbb  {R}^2$ to $\mathbb  {R}$}}{38}{figure.8}\protected@file@percent }
\newlabel{}{{8}{38}{Point from $\mathbb {R}^2$ to $\mathbb {R}$}{figure.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.5}Gradient Projection Method}{38}{subsection.7.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.5.1}Def: \underline  {fixed point} in fixed step-size steepest descent method, $\tilde  {x}=[\tilde  {x}-\alpha \nabla f(\tilde  {x})]^\&$}{38}{subsubsection.7.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.5.2}Prop: $L-$smooth, $0<\alpha <\frac  {2}{L}$ $\Rightarrow $ limit point is a fixed point (in fixed step-size steepest descent method)}{39}{subsubsection.7.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.5.3}Prop: $x$ is minimizer in convex func $\Leftrightarrow $ fixed point (in fixed step-size steepest descent method)}{39}{subsubsection.7.5.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.5.4}Thm: Convergence of Gradient Projection: Convex, $L-$smooth, $0<\alpha <\frac  {2}{L}$ $\Rightarrow $ $f(x_k)\rightarrow f(x^*)$ at rate $\frac  {1}{k}$}{39}{subsubsection.7.5.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.5.5}Thm: Strongly convex, Lipschitz gradient $\Rightarrow $ $\{x_k\}$ converges to $x^*$ geometrically}{40}{subsubsection.7.5.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {8}Optimization with Equality Constraints}{40}{section.8}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.1}Basic}{40}{subsection.8.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.2}Lagrange Mutiplier Theorem}{40}{subsection.8.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.2.1}First-order necessary condition: $\exists \lambda , \nabla f(x^*)+\DOTSB \sum@ \slimits@ _{i=1}^m\lambda _i \nabla h_i(x^*)=0$}{40}{subsubsection.8.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.2.2}Second-order necessary condition: $z^T\left (\nabla ^2 f(x^*)+\DOTSB \sum@ \slimits@ _{i=1}^m\lambda _i \nabla ^2 h_i(x^*)\right )z\geq 0,\forall z\in V(x^*)$}{42}{subsubsection.8.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.2.3}Sufficient Condition: $\exists \lambda $ 1. $\nabla f(x^*)+\DOTSB \sum@ \slimits@ _{i=1}^m\lambda _i \nabla h_i(x^*)=0$ 2. $z^T\left (\nabla ^2 f(x^*)+\DOTSB \sum@ \slimits@ _{i=1}^m\lambda _i \nabla ^2 h_i(x^*)\right )z> 0,\forall z\in V(x^*),z\neq  0$}{43}{subsubsection.8.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.2.4}Lagrangian Function}{43}{subsubsection.8.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.2.5}Example}{43}{subsubsection.8.2.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.2.6}Sensitivity Analysis $f(x^*(u))=f(x^*)-\lambda ^Tu+O(\|u\|)$}{44}{subsubsection.8.2.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.2.7}Linear Constraints}{45}{subsubsection.8.2.7}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {9}Optimization with Inequality Constraints}{46}{section.9}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {9.1}Basic}{46}{subsection.9.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {9.1.1}Active vs. Inactive Inequality Constraints}{46}{subsubsection.9.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {9.1.2}ICP $\rightarrow $ ECP}{46}{subsubsection.9.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {9.1.3}Intuition $\mu _j\geq 0, \forall j\in A(x^*)$}{47}{subsubsection.9.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {9.1.4}Complementary Slackness}{47}{subsubsection.9.1.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {9.2}Karush–Kuhn–Tucker (KKT) Necessary Conditions}{47}{subsection.9.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {9.3}Karush–Kuhn–Tucker (KKT) Sufficient Conditions}{49}{subsection.9.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {9.4}General Sufficiency Condition}{50}{subsection.9.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {9.5}Barrier Method}{51}{subsection.9.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {9.6}An Exmaple Using KKT or Barrier}{52}{subsection.9.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {9.6.1}Solution using KKT conditions}{52}{subsubsection.9.6.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {9.6.2}Solution using logarithmic barrier}{53}{subsubsection.9.6.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {9.7}Penalty Method (For ECP)}{53}{subsection.9.7}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {10}Duality}{54}{section.10}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {10.1}Weak Duality Theorem: $\qopname  \relax m{max}_{(\lambda ,\mu )\in G}D(\lambda ,\mu )\leq \qopname  \relax m{min}_{x\in F}f(x)$}{55}{subsection.10.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {10.2}Strong Duality Theorem: under some conditions, $\qopname  \relax m{max}_{(\lambda ,\mu )\in G}D(\lambda ,\mu )= \qopname  \relax m{min}_{x\in F}f(x)$}{55}{subsection.10.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {10.2.1}Slater's sufficient condition for strong duality}{57}{subsubsection.10.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {10.2.2}Example}{57}{subsubsection.10.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {10.3}Dual of Linear Program}{57}{subsection.10.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {11}Augmented Lagrangian Method (adjusted penalty method)}{58}{section.11}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {11.1}Motivation}{58}{subsection.11.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {11.2}Augemented Lagrangian Method}{58}{subsection.11.2}\protected@file@percent }
\gdef \@abspage@last{59}
