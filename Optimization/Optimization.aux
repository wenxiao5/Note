\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Unconstrained Optimization}{8}{section.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Basic Definitions}{8}{subsection.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.1.1}Optimization in a Set}{8}{subsubsection.1.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.1.2}Minimizer}{8}{subsubsection.1.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.1.3}Stationary Point, Saddle Point}{8}{subsubsection.1.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.1.4} Conditions for Global Minimizer: (1) exists global-minimizer; (2) has the minimum value in all stationary points}{9}{subsubsection.1.1.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Special Situation: Optimization in $\mathbb  {R}$}{9}{subsection.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.2.1} Necessary condition of local-min: $f'(x^*)=0$}{9}{subsubsection.1.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.2.2} Sufficient condition of local-min: $f'(x^*)=0, f''(x^*)\geq 0$}{9}{subsubsection.1.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.2.3} Sufficient condition of global-min: $f'(x^*)=0$ and $f''(x)\geq 0,\forall x\in I$}{9}{subsubsection.1.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3} Restriction to a Line}{10}{subsection.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.3.1} Definition: $\phi _{\vec  {u}}(t)=f(\vec  {x}+t\vec  {u}),\ \vec  {x},\vec  {u}\in \mathbb  {R}^n, t\in \mathbb  {R}$}{10}{subsubsection.1.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.3.2}Derivatives: $\phi '_{\vec  {u}}(t)=\nabla f(\vec  {x}+t\vec  {u})\vec  {u}$, $\phi ^{''}_{\vec  {u}} (t)=\vec  {u}^T {Hf}(\vec  {x}+t\vec  {u})\vec  {u}$}{10}{subsubsection.1.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.3.3}Lemma: $x^*$ is a global minimizer of $f$ $\Leftrightarrow $ $t=0$ is the global minimizer of $\phi _{\vec  {u}}(t)=f(\vec  {x}+t\vec  {u})$, $\forall \vec  {u}\in \mathbb  {R}^n$}{11}{subsubsection.1.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4}General: Optimization in $\mathbb  {R}^n$}{11}{subsection.1.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.4.1}Local-min Necessary Condition $1$: $\nabla f$ is continuous, $x^*$ is a local minimizer $\Rightarrow \nabla f(x^*)=0$}{11}{subsubsection.1.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.4.2}Local-min Necessary Condition $2$: $Hf$ is continuous, $x^*$ is a local minimizer $\Rightarrow \nabla ^2 f(x^*)\succeq 0$}{12}{subsubsection.1.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.4.3}Local-min Sufficient Condition $1$: $Hf$ is continuous, $\nabla f(\vec  {x}^*)=0$, $\vec  {u}^T Hf(\vec  {x}) \vec  {u}\geq 0,\forall \vec  {u}\in \mathbb  {R}^n$ and $\exists r>0, \|\vec  {x}-\vec  {x}^*\|<r$ $\Rightarrow $ $\vec  {x}^*$ is a local minimizer}{13}{subsubsection.1.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.4.4}Local-min Sufficient Condition $1'$: $Hf$ is continuous, $\nabla f(\vec  {x}^*)=0$, $\nabla ^2 f(\vec  {x}^*)\succ 0$ $\Rightarrow $ $\vec  {x}^*$ is a local minimizer}{13}{subsubsection.1.4.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.4.5}Global-min Sufficient Condition: $Hf$ is continuous, $\nabla f(\vec  {x}^*)=0$, $\nabla ^2f(\vec  {x})\succeq 0,\forall \vec  {x}$ $\Rightarrow $ $\vec  {x}^*$ is a global minimizer}{13}{subsubsection.1.4.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.4.6}$Hf(\vec  {x}^*)$ is indefinite $\Rightarrow $ $\vec  {x}^*$ is saddle point}{14}{subsubsection.1.4.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.4.7}$Hf(\vec  {x}^*)\succ 0$/$\prec 0$ $\Rightarrow $ critical point $\vec  {x}^*$ is strictly local-min/local-max}{14}{subsubsection.1.4.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.4.8}Steps to Find Minimum in $\mathbb  {R}^n$}{14}{subsubsection.1.4.8}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.5}Existence of Global-min}{15}{subsection.1.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.5.1}(Bolzano-)Weierstrass Theorem: Compact set $X$ $\Rightarrow $ $\exists $ global-min/max}{15}{subsubsection.1.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.5.2}Coercive function $f$ $\Rightarrow $ $\exists $ global-min}{15}{subsubsection.1.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.5.3}Method of finding-global-min-among-stationary-points (FGMSP)}{16}{subsubsection.1.5.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Convexity}{16}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Convex Set}{16}{subsection.2.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Convex Set}}{16}{figure.1}\protected@file@percent }
\newlabel{}{{1}{16}{Convex Set}{figure.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.1}Prop: convex sets $e_1,e_2,...e_n$, then $\cap _{i=1}^ne_i$ is convex}{17}{subsubsection.2.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Convex Hull}{17}{subsection.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.1}Convex Hull $conv(S)$ is the set of all convex combinations of points in $S$}{17}{subsubsection.2.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.2}Theorem: convex set $S$, convex combination $\lambda _1 x_1+\cdots  +\lambda _k x_k\in S$, $\forall x_1,...,x_k\in S$}{17}{subsubsection.2.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.3}Corollary: $conv(S)$ is the smallest convex set containing $S$}{17}{subsubsection.2.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Convex Function}{18}{subsection.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.1}Definition: $f$ is convex $\Leftrightarrow $ $f(\alpha x+(1-\alpha ) y) \leq \alpha f(x)+(1-\alpha ) f(y), \forall x, y \in C, \forall \alpha \in [0,1]$}{18}{subsubsection.2.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.2}First-order: $f$ is convex $\Leftrightarrow $ $f(z) \geq f(x)+(z-x)^{T} \nabla f(x), \forall x, z \in C$}{18}{subsubsection.2.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.3}Second-order: $f$ is convex $\Leftrightarrow $ $\nabla ^{2} f(x) \succeq 0,\ \forall x \in C$}{19}{subsubsection.2.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.4}Sufficient Condition of Strictly Convex: $\nabla ^{2} f(x) \succ 0$}{19}{subsubsection.2.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.5}Prop: Max and Linear combination of convex functions are also convex}{19}{subsubsection.2.3.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Lemma: function $f$ is a convex function iff $\phi (t)=f(\vec  {x}+t\vec  {u})$ is convex of $t$}{20}{subsection.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5}Proposition: Convex function $f$, $\nabla f(x^*)=0$ $\Rightarrow $ global-min}{20}{subsection.2.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6}Application: Unconstrained Quadratic Optimization}{21}{subsection.2.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.7}Theorem: If $f$ is convex and $g$ is convex and increasing, $(g\cdot f)(x)$ is convex}{23}{subsection.2.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.8}Corollary: $f$ is linear, $g$ is convex (not necessarily increasing) $\Rightarrow $ $g\cdot f$ is convex}{24}{subsection.2.8}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.9}Epigraph and Jensen's Inequality}{24}{subsection.2.9}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.9.1}Def: epigraph ${epi}(f)=\{(x,y)\in C\times \mathbb  {R}:y\geq f(x)\}$}{24}{subsubsection.2.9.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.9.2}Lemma: $f$ is convex function $\Leftrightarrow $ ${epi}(f)$ is a convex set}{24}{subsubsection.2.9.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.9.3}Jensen's Inequality: $f(\DOTSB \sum@ \slimits@ _{i=1}^k\lambda _i x_i)\leq \DOTSB \sum@ \slimits@ _{i=1}^k\lambda _if(x_i)$}{25}{subsubsection.2.9.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3}Geometric Program (GP)}{25}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Arithmetic Mean-Geometric Mean Inequality (A-G inequality) $\delta _1x_2+\delta _2x_2+\cdots  +\delta _nx_n\geq x_1^{\delta _1}x_2^{\delta _2}\cdots  x_n^{\delta _n}$}{25}{subsection.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Unconstrained Geometric Programs}{26}{subsection.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.1}Def: Posynomial}{26}{subsubsection.3.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.2}General Strategy: A-G inequality}{26}{subsubsection.3.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.3}Dual of the Unconstrained GP}{27}{subsubsection.3.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4}Polynomial Interpolation}{29}{section.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Method 1: $M\vec  {a}=\vec  {y}$}{29}{subsection.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Method 2: Lagrange Interpolation Formula}{29}{subsection.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Lines of Best Fit}{30}{subsection.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5}Strongly Convexity}{30}{section.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}$\mu $-Strongly Convex: $ \langle \nabla f(w)-\nabla f(v), w-v\rangle \geq \mu \|w-v\|^{2}$}{30}{subsection.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}$\mu $-strongly convex $\Leftrightarrow \nabla ^{2} f(x) \succeq \mu I\Leftrightarrow $"$f(x)-\frac  {m}{2}\|x\|^2$ is convex"}{31}{subsection.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Lemma: Strongly convexity $\Rightarrow $ Strictly convexity}{31}{subsection.5.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4}Lemma: $\nabla ^2 f(x)\succeq mI$ $\Rightarrow $ $f(y)\geq f(x)+\nabla f(x)^T(y-x)+\frac  {m}{2}\|y-x\|^2$}{31}{subsection.5.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6}Lipschitz Gradient ($L$-Smooth)}{32}{section.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Theorem: $-MI\preceq \nabla ^2 f(x)\preceq MI$ $\Rightarrow $ $f$ is $M$-smooth}{32}{subsection.6.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}Descent Lemma: $f$ is $L$-smooth $\Rightarrow $ $f(y)\leq f(x)+\nabla f(x)^T(y-x)+\frac  {L}{2}\|y-x\|^2$}{33}{subsection.6.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3}Co-coercivity Condition: $(\nabla f(x)-\nabla f(y))^T(x-y)\geq \frac  {1}{L}\|\nabla f(x)-\nabla f(y)\|^2$}{34}{subsection.6.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {7}Gradient Methods}{34}{section.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1}Steepest Descent}{35}{subsection.7.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2}Methods for Choosing Step Size $\alpha _k$}{36}{subsection.7.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.3}Algorithm Convergence}{38}{subsection.7.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.4}Convergence of The Steepest Descent with Fixed Step Size}{38}{subsection.7.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.4.1}Theorem: $f$ is $L$-smooth $\Rightarrow $ $\{x_k\}$ converges to stationary point}{38}{subsubsection.7.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.4.2}Theorem: $f$ is convex and $L$-smooth $\Rightarrow $ $f(x_k)$ converges to global-min value with rate $\frac  {1}{k}$}{40}{subsubsection.7.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.4.3}Theorem: $f$ is strongly convex and $L-$smooth $\Rightarrow $ $\{x_k\}$ converges to global-min geometrically}{42}{subsubsection.7.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.5}Convergence of Gradient Descent on Smooth Strongly-Convex Functions}{44}{subsection.7.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.6}From convergence rate to iteration complexity}{47}{subsection.7.6}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {8}Newton's Method}{48}{section.8}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.1}Generalization to Optimization}{49}{subsection.8.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.2}A New Interpretation of Newton’s Method}{49}{subsection.8.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.3}Convergence of Newton's Method}{49}{subsection.8.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.4}Note: Cons and Pros}{51}{subsection.8.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.5}Modifications to ensure global convergence}{52}{subsection.8.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.6}Quasi-Newton Methods}{52}{subsection.8.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.6.1}BFGS Method}{53}{subsubsection.8.6.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.7}Trust-Region Method}{54}{subsection.8.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.8}Cubic Regularization}{54}{subsection.8.8}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {9}Neural Networks}{55}{section.9}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {9.1}Basics}{55}{subsection.9.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Neuron Examples}}{55}{figure.2}\protected@file@percent }
\newlabel{Neuron}{{2}{55}{Neuron Examples}{figure.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Simple Neural Network}}{56}{figure.3}\protected@file@percent }
\newlabel{}{{3}{56}{Simple Neural Network}{figure.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.2}Multilayer Neural Network}{56}{subsection.9.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Multilayer Neural Network}}{56}{figure.4}\protected@file@percent }
\newlabel{}{{4}{56}{Multilayer Neural Network}{figure.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.3}A Simple Example of Back Propagation Algorithm}{57}{subsection.9.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Two Independent Pathways}}{58}{figure.5}\protected@file@percent }
\newlabel{}{{5}{58}{Two Independent Pathways}{figure.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.4}Back Propagation Algorithm}{58}{subsection.9.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {9.5}Other Methods}{60}{subsection.9.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {10}Constrained Optimization and Gradient Projection}{60}{section.10}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {10.1}Constrained Optimization: Basic}{60}{subsection.10.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {10.1.1}Def: Optimality}{60}{subsubsection.10.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {10.1.2}Prop: local-min$\Rightarrow \nabla f(x^*)^T(x-x^*)\geq 0,\forall x\in \&$ $\Leftrightarrow $ global-min in convex }{61}{subsubsection.10.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {10.1.3}Def: Interior Point}{61}{subsubsection.10.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {10.2}Constrained Optimization Example}{62}{subsection.10.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {10.3}Projection onto Closed Convex Set}{62}{subsection.10.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {10.3.1}Def: Projection $[z]^\&$}{62}{subsubsection.10.3.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Projection onto Closed Convex Set}}{63}{figure.6}\protected@file@percent }
\newlabel{}{{6}{63}{Projection onto Closed Convex Set}{figure.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Projection onto Closed non-Convex Set}}{63}{figure.7}\protected@file@percent }
\newlabel{}{{7}{63}{Projection onto Closed non-Convex Set}{figure.7}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {10.3.2}Prop: \underline  {unique} projection $[z]^\&$ on \underline  {closed convex} subset of $\mathbb  {R}^n$}{63}{subsubsection.10.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {10.3.3}Projection Theorem: $x=[z]^\&$ is projection on \underline  {closed conex} subset of $\mathbb  {R}^n$$\Leftrightarrow $ $(z-x)^T(y-x)\leq 0, \forall y\in \&$}{64}{subsubsection.10.3.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Necessary and Sufficient Condition for Projection}}{64}{figure.8}\protected@file@percent }
\newlabel{}{{8}{64}{Necessary and Sufficient Condition for Projection}{figure.8}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {10.3.4}Prop: Projection is non-expansive $\|[x]^\&-[z]^\&\|\leq \|x-z\|,\forall x,z\in \mathbb  {R}^n$}{64}{subsubsection.10.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {10.4}Projection on (Linear) Subspaces of $\mathbb  {R}^n$}{65}{subsection.10.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {10.4.1}Orthogonality Principle in subspaces of $\mathbb  {R}^n$: $(z-y^*)^Tx= 0,\forall x\in \&$}{65}{subsubsection.10.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {10.5}Gradient Projection Method}{65}{subsection.10.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Point from $\mathbb  {R}^2$ to $\mathbb  {R}$}}{66}{figure.9}\protected@file@percent }
\newlabel{}{{9}{66}{Point from $\mathbb {R}^2$ to $\mathbb {R}$}{figure.9}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {10.5.1}Def: \underline  {fixed point} in fixed step-size steepest descent method, $\tilde  {x}=[\tilde  {x}-\alpha \nabla f(\tilde  {x})]^\&$}{66}{subsubsection.10.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {10.5.2}Prop: $L-$smooth, $0<\alpha <\frac  {2}{L}$ $\Rightarrow $ limit point is a fixed point (in fixed step-size steepest descent method)}{66}{subsubsection.10.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {10.5.3}Prop: $x$ is minimizer in convex func $\Leftrightarrow $ fixed point (in fixed step-size steepest descent method)}{67}{subsubsection.10.5.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {10.5.4}Thm: Convergence of Gradient Projection: Convex, $L-$smooth, $0<\alpha <\frac  {2}{L}$ $\Rightarrow $ $f(x_k)\rightarrow f(x^*)$ at rate $\frac  {1}{k}$}{67}{subsubsection.10.5.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {10.5.5}Thm: Strongly convex, Lipschitz gradient $\Rightarrow $ $\{x_k\}$ converges to $x^*$ geometrically}{67}{subsubsection.10.5.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {11}Optimization with Equality Constraints}{68}{section.11}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {11.1}Basic}{68}{subsection.11.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {11.2}Lagrange Mutiplier Theorem}{69}{subsection.11.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {11.2.1}First-order necessary condition: $\exists \lambda , \nabla f(x^*)+\DOTSB \sum@ \slimits@ _{i=1}^m\lambda _i \nabla h_i(x^*)=0$}{69}{subsubsection.11.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {11.2.2}Second-order necessary condition: $z^T\left (\nabla ^2 f(x^*)+\DOTSB \sum@ \slimits@ _{i=1}^m\lambda _i \nabla ^2 h_i(x^*)\right )z\geq 0,\forall z\in V(x^*)$}{70}{subsubsection.11.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {11.2.3}Sufficient Condition: $\exists \lambda $ 1. $\nabla f(x^*)+\DOTSB \sum@ \slimits@ _{i=1}^m\lambda _i \nabla h_i(x^*)=0$ 2. $z^T\big  (\nabla ^2 f(x^*)+\DOTSB \sum@ \slimits@ _{i=1}^m\lambda _i \nabla ^2 h_i(x^*)\big  )z> 0,\forall z\in V(x^*),z\neq  0$}{71}{subsubsection.11.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {11.2.4}Lagrangian Function}{72}{subsubsection.11.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {11.2.5}Example}{72}{subsubsection.11.2.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {11.2.6}Sensitivity Analysis $f(x^*(u))=f(x^*)-\lambda ^Tu+O(\|u\|)$}{73}{subsubsection.11.2.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {11.2.7}Linear Constraints}{74}{subsubsection.11.2.7}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {12}Optimization with Inequality Constraints}{76}{section.12}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {12.1}Basic}{76}{subsection.12.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {12.1.1}Active vs. Inactive Inequality Constraints}{76}{subsubsection.12.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {12.1.2}ICP $\rightarrow $ ECP}{76}{subsubsection.12.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {12.1.3}Intuition $\mu _j\geq 0, \forall j\in A(x^*)$}{77}{subsubsection.12.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {12.1.4}Complementary Slackness}{77}{subsubsection.12.1.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {12.2}Karush–Kuhn–Tucker (KKT) Necessary Conditions}{77}{subsection.12.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {12.3}Karush–Kuhn–Tucker (KKT) Sufficient Conditions}{79}{subsection.12.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {12.4}General Sufficiency Condition}{82}{subsection.12.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {12.5}Barrier Method}{83}{subsection.12.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {12.6}An Exmaple Using KKT or Barrier}{84}{subsection.12.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {12.6.1}Solution using KKT conditions}{84}{subsubsection.12.6.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {12.6.2}Solution using logarithmic barrier}{85}{subsubsection.12.6.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {12.7}Penalty Method (For ECP)}{85}{subsection.12.7}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {13}Duality}{86}{section.13}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {13.1}Weak Duality Theorem: $\max _{(\lambda ,\mu )\in G}D(\lambda ,\mu )\leq \min _{x\in F}f(x)$}{87}{subsection.13.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {13.2}Strong Duality Theorem: under some conditions, $\max _{(\lambda ,\mu )\in G}D(\lambda ,\mu )= \min _{x\in F}f(x)$}{88}{subsection.13.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {13.2.1}Slater's sufficient condition for strong duality}{89}{subsubsection.13.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {13.2.2}Example}{90}{subsubsection.13.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {13.3}Dual of Linear Program}{90}{subsection.13.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {14}Augmented Lagrangian Method (adjusted penalty method)}{91}{section.14}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {14.1}Motivation}{91}{subsection.14.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {14.2}Augemented Lagrangian Method}{92}{subsection.14.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {14.2.1}Method of Multipliers}{93}{subsubsection.14.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {15}Sub-gradient Methods}{95}{section.15}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {15.1}Sub-gradient}{95}{subsection.15.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {15.2}Sub-differential}{96}{subsection.15.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {15.3}More examples}{96}{subsection.15.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {15.4}First-order necessary conditions for optimality in terms of subgradient}{98}{subsection.15.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {15.5}Properties of Subgradients}{98}{subsection.15.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {15.6}Sub-gradient Descent for Unconstrained Optimization}{98}{subsection.15.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {15.7}(Revised) Sub-gradient "descent" with diminishing stepsize}{100}{subsection.15.7}\protected@file@percent }
\gdef \@abspage@last{101}
