%!TEX program = xelatex
\documentclass[11pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{ctex}
\usepackage{authblk}
\usepackage{tikz}
\usepackage{pgfplots}
\usepackage{verbatim}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{indentfirst}
\usepackage{amssymb}
\usepackage{enumerate}
\setlength{\parindent}{0pt}
\usetikzlibrary{shapes,snakes}
\newcommand{\argmax}{\operatornamewithlimits{argmax}}
\newcommand{\argmin}{\operatornamewithlimits{argmin}}

\DeclareMathOperator{\col}{col}
\usepackage{booktabs}
\newtheorem{theorem}{Theorem}
\newtheorem{proposition}{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem{example}{Example}
\newtheorem{corollary}{Corollary}
\newtheorem{definition}{Definition}
\newtheorem{note}{Note}
\newtheorem{claim}{Claim}
\usepackage{graphicx}
\usepackage{geometry}
\usepackage{hyperref}
\newcommand{\code}{	exttt}
\geometry{a4paper,scale=0.8}
\title{Optimization}
\author[*]{Wenxiao Yang}
\affil[*]{Department of Mathematics, University of Illinois at Urbana-Champaign}
\date{2022}





\begin{document}
\maketitle
\tableofcontents
\newpage

\section{Unconstrained Optimization}
\subsection{Conditions for Optimality}
Function: $f:\mathbb{R}^n \rightarrow	\mathbb{R}^n$, $x\in \&,\ \&\subseteq \mathbb{R}^n$.

Terminology: $x^*$ will always be the optimal input at some function.

\subsection{Global minimizer, Local minimizer}
\begin{definition}
    \quad\\
    Say $x^*$ is a \underline{global minimizer(minimum)} of $f$ if $f(x^*)\leq f(x), \forall x\in \&$.

    Say $x^*$ is a \underline{unique global minimizer(minimum)} of $f$ if $f(x^*)< f(x), \forall x\neq x^*$.

    Say $x^*$ is a \underline{local minimizer(minimum)} of $f$ if $\exists r>0$ so that $f(x^*)\leq f(x)$ when $\|x-x^*\|<r$.
\end{definition}

A minimizer is \underline{strict} if $f(x^*)< f(x)$ for all relevant $x$.

\subsection{Optimization in $\mathbb{R}$}
\subsubsection{Theorem: local minimizer $\Rightarrow f'(x^*)=0$}

\begin{theorem}
If $f(x)$ is differentiable function and $x^*$ is a local minimizer, then $f'(x^*)=0$.
\end{theorem}

\begin{proof}
\quad\\
Def of $f'(x)=\lim_{h \rightarrow 0} \frac{f(x+h)-f(x)}{h}$\\
Def of local minimizer: $f(x^*)-f(x)\geq 0, |x^*-x|<r$\\
when $0<h<r$, $\frac{f(x+h)-f(x)}{h}\geq 0$; when $-r<h<0$, $\frac{f(x+h)-f(x)}{h}\leq 0$. Then $f'(x)=0$.
\end{proof}

\subsubsection{Theorem: $f'(x^*)=0, f''(x^*)\geq 0 \Rightarrow$ local minimizer}
\begin{theorem}
    If $f:\mathbb{R} \rightarrow \mathbb{R}$ is a function with a continuous second derivative and $x^*$ is a critical point of $f$ (i.e. $f'(x)=0$), then:\\
    (1): If $f''(x)\geq 0,\ \forall x\in\mathbb{R}$, then $x^*$ is a global minimizer on $\mathbb{R}$.\\
    (2): If $f''(x)\geq 0,\ \forall x\in[a,b]$, then $x^*$ is a global minimizer on $[a,b]$.\\
    (3): If we only know $f''(x^*)\geq 0$, $x^*$ is a local minimizer.
\end{theorem}
\begin{proof}
\quad\\
(1)$f(x)=f(x^*)+f'(x^*)(x-x^*)+\frac{1}{2}f''(\xi)(x-x^*)^2=f(x^*)+0+\textit{something non negative}\geq f(x^*)\  \forall x$\\
(2) Similar to (1)\\
(3)$f''(x^*)\geq 0,\ f''$ continuous $\Rightarrow \exists r$ s.t. $f''(x)\geq 0$ $\forall x\in[x^*-\frac{r}{2},x^*+\frac{r}{2}]$, then $x$ is a local minimizer.
\end{proof}


\subsection{Optimization in $\mathbb{R}^n$}
\subsubsection{Necessary Conditions for Optimality: Local Extremum $\Rightarrow \nabla f(x^*)=0$}
A base point $x$, we consider an arbitrary direction $u$. $\{x+tu| t\in \mathbb{R}\}$

For $\alpha>0$ sufficiently small:
\begin{enumerate}
    \item $f(x^*)\leq f(x^*+\alpha u)$
    \item $g(\alpha)=f(x^*+tu)-f(x^*)\geq 0$
    \item $g(\beta)$ is continuously differentiable for $\beta\in[0,\alpha]$
\end{enumerate}

By chain rule, $$g'(\beta)=\sum_{i=1}^n \frac{\partial f}{\partial x_i}(x^*+\beta u)u_i$$

By Mean Value Theorem, $$g(\alpha)=g(0)+g'(\beta)\alpha\text{ for some }\beta\in[0,\alpha]$$
Thus $$g(\alpha)=\alpha\sum_{i=1}^n \frac{\partial f}{\partial x_i}(x^*+\beta u)u_i\geq 0$$
$$\Rightarrow \sum_{i=1}^n \frac{\partial f}{\partial x_i}(x^*+\beta u)u_i\geq 0$$
Letting $\alpha \rightarrow	0$ and hence $\beta \rightarrow	0$, we get $$\sum_{i=1}^n \frac{\partial f}{\partial x_i}(x^*)u_i\geq 0\text{ for all }u\in \mathbb{R}^n$$
By choosing $u=[1,0,...,0]^T$, $u=[-1,0,...,0]^T$, we get $$\frac{\partial f(x^*)}{\partial x_1}\geq 0,\ \frac{\partial f(x^*)}{\partial x_1}\leq 0 \Rightarrow	\frac{\partial f(x^*)}{\partial x_1}= 0$$
Similarly, we can get $$\nabla f(x^*)=[\frac{\partial f(x^*)}{\partial x_1},\frac{\partial f(x^*)}{\partial x_2},...,\frac{\partial f(x^*)}{\partial x_n}]^T=0$$

\begin{theorem}
If $f$ is continuously differentiable and $x^*$ is a local extremum. Then $\nabla f(x^*)=0$.
\end{theorem}

\subsubsection{Stationary Point, Saddle Point}
All points $x^*$ s.t. $\nabla f(x^*)=0$ are called \underline{stationary points}.

Thus, all extrema are stationary points.

But not all stationary points have to be extrema.

\underline{Saddle points} are the stationary points neither local minimum nor local maximum.

\begin{example}
$f(x)=x^3$, $x=0$ is a stationary point but not extrema. (saddle point)
\end{example}

\subsubsection{Second Order Necessary Condition}
\begin{definition}
The Hessian of $f$ at point $x$ is an $n\times n$ symmetric matrix denoted by $\nabla^2 f(x)$ with $[\nabla^2 f(x)]_{ij}=\frac{\partial^2 f(x)}{\partial x_i\partial x_j}$
\end{definition}
\begin{theorem}
Suppose $f$ is twice continuously differentiable and $x^*$ in local \underline{minimum}. Then $$\nabla f(x^*)=0\text{ and }\nabla^2 f(x^*)\succeq 0$$
\end{theorem}
\begin{proof}
\quad\\
$\nabla f(x^*)=0$ already proved before.

Let $\alpha$ be small enough so that $g(\alpha)=f(x^*+\alpha u)-f(x^*)\geq 0$.

By Taylor series expansion,
\begin{equation}
    \begin{aligned}
        g(\alpha)&=g(0)+\alpha g'(0)+\frac{\alpha^2}{2}g''(0)+O(\alpha^2)\\
        g'(\alpha)&=\sum_{i=1}^n \frac{\partial f}{\partial x_i}(x^*+\beta u)u_i=\nabla f(x^*+\alpha u)^T u\\
        g''(\alpha)&=\sum_{i=1}^n\sum_{j=1}^n \frac{\partial^2 f}{\partial x_i\partial x_j}(x^*+\beta u)u_iu_j=u^T\nabla^2 f(x^*+\alpha u) u
    \end{aligned}
    \nonumber
\end{equation}
\begin{equation}
    \begin{aligned}
        g'(0)=\nabla f(x^*)^T u=0;\ g''(0)=u^T\nabla^2 f(x^*) u\\
        g(\alpha)=\frac{\alpha^2}{2}u^T\nabla^2 f(x^*) u+O(\alpha^2)\geq 0\\
        \text{When }\alpha \rightarrow 0,\text{ we get } u^T\nabla^2 f(x^*) u\geq 0,\ \forall u\in \mathbb{R}^n\\
        \Rightarrow	\nabla^2 f(x^*)\succeq 0
    \end{aligned}
    \nonumber
\end{equation}
\end{proof}

\subsubsection{Sufficient Conditions for Optimality}
\begin{theorem}
Suppose $f$ is twice continuously differentiable in a neighborhood of $x^*$ and
(1) $\nabla f(x^*)=0$; (2) $\nabla^2 f(x^*)\succ 0$ ($u^T\nabla^2 f(x^*) u>0$, $\forall u\in \mathbb{R}^n$).
Then $x^*$ is local minimum.
\end{theorem}
\begin{proof}
\quad\\
Consider $u\in \mathbb{R}^n$, $\alpha>0$ and let
\begin{equation}
    \begin{aligned}
        g(\alpha)&=f(x^*+\alpha u)-f(x^*)\\
        &=\frac{\alpha^2}{2}u^T\nabla^2 f(x^*) u+O(\alpha^2)\geq 0\\
        &=\frac{\alpha^2}{2}[u^T\nabla^2 f(x^*) u+2\frac{O(\alpha^2)}{\alpha^2}]\\
        &u^T\nabla^2 f(x^*) u>0;\ \frac{O(\alpha^2)}{\alpha^2}\rightarrow 0\\
        &\Rightarrow g(\alpha)>0\text{ for }\alpha\text{ sufficiently small for all }u\neq 0\\
        &\Rightarrow x^*\text{ is local minimum}.
    \end{aligned}
    \nonumber
\end{equation}

(specially if $\|u\|=1$, $u^T\nabla^2 f(x^*) u\geq \lambda_{\min}(\nabla^2 f(x^*))$, $\lambda_{\min}(\nabla^2 f(x^*))$ is the minimal eigenvalues of $\nabla^2 f(x^*)$.)
\end{proof}






\subsubsection{Using Optimality Conditions to Find Minimum}
\begin{enumerate}
    \item Find all points satisfying necessary condition $\nabla f(x)=0$ (all stationary points)
    \item Filter out points that don't satisfy $\nabla^2 f(x)\geq 0$
    \item Points with $\nabla^2 f(x)> 0$ are strict local minimum.
    \item Among all points with $\nabla^2 f(x)\geq 0$, declare a global minimum, one with the smallest value of $f$, assuming that global minimum exists.
\end{enumerate}
\begin{example}
$f(x)=2x^2-x^4$
\end{example}
\begin{equation}
    \begin{aligned}
        f'(x)&=4x-4x^3=0\\
        \Rightarrow& x=0,x=1,x=-1\text{ are stationary points}\\
        f''(x)&=4-12x^2=\left\{\begin{matrix}
            4&\text{if }x=0\\
            -8&\text{if }x=1,-1
        \end{matrix}\right.\\
        \Rightarrow	&x=0\text{ is the only local min, and it is strict}
    \end{aligned}
    \nonumber
\end{equation}
But $-f(x) \rightarrow \infty$ as $|x|\rightarrow \infty \Rightarrow$ no global min, but global max exists. $f(1),f(-1)$ are strict local max and both global max.

\subsubsection{Fix Conditions for Global Optimality}
\textbf{Claim 1}: Consider a differentiable function $f$. Suppose:

(C1) $f$ has at least one global minimizer;

(C2) The set of stationary points is $S$, and $f\left(x^{*}\right) \leq f(x), \forall x \in S$.

Then $x^{*}$ is a global minimizer of $f^{*}$.
\begin{proof}
\quad\\
Suppose $\hat{x}$ is a global minimizer of $f$, i.e.,
$$
f(\hat{x}) \leq f(x), \forall x .
$$
By the necessary optimality condition, we have $\nabla f(\hat{x})=0$, thus $\hat{x} \in S$. By (C2), we have
$$
f\left(x^{*}\right) \leq f(\hat{x}) .
$$
Combining the two inequalities, we have $f(\hat{x}) \leq f\left(x^{*}\right) \leq f(\hat{x})$, thus $f(\hat{x})=f\left(x^{*}\right)$. Plugging into the second inequality, we have $f\left(x^{*}\right) \leq f(x), \forall x$. Thus $x^{*}$ is a global minimizer of $f^{*} .$
\end{proof}



\subsection{Optimization in a Set}
$$\begin{array}{ll}\text { minimize } & f(x) \\ \text { subject to } & x \in X\end{array}$$
- Objective function $f: \mathbb{R}^{n} \rightarrow \mathbb{R}$ is a continuous function

- Optimization variable $x \in X$

- Local minimum of $f$ on $X: \exists \epsilon>0$ s.t. $f(x) \geq f(\hat{x})$, for all $x \in X$ such that $\|x-\hat{x}\| \leq \epsilon$;

i.e., $x^{*}$ is the best in the intersection of a small neighborhood and $X$

- Global minimum of $f$ on $X: f(x) \geq f\left(x^{*}\right)$ for all $x \in X$

"Strict global minimum", "strict local minimum" "local maximum", "global maximum" of $f$ on $X$ are defined accordingly

\subsubsection{Existence of Global-min}
\begin{theorem}[Bolzano-Weierstrass Theorem (compact domain)]
    Any continuous function $f$ has at least one global minimizer on any \textbf{compact set} $X$.

    That is, there exists an $x^{*} \in X$ such that $f(x) \geq f\left(x^{*}\right), \forall x \in X$.
\end{theorem}

\begin{corollary}[bounded level sets]
    Suppose $f: \mathbb{R}^{d} \rightarrow \mathbb{R}$ is a continuous function. If for a certain $c$, the level set
    $$
    \{x \mid f(x) \leq c\}
    $$
    is \textbf{non-empty} and \textbf{compact}, then the global minimizer of $f$ exists, i.e., there exists $x^{*} \in \mathbb{R}^{d}$ s.t.
    $$
    f\left(x^{*}\right)=\inf _{x \in \mathbb{R}^{d}} f(x)
    $$
\end{corollary}
\begin{example}
    $f(x) = x^2$.
    Level set $\{x|x^2 \leq 1\}$ is $\{x|âˆ’1\leq x\leq 1\}$: non-empty compact. Thus there exists a global minimum.
\end{example}
\begin{corollary}[coercive]
    Suppose $f: \mathbb{R}^{d} \rightarrow \mathbb{R}$ is a continuous function. If $f(x) \rightarrow \infty$ as $\|x\| \rightarrow \infty$, then the global minimizer of $f$ over $\mathbb{R}^{d}$ exists.
\end{corollary}
\begin{proof}
Let $\alpha\in \mathbb{R}^d$ be chosen so that the set $S = \{x |f(x) \leq \alpha\}$ is non-empty. By coercivity,
this set is compact.
\end{proof}
Coercive $\Rightarrow$ one non-empty bounded level set; but not the other way.

Claim (all level sets bounded $\Leftrightarrow$ coercive): Let $f$ be a continuous function, then $f$ is coercive iff $\{x | f(x) \leq \alpha\}$ is compact for any $\alpha$.

\subsection{Method of finding-global-min-among-stationary-points (FGMSP)}
Method of finding-global-min-among-stationary-points (FGMSP):

Step 0: Verify coercive or bounded level set:

- Case 1: success, go to Step $1 .$

- Case 2: otherwise, try to show non-existence of global-min. If success, exit and report "no global-min exists".

- Case 3: cannot verify coercive or bounded level set; cannot show non-existence of global-min. Exit and report "cannot decide".

Step 1: Find all stationary points (candidates) by solving $\nabla f(\mathbf{x})=0$;

Step 2 (optional): Find all candidates s.t. $\nabla^{2} f(\mathbf{x}) \succeq 0$.

Step 3: Among all candidates, find one candidate with the minimal value. Output this candidate, and report "find a global $\mathrm{min}$ ".






\section{Convexity}
\subsection{Definition}
\textbf{Convex set} $C: x, y \in C$ implies $\lambda x+(1-\lambda) y \in C$, for any $\lambda \in[0,1]$.

\textbf{Convex function} (0-th order): $f$ is convex in a convex set $C$ iff $f(\alpha x+(1-\alpha) y) \leq \alpha f(x)+(1-\alpha) f(y), \forall x, y \in C, \forall \alpha \in[0,1] .$

Property (1st order) If $f$ is differentiable, then $f$ is convex iff $f(z) \geq f(x)+(z-x)^{T} \nabla f(x), \ \forall x, z \in C .$ The inequality is strict for strict convexity.
\begin{proof}
\quad\\
\begin{enumerate}[(i)]
    \item "$\Rightarrow$" \begin{equation}
        \begin{aligned}
            f(x+\alpha (y-x))&\leq (1-\alpha)f(x)+\alpha f(y), \forall \alpha \in (0,1)\\
            \Rightarrow	\frac{f(x+\alpha(y-x))-f(x)}{\alpha}&\leq f(y)-f(x)\\
            \text{Limit as }\alpha \rightarrow 0 \Rightarrow (y-x)^{T} \nabla f(x)&\leq f(y)-f(x)
        \end{aligned}
        \nonumber
    \end{equation}
    \item "$\Leftarrow$" Let $g=\alpha x+(1-\alpha) y$
    \begin{equation}
        \begin{aligned}
            f(g)+(x-g)^{T} \nabla f(g)&\leq f(x)\\
            f(g)+(y-g)^{T} \nabla f(g)&\leq f(y)\\
            \Rightarrow	f(g)&\leq \alpha f(x)+(1-\alpha)f(y)\\
            f(\alpha x+(1-\alpha) y)&\leq \alpha f(x)+(1-\alpha)f(y)
        \end{aligned}
        \nonumber
    \end{equation}
\end{enumerate}
\end{proof}

Property (2nd order): If $f$ is twice differentiable, then $f$ is convex iff
$$
\nabla^{2} f(x) \succeq 0,\ \forall x \in C .
$$
Strictly convex: $\nabla^{2} f(x) \succ 0,\ \forall x \in C \Rightarrow	$ $f$ is strictly convex.

\textbf{Note:} $f$ is strictly convex $\nRightarrow \nabla^{2} f(x) \succ 0$.
\begin{example}
$f(x)=x^4\text{(strictly convex)}$, $\frac{d^2f(x)}{dx^2}=12x^2(=0\text{ at }x=0)$
\end{example}

A function $f$ is a \textbf{concave function} iff $-f$ is a convex function.

\textbf{Convex set graph}:
\begin{center}\begin{figure}[htbp]
    \centering
    \includegraphics[scale=0.3]{Convex_set.png}
    \caption{}
    \label{}
\end{figure}\end{center}

\begin{claim}
Suppose $f$ is a convex function over $\mathbb{R}^n$ and define the set $$C=\{x\in \mathbb{R}^n| f(x)\leq a\}, a\in \mathbb{R}$$ then $C$ is a convex set.
\end{claim}

\begin{claim}
If $f_1,f_2,...,f_k$ are convex functions over convex set $\&$,
\begin{enumerate}
    \item $f_{sum}(x)=\sum_{i=1}^kf_i(x)$ is convex over $\&$
    \item $f_{max}(x)=\max_{i=1,...,k}f_i(x)$ is convex over $\&$
\end{enumerate}
\end{claim}
\begin{proof}
\quad\\
(2)
\begin{equation}
    \begin{aligned}
        f_{max}(\alpha x+(1-\alpha)y)&=\max_{i=1,...,k}f_i(\alpha x+(1-\alpha)y)\\
    &\leq \max_{i=1,...,k}[\alpha f_i(x)+(1-\alpha)f_i(y)]\\
    &\leq \max_{i=1,...,k}\alpha f_i(x)+\max_{i=1,...,k}(1-\alpha)f_i(y)\\
    &=\alpha f_{max}(x)+(1-\alpha)f_{max}(y)
    \end{aligned}
    \nonumber
\end{equation}
\end{proof}












\subsection{Convex$\Rightarrow$Stationary point is global-min}
\begin{proposition}
    Let $f: X \longmapsto \mathbb{R}$ be a convex function over the convex set $X$.

    (a) A local-min of $f$ over $X$ is also a global-min over $X$. If $f$ is strictly convex,then min is unique.

    (b) If $X$ is open (e.g. $\mathbb{R}^{n}$ ), then $\nabla f\left(x^{*}\right)=0$ is a necessary and sufficient condition for $x^{*}$ to be a global minimum.
\end{proposition}
\begin{proof}
\quad\\
Proof based on a property: If $f$ is differentiable over $C$ (open), then $f$ is convex iff
$$
f(z) \geq f(x)+(z-x)^{\prime} \nabla f(x), \quad \forall x, z \in C .
$$
\end{proof}














\begin{corollary}
    Let $f: X \longmapsto \mathbb{R}$ be a concave function over the convex set $X$.

    (a) A local-max of $f$ over $X$ is also a global-max over $X$.

    (b) If $X$ is open (e.g. $\mathbb{R}^{n}$ ), then $\nabla f\left(x^{*}\right)=0$ is a necessary and sufficient condition for $x^{*}$ to be a global maximum.
\end{corollary}




\subsection{Unconstrained Quadratic Optimization}
$$\begin{array}{ll}\text { minimize } & f(\mathbf{w})=\frac{1}{2} \mathbf{w}^{T} \mathbf{Q} \mathbf{w}-\mathbf{b}^{T} \mathbf{w} \\ \text { subject to } & \mathbf{w} \in \mathbb{R}^{d}\end{array}$$
where $\mathbf{Q}$ is a symmetric $d \times d$ matrix. (what if non-symmetric?)
$$\nabla f(\mathbf{w})=\mathbf{Q}\mathbf{w}-\mathbf{b},\ \nabla^2 f(\mathbf{w})=\mathbf{Q}$$
\begin{enumerate}[(i)]
    \item $\mathbf{Q}\succeq 0 \Leftrightarrow	f$ is convex.
    \item $\mathbf{Q}\succ 0 \Leftrightarrow	f$ is strictly convex.
    \item $\mathbf{Q}\preceq 0 \Leftrightarrow	f$ is concave.
    \item $\mathbf{Q}\prec 0 \Leftrightarrow	f$ is strictly concave.
\end{enumerate}









- Necessary condition for (local) optimality
$$
\mathbf{Q} \mathbf{w}=\mathbf{b}, \quad \mathbf{Q} \succeq 0
$$

Case 1: $\mathbf{Q w}=\mathbf{b}$ has no solution, i.e. $\mathbf{b} \notin R(\mathbf{Q})$. No stationary point, no lower bound ($f$ can achieve $-\infty$).

Case 2: $\mathbf{Q}$ is not PSD ( $f$ is non-convex)
No local-min, no lower bound ($f$ can achieve $-\infty$).

Case 3: $\mathbf{Q} \succeq 0$ (PSD) and $\mathbf{b} \in R(\mathbf{Q})$. Convex, has global-min, 
any stationary point is a global optimal solution.

\begin{example}
    Toy Problem 1: $\min _{x, y \in \mathbb{R}} f(x, y) \triangleq x^{2}+y^{2}+\alpha x y$.
\end{example}
\begin{enumerate}
    \item Step 1: First order condition: $2 x^{*}+\alpha y^{*}=0,2 y^{*}+\alpha x^{*}=0$.
    
    - We get $4 x^{*}=-2 \alpha y^{*}=\alpha^{2} x^{*}$. So $\left(4-\alpha^{2}\right) x^{*}=0$.
    
    - Case 1: $\alpha^{2}=4$. If $x^{*}=-\alpha y^{*} / 2$, then $\left(x^{*}, y^{*}\right)$ is a stationary point.
    
    - Case 2: $\alpha^{2} \neq 4$. Then $x^{*}=0 ; y^{*}=-\alpha x^{*} / 2=0$. So $(0,0)$ is stat-pt.
    \item Step 2: Check convexity. Hessian $\nabla^{2} f(x, y)=\left(\begin{array}{ll}2 & \alpha \\ \alpha & 2\end{array}\right)$.
    
    Eigenvalues $\lambda_{1}, \lambda_{2}$ satisfy $\left(\lambda_{i}-2\right)^{2}=\alpha^{2}, i=1,2$.
    Thus $\lambda_{1,2}=2 \pm|\alpha|$.

    - If $|\alpha| \leq 2$, then $\lambda_{i} \geq 0, \forall i$. Thus $f$ is convex. Any stat-pt is global-min.

    - If $|\alpha|>2$, at least one $\lambda_{i}<0$, thus $f$ is not convex.
    \item Step 3 (can be skipped now): For non-convex case $(|\alpha|>2)$, prove no lower bound.
    
    $f(x, y)=(x+\alpha y / 2)+\left(1-\alpha^{2} / 4\right) y^{2}$. Pick $y=M, x=-\alpha M / 2$, then
    $f(x, y)=\left(1-\alpha^{2} / 4\right) M^{2} \rightarrow-\infty$ as $M \rightarrow \infty$.
\end{enumerate}

Summary:

If $|\alpha|>2$, no global-min, $(0,0)$ is stat-pt;

if $|\alpha|=2$, any $(-0.5 \alpha t, t), t \in \mathbb{R}$ is a stat-pt and global-min;

if $|\alpha|<2,(0,0)$ is the unique stat-pt and global-min.

\begin{example}
Linear Regression
\end{example}
$\text{minimize } f(\mathbf{w})=\frac{1}{2}\left\|\mathbf{X}^{T} \mathbf{w}-\mathbf{y}\right\|^{2}$ subject to $ \mathbf{w} \in \mathbb{R}^{d}$

$n$ data points, $d$ features

- $\mathbf{X}$ may be wide (under-determined), tall (over-determined), or rank-deficient

- Note that comparing with the previous case, $\mathbf{Q}=\mathbf{X X}^{T} \in \mathbb{R}^{d \times d}$, $\mathbf{b}=\mathbf{X} \mathbf{y} \in \mathbb{R}^{d \times 1}$

- $\mathbf{Q} \succeq 0$; Case 2 never happens!

- First order condition $\mathbf{X X}^{\top} \mathbf{w}^{*}=\mathbf{X} \mathbf{y}$.

\quad - It always has a solution; Case 1 never happens!

\textbf{Claim}: Linear regression problem is always convex; it has global-min.

First order condition
$$
\mathbf{X X}^{\top} \mathbf{w}^{*}=\mathbf{X} \mathbf{y}
$$
which always has a solution.

If $X X^{\top} \in \mathbb{R}^{d \times d}$ is invertible (only happen when $n \geq d$ ), then there is a unique stationary point $x=\left(A^{\top} A\right)^{-1} A^{\top} b$. It is also a global minimum.

If $X X^{\top} \in \mathbb{R}^{d \times d}$ is not invertible, then there can be infinitely many stationary points, which are the solutions to the linear equation.
All of them are global minima, giving the same function value.

\subsection{Strongly Convexity}
\subsubsection{$\mu$-Strongly Convex}
\textbf{Definition}: We say $f: C \rightarrow \mathbb{R}$ is a $\mu$-strongly convex function in a convex set $C$ if $f$ is differentiable and
$$
\langle\nabla f(w)-\nabla f(v), w-v\rangle \geq \mu\|w-v\|^{2}, \quad \forall w, v \in C .
$$
If $f$ is twice differentiable, then $f$ is $\mu$-strongly convex iff
$$
\nabla^{2} f(x) \succeq \mu I, \quad \forall x \in C .
$$
\begin{definition}
    A twice continuously differentiable function is \underline{strongly convex} if $$\exists m>0\text{ s.t. }\nabla^2 f(x)\succeq mI\quad \forall x$$
    which is also called $m-$strongly convex.
\end{definition}
Namely, all eigenvalues of the Hessian at any point is at least $\mu$.

if $f(w)$ is convex, then $f(w)+\frac{\mu}{2}\|w\|^{2}$ is $\mu$-strongly convex.

- In machine learning, easy to change a convex function to a strongly convex function: just add a regularizer

\subsubsection{Lemma: Strongly convexity $\Rightarrow$ Strictly convexity}
\begin{lemma}
    Strongly convexity $\Rightarrow$ Strictly convexity.
\end{lemma}
\begin{proof}
\begin{equation}
    \begin{aligned}
        \nabla^2 f(x)&\succeq mI \Rightarrow \nabla^2 f(x)-mI\succeq 0\\
        & \Rightarrow \forall \mathfrak{Z}\neq 0\quad \mathfrak{Z}^T(\nabla^2 f(x)-mI)\mathfrak{Z}\geq 0\\
        & \Rightarrow \mathfrak{Z}^T\nabla^2 f(x)\mathfrak{Z}\geq m\mathfrak{Z}^T\mathfrak{Z}>0
    \end{aligned}
    \nonumber
\end{equation}
\end{proof}
\textbf{Note: }converse is not true: e.g. $f(x)=x^4$ is strictly convex but $\nabla^2 f(0)=0$

\subsubsection{Lemma: $\nabla^2 f(x)\succeq mI$ $\Rightarrow$ $f(y)\geq f(x)+\nabla f(x)^T(y-x)+\frac{m}{2}\|y-x\|^2$}
\begin{lemma}
$\nabla^2 f(x)\succeq mI\quad \forall x$

$$\Rightarrow f(y)\geq f(x)+\nabla f(x)^T(y-x)+\frac{m}{2}\|y-x\|^2$$
\end{lemma}
\begin{proof}
    By Taylor's Theorem,
    \begin{equation}
        \begin{aligned}
            f(y)&=f(x)+\nabla f(x)^T(y-x)+\frac{1}{2}(y-x)^T \nabla^T f((1-\beta)x+\beta y)(y-x),\quad \text{for some }\beta\in[0,1]\\
            &\geq f(x)+\nabla f(x)^T(y-x)+\frac{1}{2}(y-x)^Tm(y-x)\\
            &\geq f(x)+\nabla f(x)^T(y-x)+\frac{m}{2}\|y-x\|^2
        \end{aligned}
        \nonumber
    \end{equation}
\end{proof}

















\section{Gradient Methods}
\begin{definition}[Iterative Descent]
Start at some point $x_0$, and successively generate $x_1,x_2,..$ s.t. $$f(x_{k+1})<f(x_k)\quad k=0,1,...$$
\end{definition}

\begin{definition}[\textbf{General Gradient Descent Algorithm}]
    Assume that $\nabla f(x_k)\neq 0$. Then
    $$x_{k+1}=x_k+\alpha_k d_k$$
    where $d_k$ is s.t. $d_k$ has a positive projection along $-\nabla f(x_k)$,
    $$\nabla f(x_k)^T d_k<0 \equiv -\nabla f(x_k)^T d_k>0$$
\end{definition}
\begin{enumerate}[$\bullet$]
    \item If $d_k=-\nabla f(x_k)$ we get \textbf{steepest descent}.
    \item Often $d_k$ is constructed using matrix $D_k \succ 0$ $$d_k=-D_k\nabla f(x_k)$$
\end{enumerate}

\subsection{Steepest Descent}
We want the $x_k$ that decreases the function most.
\begin{proposition}
$-\nabla f(x_k)$ is the direction deceases the function most.
\end{proposition}
\begin{proof}
Suppose the direction is $v\in \mathbb{R}^n, v\neq 0$.
$$f(x+\alpha v)=f(x)+\alpha v^T \nabla f(x)+O(\alpha)$$
The rate of change of $f$ along direction $v$:
$$\lim_{\alpha \rightarrow 0}\frac{f(x+\alpha v)-f(x)}{\alpha}=v^T\nabla f(x)$$
By Cauchy-schwarz inequality,
$$|v^T\nabla f(x)|\leq \|v\|\|\nabla f(x)\|$$
Equation holds when $v=\beta \nabla f(x)$. Hence, $-\nabla f(x)$ is the direction decreases the function most.
\end{proof}

\begin{definition}[\textbf{Steepest Descent Algorithm}]
$$x_{k+1}=x_k-\alpha_k \nabla f(x_k)$$
$\alpha_k$ is the step size, which need to choose carefully.
\end{definition}

\subsection{Methods for Choosing $\alpha_k$}
\begin{enumerate}[Method (1):]
    \item Fixed step size: $\alpha_k=\alpha$ (can have issue with \textit{convergence})
    \item \textbf{Optimal Line Search}: choose $\alpha_k$ to optimize the value of next iteration, i.e. solve $$\min_{\alpha\geq 0}f(x_k+\alpha d_k)$$ (may be \textit{difficult in practice})
    \item \textbf{Armijo's Rule} (successive step size reduction):$$f(x_k+\alpha_k d_k)=f(x_k)+\alpha_k \nabla f(x_k)^T d_k+O(\alpha_k)$$
    Since $\nabla f(x_k)^T d_k<0$, $f$ decreases when $\alpha_k$ is sufficiently small. But we also don't want $\alpha_k$ to be too small (slow).

\end{enumerate}

\subsection{Armijo's Rule}
\begin{enumerate}[(i)]
    \item Initialize $\alpha_k=\tilde{\alpha}$. Let $\sigma,\beta \in (0,1)$ be prespecified paramenters.
    \item If $f(x_k)-f(x_k+\alpha_k d_k)\geq-\sigma \alpha_k \nabla f(x_k)^T d_k$, stop.
    
    (Which shows $f(x_k+\alpha_k d_k)$ is at least smaller than $f(x_k)$ in a degree that correlated with $\nabla f(x_k)^T d_k$)
    \item Else, set $\alpha_k=\beta\alpha_k$ and go back to step 2. (use a smaller $\alpha_k$)
    
    Termination at \underline{smallest integer $m$} s.t. $$f(x_k)-f(x_k+\beta^m \tilde{\alpha} d_k)\geq-\sigma\beta^m \tilde{\alpha}\nabla f(x)^T d_k$$

    In Bersekas's book: $\sigma\in[10^{-5},10^{-1}],\beta\in[\frac{1}{10},\frac{1}{2}]$.

    As $\sigma,\beta$ are smaller, the algorithm is quicker.
\end{enumerate}

\subsection{Armijo's Rule for Steepest Descent}
$\alpha_k=\tilde{\alpha}\beta^{m_k}$, where $m_k$ is smallest $m$ s.t. $$f(x_k)-f(x_k-\tilde{\alpha}\beta^{m} \nabla f(x_k))\geq \sigma\tilde{\alpha}\beta^{m}\|\nabla f(x)\|^2$$

\begin{proposition}
Assume $\inf_x f(x)>-\infty$. Then every limit point of $\{x_k\}$ for steepest descent with Armijo's rule is a \underline{stationary point} of $f$.
\end{proposition}
\begin{proof}
Assume that $\bar{x}$ is a limit point of $\{x_k\}$ s.t. $\nabla f(\bar{x})\neq 0$.
\begin{enumerate}[$\bullet$]
    \item Since $\{f(x_k)\}$ is monotonically non-increasing and bounded below, $\{f(x_k)\}$ converges.
    \item $f$ is continuous $\Rightarrow$ $f(\bar{x})$ is a limit point of $\{f(x_k)\}$ $\Rightarrow$ $\lim_{k \rightarrow \infty}f(x_k)=f(\bar{x})$ $\Rightarrow$ $f(x_k)-f(x_{k+1})\rightarrow 0$
    \item By definition of Armijo's rule: $$f(x_k)-f(x_{k+1})\geq \sigma\alpha_k\|\nabla f(x_k)\|^2$$
    
    Hence, $\sigma\alpha_k\|\nabla f(x_k)\|^2 \rightarrow 0$.

    Since $\nabla f(\bar{x})\neq 0$, $\lim_{k \rightarrow \infty}\alpha_k=0$

    $$ln\alpha_k=ln (\tilde{\alpha}\beta^{m_k})=ln\tilde{\alpha}+m_kln\beta \Rightarrow	m_k=\frac{ln\alpha_k-ln\tilde{\alpha}}{ln\beta}\Rightarrow \lim_{k \rightarrow \infty}m_k=\infty$$
    Exist $\bar{k}$ s.t. $m_k>1,\forall k>\bar{k}$
    $$f(x_k)-f(x_k-\frac{\alpha_k}{\beta}\nabla f(x_k))<\sigma\frac{\alpha_k}{\beta}\|\nabla f(x_k)\|^2,\forall k>\bar{k}$$
    By Taylor's Theorem,
    $$f(x_k-\frac{\alpha_k}{\beta}\nabla f(x_k))=f(x_k)-\nabla f(x_k-\frac{\bar{\alpha}_k}{\beta}\nabla f(x_k))^T\frac{\alpha_k}{\beta}\nabla f(x_k)$$ for some $\bar{\alpha}_k\in(0,\alpha_k)$

    Hence, \begin{equation}
        \begin{aligned}
            \nabla f(x_k-\frac{\bar{\alpha}_k}{\beta}\nabla f(x_k))^T\frac{\alpha_k}{\beta}\nabla f(x_k)&<\sigma\frac{\alpha_k}{\beta}\|\nabla f(x_k)\|^2\\
            \nabla f(x_k-\frac{\bar{\alpha}_k}{\beta}\nabla f(x_k))^T\nabla f(x_k)&<\sigma\|\nabla f(x_k)\|^2,
            \forall k>\bar{k}\\
            \text{As }\alpha_k \rightarrow 0& \Rightarrow \bar{\alpha}_k\rightarrow 0\\
            \|\nabla f(x_k)\|^2&<\sigma\|\nabla f(x_k)\|^2
        \end{aligned}
        \nonumber
    \end{equation}
    Which contradicts to $\sigma<1$.
\end{enumerate}
\end{proof}


\section{Convergence of GD with Constant Stepsize}
\subsection{Lipschitz Gradient ($L$-Smooth)}
\begin{definition}[Lipschitz Continunity]
A function $g: \mathbb{R}^n \rightarrow	\mathbb{R}^m$ is called Lipschitz (continuous) if $\exists L>0$ s.t.
$$\|g(y)-g(x)\|\leq L\|y-x\|,\forall x,y\in \mathbb{R}^n$$
$L$ is Lipschitz constant. $g$ is called $L$-smooth.
\end{definition}

\begin{definition}[Lipschitz Gradient]
    $\nabla f(x)$ is Lipschitz if $\exists L>0$ s.t. $$\|\nabla f(x)-\nabla f(y)\|\leq L\|x-y\|,\forall x,y\in \mathbb{R}^n$$
\end{definition}

\begin{example}
    \quad

\begin{enumerate}
    \item $f(x)=\|x\|^4$, $\nabla f(x)=4\|x\|^2x$
    
    Test $\|\nabla f(x)-\nabla f(-x)\|\leq L\|2x\|$, $8\|x\|^2\|x\|\leq 2L\|x\|$ which doesn't hold when $\|x\|^2>\frac{L}{4}$.
    \item If $f$ is twice continuously differentiable with $\nabla^2 f(x)\succeq -MI$ and $\nabla^2 f(x)\preceq  MI$ then $\|\nabla f(x)-\nabla f(y)\|\leq M\|x-y\|,\forall x,y\in \mathbb{R}^n$. ($A\succeq B$ means $A-B\succeq 0$, $A\preceq B$ means $A-B\preceq 0$)
\end{enumerate}
\end{example}

\subsubsection{Theorem: $-MI\preceq\nabla^2 f(x)\preceq  MI$ $\Rightarrow$ $\nabla f(x)$ is Lipschitz with constant $M$}
\begin{theorem}
$-MI\preceq\nabla^2 f(x)\preceq  MI, \forall x \Rightarrow \|\nabla f(x)-\nabla f(y)\|\leq M\|x-y\|,\forall x,y$
\end{theorem}
\begin{proof}
For symmetric $A$,
\begin{enumerate}
    \item $x^TAx\leq \lambda_{\max}(A)\|x\|^2$
    \item $\lambda_{i}(A^2)=\lambda_i^2(A)$
    \item $-MI\preceq A\preceq  MI \Rightarrow \lambda_{\min}(A)\geq-M,\lambda_{\max}(A)\leq M$
\end{enumerate}
Define $g(t)=\frac{\partial f}{\partial x_i}(x+t(y-x))$. Then
\begin{equation}
    \begin{aligned}
        g(1)&=g(0)+\int_0^1g'(s)ds\\
        \Rightarrow	\frac{\partial f(y)}{\partial x_i}&=\frac{\partial f(x)}{\partial x_i}+\int_0^1\sum_{j=1}^n \frac{\partial^2 f(x+s(y-x))}{\partial x_i\partial x_j}(y_j-x_j)ds\\
        \nabla f(y)&=\nabla f(x)+\int_0^1 \nabla^2 f(x+s(y-x))(y-x)ds\\
        \|\nabla f(y)-\nabla f(x)\|&=\|\int_0^1 \nabla^2 f(x+s(y-x))(y-x)ds\|\\
        &\leq \int_0^1\|\nabla^2 f(x+s(y-x))(y-x)\|ds\\
        &=\int_0^1\sqrt{(y-x)^T[\nabla^2 f(x+s(y-x))]^2(y-x)}ds\\
        &(\text{Set }H=\nabla^2 f(x+s(y-x)))\\
        &\leq \int_0^1\sqrt{\lambda_{\max}(H^2)\|y-x\|^2}ds\\
        &\leq M\|y-x\|
    \end{aligned}
    \nonumber
\end{equation}
\end{proof}

\subsubsection{Descent Lemma: $\nabla f(x)$ is Lipschitz with constant $L$ $\Rightarrow	f(y)\leq f(x)+\nabla f(x)^T(y-x)+\frac{L}{2}\|y-x\|^2$}
\begin{lemma}[Descent Lemma]
Let $f: \mathbb{R}^n \rightarrow \mathbb{R}$ be continuously differentiable with a Lipschitz gradient with Lipschitz constant $L$. Then
\begin{equation}
    \begin{aligned}
        f(y)\leq f(x)+\nabla f(x)^T(y-x)+\frac{1}{2}L\|y-x\|^2
    \end{aligned}
    \nonumber
\end{equation}
\end{lemma}
\begin{proof}
Let $g(t)=f(x+t(y-x))$. Then $g(0)=f(x)$ and $g(1)=f(y)$, $g(1)=g(0)+\int_0^1g'(t)dt$.

Where $g'(t)=\nabla f(x+t(y-x))^T(y-x)$

\begin{equation}
    \begin{aligned}
        \Rightarrow	f(y)&=f(x)+\int_0^1\nabla f(x+t(y-x))^T(y-x)dt\\
        &=f(x)+\int_0^1(\nabla f(x+t(y-x))-\nabla f(x))^T(y-x)dt+\nabla f(x)^T(y-x)\\
        &\leq f(x)+\int_0^1\|\nabla f(x+t(y-x))-\nabla f(x)\|\|y-x\|dt+\nabla f(x)^T(y-x)\\
        &\leq f(x)+L\int_0^1\|t(y-x)\|\|y-x\|dt+\nabla f(x)^T(y-x)\\
        &=f(x)+\frac{1}{2}L\|y-x\|^2+\nabla f(x)^T(y-x)
    \end{aligned}
    \nonumber
\end{equation}
\end{proof}


\subsection{Convergence of Steepest Descent with Fixed Stepsize}
\subsubsection{Theorem: $f$ has Lipschitz gradient $\Rightarrow$ $\{x_k\}$ converges to stationary point}
\begin{theorem}
Consider the GD algorithm $$x_{k+1}=x_k-\alpha \nabla f(x_k),\quad k=0,1,...$$
Assume that $f$ has Lipschitz gradient with a Lipschitz gradient with Lipschitz constant $L$. Then if $\alpha$ is sufficiently small $(\alpha\in(0,\frac{2}{L}))$ and $f(x)\geq f_{\min}$ for all $x\in \mathbb{R}^n$,

(1). $f(x_{k+1})\leq f(x_k)-\alpha (1-\frac{L\alpha}{2})\|\nabla f(x_k)\|^2$

(2). $\sum_{k=0}^N\|\nabla f(x_k)\|^2\leq \frac{f(x_0)-f_{\min}}{\alpha(1-\frac{L\alpha}{2})}$

(3). every limit point of $\{x_k\}$ is a \underline{stationary point} of $f$.
\end{theorem}
\begin{proof}
    Applying the descent lemma,
    \begin{equation}
        \begin{aligned}
            f(x_{k+1})&\leq f(x_k)+\nabla f(x_k)^T(x_{k+1}-x_k)+\frac{L}{2}\|x_{k+1}-x_k\|^2\\
            &=f(x_k)-\alpha\nabla f(x_k)^T \nabla f(x_k)+\frac{L}{2}\alpha^2\|\nabla f(x_k)\|^2\\
            &=f(x_k)+\alpha (\frac{L\alpha}{2}-1)\|\nabla f(x_k)\|^2\\
        \end{aligned}
        \nonumber
    \end{equation}
    \begin{equation}
        \begin{aligned}
            \Rightarrow	\alpha (1-\frac{L\alpha}{2})\|\nabla f(x_k)\|^2&\leq f(x_k)-f(x_{k+1})\\
            \alpha \sum_{k=0}^N(1-\frac{L\alpha}{2})\|\nabla f(x_k)\|^2&\leq f(x_0)-f(x_{N+1})\\
            &\leq f(x_0)-f_{\min}
        \end{aligned}
        \nonumber
    \end{equation}
    If $\alpha\in(0,\frac{2}{L})$, i.e. $\alpha(1-\frac{L\alpha}{2})$,
    \begin{equation}
        \begin{aligned}
            \sum_{k=0}^N\|\nabla f(x_k)\|^2&\leq \frac{f(x_0)-f_{\min}}{\alpha(1-\frac{L\alpha}{2})}<\infty,\forall N\\
            \Rightarrow	\lim_{k \rightarrow	\infty}\nabla f(x_k)&=0
        \end{aligned}
        \nonumber
    \end{equation}
    If $\bar{x}$ is a limit point of $\{x_k\}$, $\lim_{k \rightarrow \infty}x_k=\bar{x}$.

    By continunity of $\nabla f$, $\nabla f(\bar{x})=0$
\end{proof}

\begin{example}
    $f(x)=\frac{1}{2}x^2,x\in \mathbb{R}$, $\nabla f(x)=x$, Lipschitz with $L=1$.
    \begin{equation}
        \begin{aligned}
            x_{k+1}&=x_k-\alpha \nabla f(x_k)\\
            &=x_k(1-\alpha)
        \end{aligned}
        \nonumber
    \end{equation}
    $0<\alpha<\frac{2}{L}=2$ is needed for convergence.
\end{example}

\begin{enumerate}[Test $(1)$]
    \item $\alpha=1.5$ Then $x_{k+1}=x_k(-0.5)$, $$\Rightarrow x_k=x_0(-0.5)^k \rightarrow 0 \text{ as } k \rightarrow \infty$$
    \item $\alpha=2.5$ Then $x_{k+1}=x_k(-1.5)$. $$\Rightarrow x_k=x_0(-1.5)^k \Rightarrow	|x_k| \rightarrow	\infty$$
    \item $\alpha=2$ Then $x_{k+1}=-x_k$. $$\Rightarrow	 x_k=(-1)^kx_0 \Rightarrow \text{ oscillation between }-x_0,x_0$$
\end{enumerate}

\begin{example}
What if gradient is not Lipschitz? e.g. $f(x)=x^4,x\in \mathbb{R}$, $\nabla f(x)=4x^3$, $x=0$ is the only stationary point (global-min)
$$x_{k+1}=x_k-4\alpha x_k^3=x_k(1-4\alpha x_k^2)$$
\end{example}
\begin{enumerate}[$\bullet$]
    \item $|x_1|=|x_0|$, then $|x_k|=|x_0|$ for all $k$, and $\{x_k\}$ stays bounded away from $0$, except if $x_0=0$
    \item \begin{equation}
        \begin{aligned}
            |x_1|<|x_0| &\Leftrightarrow	|x_0||1-4\alpha x_0^2|<|x_0|\\
            &\Leftrightarrow -1<1-4\alpha x_0^2<1\\
            &\Leftrightarrow 0<x_0^2<\frac{1}{2\alpha} \Leftrightarrow	0<|x_0|<\frac{1}{\sqrt{2\alpha}}\\
        \end{aligned}
        \nonumber
    \end{equation}
    \item Therefore, if $|x_1|<|x_0|$, then $|x_1|<|x_0|<\frac{1}{\sqrt{2\alpha}}$ $\Rightarrow	|x_2|<|x_1|,...,|x_{k+1}|<|x_k|,\forall k \Rightarrow \{|x_k|\}$convergences
    \item And if $|x_1|>|x_0|$, then $|x_{k+1}|>|x_k|$ for all $k$ and $\{x_k\}$ stays bounded away from $0$.
\end{enumerate}
\begin{claim}
$0<|x_0|<\frac{1}{\sqrt{2\alpha}} \Rightarrow |x_k| \rightarrow	0$
\end{claim}
\begin{proof}
Suppose $|x_k| \rightarrow	c>0$. Then $\frac{|x_{k+1}|}{|x_k|} \rightarrow	1$

But $\frac{|x_{k+1}|}{|x_k|} = |1-4\alpha x_k^2| \rightarrow |1-4\alpha c^2|$. Thus $|1-4\alpha c^2|=1 \Rightarrow	c=\frac{1}{\sqrt{2\alpha}}$, which contradicts to $c<|x_0|<\frac{1}{\sqrt{2\alpha}}$, hence $c=0$

\end{proof}


\subsection{Convergence of GD for convex functions}
\subsubsection{Theorem: $f$ is convex and has Lipschitz gradient $\Rightarrow$ $f(x_k)$ converges to global-min value with rate $\frac{1}{k}$}
\begin{theorem}
Consider the GD algorithm $$x_{k+1}=x_k-\alpha \nabla f(x_k),\quad k=0,1,...$$
Assume that $f$ has Lipschitz gradient with Lipschitz constant $L$. Further assume that
\begin{enumerate}[(a)]
    \item $f$ is a convex function.
    \item $\exists x^*$ s.t. $f(x^*)=\min f(x)$
\end{enumerate}
Then for sufficiently small $\alpha$:
\begin{enumerate}[(i)]
    \item $\lim_{k \rightarrow \infty} f(x_k)=\min f(x)=f(x^*)$
    \item $f(x_k)$ converges to $f(x^*)$ at rate $\frac{1}{k}$.
\end{enumerate}
\end{theorem}
\begin{proof}
\begin{equation}
    \begin{aligned}
        \|x_{k+1}-x^*\|^2&=\|x_k-\alpha \nabla f(x_k)-x^*\|^2\\
        &=\|x_k-x^*\|^2+\alpha^2\|\nabla f(x_k)\|^2-2\alpha \nabla f(x)^T(x_k-x^*)
    \end{aligned}
    \nonumber
\end{equation}
By convexity,
\begin{equation}
    \begin{aligned}
        f(x^*)&\geq f(x_k)+\nabla f(x_k)^T(x^*-x_k)\\
        \Rightarrow	\nabla f(x_k)^T(x^*-x_k)&\leq f(x^*)-f(x_k)
    \end{aligned}
    \nonumber
\end{equation}
Thus,
\begin{equation}
    \begin{aligned}
        \|x_{k+1}-x^*\|^2&\leq \|x_k-x^*\|^2+\alpha^2\|\nabla f(x_k)\|^2+2\alpha (f(x^*)-f(x_k))\\
        \Rightarrow 2\alpha (f(x_k)-f(x^*))&\leq \|x_k-x^*\|^2-\|x_{k+1}-x^*\|^2+\alpha^2\|\nabla f(x_k)\|^2\\
        2\alpha\sum_{k=0}^N (f(x_k)-f(x^*))&\leq \|x_0-x^*\|^2-\|x_{N+1}-x^*\|^2+\alpha^2\sum_{k=0}^N\|\nabla f(x_k)\|^2\\
        &\leq \|x_0-x^*\|^2+\alpha^2\sum_{k=0}^N\|\nabla f(x_k)\|^2
    \end{aligned}
    \nonumber
\end{equation}
According to previous theorm, if $\alpha\in (0,\frac{2}{L})$, $\sum_{k=0}^N\|\nabla f(x_k)\|^2\leq \frac{f(x_0)-f(x^*)}{\alpha(1-\frac{L\alpha}{2})}$ and
\begin{equation}
    \begin{aligned}
        f(x_{k+1})-f(x_k)&\leq-\alpha (1-\frac{L\alpha}{2})\|\nabla f(x_k)\|^2\leq 0\\
        \Rightarrow	f(x_N)&\leq f(x_k),\quad \forall k=0,1...,N\\
        \Rightarrow	\sum_{k=0}^N(f(x_k)-f(x^*))&\geq (N+1)(f(x_N)-f(x^*))\\
        f(x_N)-f(x^*)&\leq \frac{1}{N+1}\sum_{k=0}^N(f(x_k)-f(x^*))\\
        &\leq \frac{1}{2\alpha(N+1)}(\|x_0-x^*\|^2+\alpha^2\frac{f(x_0)-f(x^*)}{\alpha(1-\frac{L\alpha}{2})})\\
        &\rightarrow 0 \text{ as }N \rightarrow	\infty
    \end{aligned}
    \nonumber
\end{equation}
The rate of convergence is $\frac{1}{N}$.

To make $f(x_N)-f(x^*)<\varepsilon$, we need $N\sim O(\frac{1}{\varepsilon})$.
\end{proof}

\textbf{Note: }Armijo's rule also convergencesat rate $\frac{1}{N}$ if $\nabla f$ is Lipschitz, without priot knowledge of $L$. But need $r\in[\frac{1}{2},1)$


\subsection{Convergence of GD for strongly convex functions}
Strong convexity with parameter $m$, along with $M-$Lipschitz gradient assumption (with $M\geq m$)
According to the lemmas we proved before
\begin{equation}
    \begin{aligned}
        \frac{m}{2}\|y-x\|^2\leq f(y)-f(x)-\nabla^T f(x)(y-x)\leq \frac{M}{2}\|y-x\|^2
    \end{aligned}
    \nonumber
\end{equation}

\subsubsection{Theorem: Strongly convex, Lipschitz gradient $\Rightarrow$ $\{x_k\}$ converges to global-min geometrically}
\begin{theorem}
    If $f$ has Lipschitz gradient with Lipschitz constant $L$ and strongly convex with parameter $m$, $\{x_k\}$ converges to $x^*$ \textbf{geometrically}.
\end{theorem}

\begin{equation}
    \begin{aligned}
        \|x_{k+1}-x^*\|^2&=\|x_k-\alpha \nabla f(x_k)-x^*\|^2\\
        (\nabla f(x^*)=0)\quad \quad &=\|(x_k-x^*)-\alpha (\nabla f(x_k)-\nabla f(x^*))\|^2\\
        &=\|x_k-x^*\|^2+\alpha^2\|\nabla f(x_k)-\nabla f(x^*)\|^2-2\alpha(x_k-x^*)^T(\nabla f(x_k)-0)\\
        (\nabla f\text{ is M-Lipschitz})\quad \quad &\leq \|x_k-x^*\|^2+\alpha^2M^2\|x_k-x^*\|^2+2\alpha(x^*-x_k)^T\nabla f(x_k)\\
        (\text{Strong convexity with $m$})\quad &\leq \|x_k-x^*\|^2+\alpha^2M^2\|x_k-x^*\|^2+2\alpha(f(x^*)-f(x_k)-\frac{m}{2}\|x^*-x_k\|^2)\\
        &=(1+\alpha^2M^2-\alpha m)\|x_k-x^*\|^2+2\alpha (f(x^*)-f(x_k))
    \end{aligned}
    \nonumber
\end{equation}
By strong convexity of $f$
\begin{equation}
    \begin{aligned}
        f(x_k)&\geq f(x^*)+\nabla^T f(x^*)(x_k-x^*)+\frac{m}{2}\|x_k-x^*\|^2\\
        &= f(x^*)+\frac{m}{2}\|x_k-x^*\|^2\\
        \Rightarrow	f(x^*)-f(x_k)&\leq -\frac{m}{2}\|x_k-x^*\|^2
    \end{aligned}
    \nonumber
\end{equation}
Then,
\begin{equation}
    \begin{aligned}
        \|x_{k+1}-x^*\|^2&\leq (1+\alpha^2M^2-\alpha m)\|x_k-x^*\|^2+2\alpha (-\frac{m}{2}\|x_k-x^*\|^2)\\
        &\leq (1+\alpha^2M^2-2\alpha m)\|x_k-x^*\|^2\\
        &\leq (1+\alpha^2M^2-2\alpha m)^{k+1}\|x_0-x^*\|^2\\
        \Rightarrow	\|x_{N}-x^*\|^2&\leq (1+\alpha^2M^2-2\alpha m)^N\|x_0-x^*\|^2
    \end{aligned}
    \nonumber
\end{equation}
If $\alpha\in(0,\frac{2m}{M^2})$, $1+\alpha^2M^2-2\alpha m<1$. Then $x_N \rightarrow x^*$ \textbf{geometrically} as $N \rightarrow \infty$.

\textbf{Note: }Just having $0<\alpha<\frac{2}{M}$ doesn't guarantee geometric convergence to $x^*$. e.g. $\alpha=\frac{1}{M} \Rightarrow 1+\alpha^2M^2-2m\alpha=2(1-\frac{m}{M})\geq 1$ if $\frac{m}{M}\leq 0.5$

To get the highest convergence rate:
\begin{equation}
    \begin{aligned}
        1+\alpha^2M^2-2m\alpha&=(\alpha M)^2-2\alpha M\frac{m}{M}+1\\
        &=(\alpha M-\frac{m}{M})^2+1-\frac{m^2}{M^2}
    \end{aligned}
    \nonumber
\end{equation}
Which is minimized by setting $$\alpha=\alpha^*=\frac{m}{M^2}$$
$$\min_{\alpha>0}1+\alpha^2M^2-2m\alpha=1-\frac{m^2}{M^2}\in[0,1)$$
Since $M>m$, $\alpha^*=\frac{m}{M^2}<\frac{1}{M}<\frac{2}{M}$.

With $\alpha=\alpha^*$,
\begin{equation}
    \begin{aligned}
        \|x_{N}-x^*\|^2&\leq (1-\frac{m^2}{M^2})^N\|x_0-x^*\|^2
    \end{aligned}
    \nonumber
\end{equation}

$\frac{M}{m}$ is called the \textbf{\underline{condition number}}.
\begin{enumerate}[$\bullet$]
    \item If $\frac{M}{m}>>1$, then $1-\frac{m^2}{M^2}$ is close to $1$ and convergence is slow.
    \item If $\frac{M}{m}=1$, $\alpha^*=\frac{1}{M}$, and $x_N=x^*,\forall N\geq 1$. (Convergence in one step.)
\end{enumerate}
Note that since $\nabla f(x^*)=0$,
\begin{equation}
    \begin{aligned}
        f(x_N)-f(x^*)&\leq \frac{M}{2}\|x_N-x^*\|^2\\
        &\leq (1-\frac{m^2}{M^2})^N\frac{M}{2}\|x_0-x^*\|^2\\
    \end{aligned}
    \nonumber
\end{equation}
To make $f(x_N)-f(x^*)<\varepsilon$, we only need $N\sim O(log\frac{1}{\varepsilon})$ - called "linear" convergence.

\subsubsection{Example}
\begin{example}
    $f(x)=\frac{1}{2}x^TQx+b^Tx+c,\quad Q\succ 0$, $\nabla^2 f(x)=Q$.
\end{example}
Let $\lambda_{\min}$ and $\lambda_{\max}$ be the min and max eigenvalue of $Q$. Then we know $$\lambda_{\min}\|\mathfrak{Z}\|^2\leq\mathfrak{Z}^TQ\mathfrak{Z}\leq \lambda_{\max}\|\mathfrak{Z}\|^2$$
Thus for all $\mathfrak{Z}\in \mathbb{R}^n$
$$\mathfrak{Z}^T(Q-\lambda_{\min}I)\mathfrak{Z}\geq 0 \Rightarrow	Q\succeq \lambda_{\min}I$$
Similarly, $Q\preceq \lambda_{\max}I$. Thus
$$\lambda_{\min}I\preceq \nabla^2 f(x)\preceq \lambda_{\max}I$$
$\lambda_{\min}I\preceq \nabla^2 f(x) \Leftrightarrow$ $f$ is $\lambda_{\min}$-strongly convex; $\nabla^2 f(x)\preceq \lambda_{\max}I$ is a sufficient condition for $f$ is $\lambda_{\max}$-smooth.

The condition number $=\frac{\lambda_{\max}}{\lambda_{\min}}$

\textbf{Special Case:} $Q=\mu I,\quad \mu>0$, $\lambda_{\min}=\lambda_{\max}=\mu=m=M$.

$f(x)=\frac{\mu}{2}\|x\|^2+b^Tx+c$, $\nabla f(x)=\mu x+b$, $x^*=-\frac{b}{\mu}$, $\alpha^*=\frac{m}{M^2}=\frac{1}{\mu}$,
\begin{equation}
    \begin{aligned}
        x_1=x_0-\alpha^*\nabla f(x_0)=x_0-\frac{1}{\mu}(\mu x_0+b)=-\frac{b}{\mu}=x^*
    \end{aligned}
    \nonumber
\end{equation}
Convergence in one step!

\subsection{Convergence of Gradient Descent on Smooth Strongly-Convex Functions}
Still consider the constant stepsize gradient method$$x_{k+1}=x_k-\alpha \nabla f(x_k)$$
\begin{theorem}
    Suppose $f$ is $L$-smooth and $m$-strongly convex. Let $x^*$ be the unique global
    min. Given a stepsize $\alpha$,if there exists $0<\rho<1$ and $\lambda\geq 0$ such that
    \begin{equation}
        \begin{aligned}
            \begin{bmatrix}
                1-\rho^2&	-\alpha\\
                -\alpha&	\alpha^2
            \end{bmatrix}+\lambda \begin{bmatrix}
                -2mL&	m+L\\
                m+L&	-2
            \end{bmatrix}
        \end{aligned}
        \nonumber
    \end{equation}
    is a negative semidefinite matrix, then the gradient method satisfies $$\|x_k-x^*\|\leq \rho^k\|x_0-x^*\|$$
\end{theorem}

\begin{lemma}
    Suppose the sequences $\left\{\xi_{k} \in \mathbb{R}^{p}: k=0,1, \ldots\right\}$ and $\left\{u_{k} \in \mathbb{R}^{p}: k=0,1,2, \ldots\right\}$ satisfy $\xi_{k+1}=\xi_{k}-\alpha u_{k} .$ In addition, assume the following inequality holds for all $k$
    $$
    \left[\begin{array}{l}
    \xi_{k} \\
    u_{k}
    \end{array}\right]^{\top} M\left[\begin{array}{l}
    \xi_{k} \\
    u_{k}
    \end{array}\right] \geq 0
    $$
    If there exist $0<\rho<1$ and $\lambda \geq 0$ such that
    $$
    \left[\begin{array}{cc}
    \left(1-\rho^{2}\right) I & -\alpha I \\
    -\alpha I & \alpha^{2} I
    \end{array}\right]+\lambda M
    $$
    is a negative semidefinite matrix, then the sequence $\left\{\xi_{k}: k=0,1, \ldots\right\}$ satisfies $\left\|\xi_{k}\right\| \leq \rho^{k}\left\|\xi_{0}\right\|$.
\end{lemma}
\begin{proof}
The key relation is
\begin{equation}
    \begin{aligned}
        \|\xi_{k+1}\|^2=\|\xi_k-\alpha u_k\|^2=\|\xi_k\|^2-2\alpha (\xi_k)^T u_k+\alpha^2\|u_k\|^2=
        \left[\begin{array}{l}
            \xi_{k} \\
            u_{k}
        \end{array}\right]^{\top} 
        \begin{bmatrix}
            I&	-\alpha I\\
            -\alpha I&	\alpha^2 I
        \end{bmatrix}
        \left[\begin{array}{l}
            \xi_{k} \\
            u_{k}
        \end{array}\right]
    \end{aligned}
    \nonumber
\end{equation}
Since $\left[\begin{array}{cc}
\left(1-\rho^{2}\right) I & -\alpha I \\
-\alpha I & \alpha^{2} I
\end{array}\right]+\lambda M$ is negative semidefinite, we have
$$\left[\begin{array}{l}
    \xi_{k} \\
    u_{k}
\end{array}\right]^{\top}
\left( \left[\begin{array}{cc}
    \left(1-\rho^{2}\right) I & -\alpha I \\
    -\alpha I & \alpha^{2} I
    \end{array}\right]+\lambda M\right)
    \left[\begin{array}{l}
        \xi_{k} \\
        u_{k}
    \end{array}\right]$$
\end{proof}











































































































\end{document}